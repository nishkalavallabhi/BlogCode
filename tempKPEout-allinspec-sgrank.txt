Twenty years of the literature on acquiring out-of-print materials
This article reviews the last two-and-a-half decades of literature on acquiring
	out-of-print materials to assess recurring issues and identify changing
	practices. The out-of-print literature is uniform in its assertion that
	libraries need to acquire o.p. materials to replace worn or damaged
	copies, to replace missing copies, to duplicate copies of heavily used
	materials, to fill gaps in collections, to strengthen weak collections,
	to continue to develop strong collections, and to provide materials for
	new courses, new programs, and even entire new libraries
['out-of-print materials', 'recurring issues', 'changing practices', 'out-of-printbooks', 'library materials', 'acquisition']
['print material', 'half decade', 'entire new library', 'new program', 'material', 'print literature', 'new course', 'strong collection', 'literature', 'weak collection']

A new method of systemological analysis coordinated with the procedure of
	object-oriented design. II
For pt.I. see Vestn. KhGPU, no.81, p.15-18 (2000). The paper presents the
	results of development of an object-oriented systemological method used
	to design complex systems. A formal system representation, as well as
	an axiomatics of the calculus of systems as functional flow-type
	objects based on a Node-Function-Object class hierarchy are proposed. A
	formalized NFO/UFO analysis algorithm and CASE tools used to support it
	are considered
['systemological analysis', 'object-oriented design', 'complex systems design', 'formalsystem representation', 'axiomatics', 'functional flow-type objects', 'formalized nfo/ufo analysis algorithm', 'case tools']
['ufo analysis algorithm', 'formal system representation', 'object class hierarchy', 'systemological analysis', 'complex system', 'new method', 'systemological method', 'object', 'functional flow', 'p.15']

Mathematical fundamentals of constructing fuzzy Bayesian inference techniques
Problems and an associated technique for developing a Bayesian approach to
	decision-making in the case of fuzzy data are presented. The concept of
	fuzzy and pseudofuzzy quantities is introduced and main operations with
	pseudofuzzy quantities are considered. The basic relationships and the
	principal concepts of the Bayesian decision procedure based on the
	modus-ponens rule are proposed. Some problems concerned with the
	practical realization of the fuzzy Bayesian method are considered
['mathematical fundamentals', 'fuzzy bayesian inference techniques', 'decisionmaking', 'pseudofuzzy quantities', 'modus-ponens rule']
['fuzzy bayesian inference technique', 'pseudofuzzy quantity', 'bayesian decision procedure', 'mathematical fundamental', 'main operation', 'principal concept', 'fuzzy datum', 'basic relationship', 'bayesian approach', 'ponen rule']

Solution of the safe problem on (0,1)-matrices
A safe problem with mn locks is studied. It is reduced to a system of linear
	equations in the modulo 2 residue class. There are three possible
	variants defined by the numbers m and n evenness, with only one of them
	having a solution. In two other cases, correction of the initial state
	of the safe insuring a solution is proposed
['safe problem', 'mn locks', 'linear equations', 'modulo 2 residue class', '(0', '1)-matrices', 'computer games', 'linear diophantine equations']
['safe problem', 'initial state', 'number m', 'solution', 'residue class', 'mn lock', 'safe', '0,1)-matrice', 'equation', 'modulo']

Accelerated simulation of the steady-state availability of non-Markovian
	systems
A general accelerated simulation method for evaluation of the steady-state
	availability of non-Markovian systems is proposed. It is applied to the
	investigation of a class of systems with repair. Numerical examples are
	given
['accelerated simulation', 'steady-state availability', 'non-markovian systems', 'general accelerated simulation method', 'numerical examples']
['general accelerated simulation method', 'steady', 'state availability', 'system', 'non', 'state', 'markovian system', 'availability', 'markovian', 'evaluation']

Computational finite-element schemes for optimal control of an elliptic system
	with conjugation conditions
New optimal control problems are considered for distributed systems described
	by elliptic equations with conjugate conditions and a quadratic
	minimized function. Highly accurate computational discretization
	schemes are constructed for the case where a feasible control set u/sub
	delta / coincides with the full Hilbert space u of controls
['optimal control problems', 'distributed systems', 'elliptic equations', 'conjugateconditions', 'quadratic minimized function', 'computational discretizationschemes']
['new optimal control problem', 'accurate computational discretization', 'conjugation condition', 'conjugate condition', 'elliptic system', 'element scheme', 'computational finite', 'elliptic equation', 'feasible control', 'scheme']

Identification of states of complex systems with estimation of admissible
	measurement errors on the basis of fuzzy information
The problem of identification of states of complex systems on the basis of
	fuzzy values of informative attributes is considered. Some estimates of
	a maximally admissible degree of measurement error are obtained that
	make it possible, using the apparatus of fuzzy set theory, to correctly
	identify the current state of a system
['complex systems states identification', 'admissible measurement errors', 'fuzzyinformation', 'informative attributes', 'measurement error', 'fuzzy settheory']
['complex system', 'measurement error', 'fuzzy information', 'informative attribute', 'fuzzy value', 'identification', 'admissible degree', 'fuzzy set theory', 'state', 'basis']

A new approach to the decomposition of Boolean functions by the method of
	q-partitions.II. Repeated decomposition
For pt.I. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the
	decomposition of Boolean,functions that depend on n variables and are
	represented in various forms is considered. The approach is based on
	the method of q-partitioning of minterms and on the introduced concept
	of a decomposition clone. The theorem on simple disjunctive
	decomposition of full and partial functions is formulated. The approach
	proposed is illustrated by examples
['boolean functions decomposition', 'minterms', 'decomposition clone', 'disjunctivedecomposition', 'partial functions', 'logic synthesis', 'q-partitions']
['new approach', 'boolean function', 'repeated decomposition', 'decomposition', 'approach', 'partial function', 'method', 'q', 'simple disjunctive', 'boolean']

Nonlinear extrapolation algorithm for realization of a scalar random process
A method of construction of a nonlinear extrapolation algorithm is proposed.
	This method makes it possible to take into account any nonlinear random
	dependences that exist in an investigated process and are described by
	mixed central moment functions. The method is based on the V. S.
	Pugachev canonical decomposition apparatus. As an example, the problem
	of nonlinear extrapolation is solved for a moment function of third
	order
['nonlinear extrapolation algorithm', 'scalar random process', 'nonlinear randomdependences', 'mixed central moment functions', 'canonical decompositionapparatus', 'moment function']
['pugachev canonical decomposition apparatus', 'nonlinear extrapolation algorithm', 'mixed central moment function', 'extrapolation algorithm', 'scalar random process', 'nonlinear extrapolation', 'method', 'investigated process', 'v. s.', 'construction']

A method for solution of systems of linear algebraic equations with
	m-dimensional lambda -matrices
A system of linear algebraic equations with m-dimensional lambda -matrices is
	considered. The proposed method of searching for the solution of this
	system lies in reducing it to a numerical system of a special kind
['linear algebraic equations', 'numerical system', 'm-dimensional lambda -matrices']
['linear algebraic equation', 'dimensional lambda -matrice', 'system', 'solution', 'method', 'proposed method', 'numerical system', 'special kind']

Compatibility of systems of linear constraints over the set of natural numbers
Criteria of compatibility of a system of linear Diophantine equations, strict
	inequations, and nonstrict inequations are considered. Upper bounds for
	components of a minimal set of solutions and algorithms of construction
	of minimal generating sets of solutions for all types of systems are
	given. These criteria and the corresponding algorithms for constructing
	a minimal supporting set of solutions can be used in solving all the
	considered types of systems and systems of mixed types
['linear constraints', 'set of natural numbers', 'linear diophantine equations', 'strict inequations', 'nonstrict inequations', 'upper bounds', 'minimalgenerating sets']
['minimal generating set', 'linear diophantine equation', 'system', 'compatibility', 'natural number', 'nonstrict inequation', 'minimal set', 'criterion', 'linear constraint', 'set']

Books on demand: just-in-time acquisitions
The Purdue University Libraries Interlibrary Loan unit proposed a pilot project
	to purchase patrons' loan requests from Amazon. com, lend them to the
	patrons, and then add the titles to the collection. Staff analyzed
	previous monograph loans, developed ordering criteria, implemented the
	proposal as a pilot project for six months, and evaluated the resulting
	patron comments, statistics, and staff perceptions. As a result of
	enthusiastic patron comments and a review of the project statistics,
	the program was extended
['purdue university libraries interlibrary loan unit', 'monograph loans', 'orderingcriteria', 'staff perceptions', 'patron comments', 'publication on demand']
['purdue university libraries interlibrary loan unit', 'pilot project', 'previous monograph loan', 'loan request', 'time acquisition', 'enthusiastic patron comment', 'patron comment', 'staff perception', 'staff', 'project statistic']

New lower bounds of the size of error-correcting codes for the Z-channel
Optimization problems on graphs are formulated to obtain new lower bounds of
	the size of error-correcting codes for the Z-channel
['lower bounds', 'error-correcting codes', 'z-channel', 'optimization problems', 'graphs']
['new low bound', 'size', 'error', 'code', 'channel', 'optimization problem', 'graph']

Descriptological foundations of programming
Descriptological foundations of programming are constructed. An explication of
	the concept of a descriptive process is given. The operations of
	introduction and elimination of abstraction at the level of processes
	are refined. An intensional concept of a bipolar function is
	introduced. An explication of the concept of introduction and
	extraction of abstraction at the bipole level is given. On this basis,
	a complete set of descriptological operations is constructed
['descriptological foundations', 'programming', 'descriptive process', 'intensionalconcept', 'bipolar function', 'bipole level']
['descriptological foundation', 'descriptological operation', 'bipolar function', 'bipole level', 'complete set', 'descriptive process', 'intensional concept', 'explication', 'concept', 'abstraction']

Precoded OFDM with adaptive vector channel allocation for scalable video
	transmission over frequency-selective fading channels
Orthogonal frequency division multiplexing (OFDM) has been applied in broadband
	wireline and wireless systems for high data rate transmission where
	severe intersymbol interference (ISI) always occurs. The conventional
	OFDM system provides advantages through conversion of an ISI channel
	into ISI-free subchannels at multiple frequency bands. However, it may
	suffer from channel spectral nulls and heavy data rate overhead due to
	cyclic prefix insertion. Previously, a new OFDM framework, the precoded
	OFDM, has been proposed to mitigate the above two problems through
	precoding and conversion of an ISI channel into ISI-free vector
	channels. In this paper, we consider the application of the precoded
	OFDM system to efficient scalable video transmission. We propose to
	enhance the precoded OFDM system with adaptive vector channel
	allocation to provide stronger protection against errors to more
	important layers in the layered bit stream structure of scalable video.
	The more critical layers, or equivalently, the lower layers, are
	allocated vector channels of higher transmission quality. The channel
	quality is characterized by Frobenius norm metrics; based on channel
	estimation at the receiver. The channel allocation information is fed
	back periodically to the transmitter through a control channel.
	Simulation results have demonstrated the robustness of the proposed
	scheme to noise and fading inherent in wireless channels
['precoded ofdm', 'scalable video transmission', 'frequency-selective fadingchannels', 'orthogonal frequency division multiplexing', 'channel spectralnulls', 'heavy data rate overhead', 'isi channel', 'isi-free vector channels', 'adaptive vector channel allocation', 'layered bit stream structure', 'lowerlayers', 'critical layers', 'channel quality', 'frobenius norm metrics', 'channel estimation', 'channel allocation information', 'control channel', 'robustness']
['adaptive vector channel', 'scalable video', 'channel allocation', 'ofdm system', 'isi channel', 'vector channel', 'division multiplexing', 'intersymbol interference', 'frequency division', 'rate transmission']

I-WAP: an intelligent WAP site management system
The popularity regarding wireless communications is such that more and more WAP
	sites have been developed with wireless markup language (WML).
	Meanwhile, to translate hypertext markup language (HTML) pages into
	proper WML ones becomes imperative since it is difficult for WAP users
	to read most contents designed for PC users via their mobile phone
	screens. However, for those sites that have been maintained with
	hypertext markup language (HTML), considerable time and manpower costs
	will be incurred to rebuild them with WML. In this paper, we propose an
	intelligent WAP site management system to cope with these problems.
	With the help of the intelligent management system, the original
	contents of HTML Web sites can be automatically translated to proper
	WAP content in an efficient way. As a consequence, the costs associated
	with maintaining WAP sites could be significantly reduced. The
	management system also allows the system manager to define the
	relevance of numerals and keywords for removing unimportant or
	meaningless contents. The original contents will be reduced and
	reorganized to fit the size of mobile phone screens, thus reducing the
	communication cost and enhancing readability. Numerical results gained
	through various experiments have evinced the effective performance of
	the WAP management system
['intelligent wap site management system', 'i-wap', 'wireless communication', 'wirelessmarkup language', 'hypertext markup language', 'html pages', 'mobile phone', 'communication cost', 'readability', 'wireless mobile internet']
['intelligent wap site management system', 'hypertext markup language', 'proper wml one', 'wireless markup language', 'html web site', 'intelligent management system', 'mobile phone', 'content', 'wireless communication', 'manpower cost']

A framework of electronic tendering for government procurement: a lesson
	learned in Taiwan
To render government procurement efficient, transparent, nondiscriminating, and
	accountable, an electronic government procurement system is required.
	Accordingly, Taiwan government procurement law (TGPL) states that
	suppliers may employ electronic devices to forward a tender. This
	investigation demonstrates how the electronic government procurement
	system functions and reengineers internal procurement processes, which
	in turn benefits both government bodies and vendors. The system
	features explored herein include posting/receiving bids via the
	Internet, vendor registration, certificate authorization, contract
	development tools, bid/request for proposal (RFP) development, online
	bidding, and online payment, all of which can be integrated easily
	within most existing information infrastructures
['electronic tendering', 'electronic government procurement system', 'taiwangovernment procurement law', 'reengineering', 'internal procurementprocesses', 'internet bids', 'vendor registration', 'certificateauthorization', 'contract development tools', 'request for proposaldevelopment', 'rfp development', 'online bidding', 'online payment', 'certification authority', 'payment gateway', 'public key infrastructure']
['reengineer internal procurement process', 'taiwan government procurement law', 'electronic government procurement system', 'electronic government procurement', 'government procurement efficient', 'electronic tendering', 'electronic device', 'government body', 'system function', 'turn benefit']

The development of a mobile manipulator imaging system for bridge crack
	inspection
A mobile manipulator imaging system is developed for the automation of bridge
	crack inspection. During bridge safety inspections, an eyesight
	inspection is made for preliminary evaluation and screening before a
	more precise inspection. The inspection for cracks is an important part
	of the preliminary evaluation. Currently, the inspectors must stand on
	the platform of a bridge inspection vehicle or a temporarily erected
	scaffolding to examine the underside of a bridge. However, such a
	procedure is risky. To help automate the bridge crack inspection
	process, we installed two CCD cameras and a four-axis manipulator
	system on a mobile vehicle. The parallel cameras are used to detect
	cracks. The manipulator system is equipped with binocular charge
	coupled devices (CCD) for examining structures that may not be
	accessible to the eye. The system also reduces the danger of accidents
	to the human inspectors. The manipulator system consists of four arms.
	Balance weights are placed at the ends of arms 2 and 4, respectively,
	to maintain the center of gravity during operation. Mechanically, arms
	2 and 4 can revolve smoothly. Experiments indicated that the system
	could be useful for bridge crack inspections
['mobile manipulator', 'imaging system', 'bridge crack inspection', 'automation', 'eyesight inspection', 'ccd cameras', 'four-axis manipulator', 'parallelcameras', 'binocular ccd', 'charge coupled devices']
['mobile manipulator imaging system', 'bridge safety inspection', 'preliminary evaluation', 'crack inspection', 'bridge inspection vehicle', 'manipulator system', 'bridge', 'precise inspection', 'binocular charge', 'parallel camera']

Integrating building management system and facilities management on the
	Internet
Recently, it is of great interest to adopt the Internet/intranet to develop
	building management systems (BMS) and facilities management systems
	(FMS). This paper addresses two technical issues: the Web-based access
	(including database integration) and the integration of BMS and FMS.
	These should be addressed for accessing BMS remotely via the Internet,
	integrating control networks using the Internet protocols and
	infrastructures, and using Internet/intranet for building facilities
	management. An experimental Internet-enabled system that integrates
	building and facilities management systems has been developed and
	tested. This system integrated open control networks with the Internet
	and is developed utilizing the embedded Web server, the PC Web server
	and the Distributed Component Object Model (DCOM) software development
	technology on the platform of an open control network. Three strategies
	for interconnecting BMS local networks via Internet/intranet are
	presented and analyzed
['intranet', 'building management systems', 'bms', 'facilities management systems', 'fms', 'web-based access', 'database integration', 'internet protocols', 'embeddedweb server', 'pc web server', 'distributed component object model', 'dcom', 'software development technology', 'open control network', 'local networkinterconnection']
['building management system', 'facility management system', 'open control network', 'distributed component object model', 'pc web server', 'internet', 'facility management', 'intranet', 'bms', 'great interest']

Modelling user acceptance of building management systems
This study examines user acceptance of building management systems (BMS) using
	a questionnaire survey. These systems are crucial for optimising
	building performance and yet it has been widely reported that users are
	not making full use of their systems' facilities. Established models of
	technology acceptance have been employed in this research, and the
	positive influence of user perceptions of ease of use and compatibility
	has been demonstrated. Previous research has indicated differing levels
	of importance of perceived ease of use relative to other factors. Here,
	perceived ease of use is shown generally to be more important, though
	the balance between this and compatibility is moderated by the user
	perceptions of voluntariness
['user acceptance modelling', 'building management systems', 'technology acceptancemodel', 'innovation characteristics', 'information systems', 'questionnairesurvey', 'user perceptions', 'ease of use', 'compatibility', 'voluntariness']
['user acceptance', 'management system', 'use relative', 'previous research', 'user perception', 'user', 'building performance', 'positive influence', 'technology acceptance', 'questionnaire survey']

Estimating populations for collective dose calculations
The collective dose provides an estimate of the effects of facility operations
	on the public based on an estimate of the population in the area.
	Geographic information system software, electronic population data
	resources, and a personal computer were used to develop estimates of
	population within 80 km radii of two sites
['collective dose calculations', 'facility operations', 'public', 'geographicinformation system software', 'electronic population data resources', 'personal computer']
['geographic information system software', 'collective dose calculation', 'electronic population datum', 'collective dose', 'estimate', 'facility operation', 'population', 'effect', 'personal computer', 'public']

A new graphical user interface for fast construction of computation phantoms
	and MCNP calculations: application to calibration of in vivo
	measurement systems
Reports on a new utility for development of computational phantoms for Monte
	Carlo calculations and data analysis for in vivo measurements of
	radionuclides deposited in tissues. The individual properties of each
	worker can be acquired for a rather precise geometric representation of
	his (her) anatomy, which is particularly important for low energy gamma
	ray emitting sources such as thorium, uranium, plutonium and other
	actinides. The software enables automatic creation of an MCNP input
	data file based on scanning data. The utility includes segmentation of
	images obtained with either computed tomography or magnetic resonance
	imaging by distinguishing tissues according to their signal
	(brightness) and specification of the source and detector. In addition,
	a coupling of individual voxels within the tissue is used to reduce the
	memory demand and to increase the calculational speed. The utility was
	tested for low energy emitters in plastic and biological tissues as
	well as for computed tomography and magnetic resonance imaging scanning
	information
['computational phantoms', 'monte carlo calculations', 'in vivo measurements', 'radionuclides', 'tissues', 'worker', 'precise geometric representation', 'mcnpinput data file', 'scanning data', 'computed tomography', 'brightness', 'graphical user interface', 'computation phantoms', 'calibration', 'in vivomeasurement systems', 'th', 'u', 'pu', 'signal', 'detector', 'individual voxels', 'memory demand', 'calculational speed', 'plastic', 'biological tissues', 'magnetic resonance imaging scanning information', 'anatomy', 'low energygamma ray emitting sources', 'actinides', 'software', 'automatic creation']
['low energy', 'magnetic resonance', 'emitting source', 'scanning datum', 'datum file', 'energy gamma', 'ray emitting', 'mcnp input', 'automatic creation', 'individual voxel']

The acquisition of out-of-print music
Non-specialist librarians are alerted to factors important in the successful
	acquisition of out-of-print music, both scholarly editions and
	performance editions. The appropriate technical music vocabulary, the
	music publishing industry, specialized publishers and vendors, and
	methods of acquisition of out-of-print printed music are introduced,
	and the need for familiarity with them is emphasized
['out-of-print music', 'scholarly editions', 'performance editions', 'technical musicvocabulary', 'music publishing industry', 'specialized publishers', 'specialized vendors', 'out-of-print printed music']
['print music', 'appropriate technical music vocabulary', 'music publishing industry', 'specialist librarian', 'factor important', 'acquisition', 'performance edition', 'scholarly edition', 'specialized publisher', 'print']

General solution of a density functionally gradient piezoelectric cantilever
	and its applications
We have used the plane strain theory of transversely isotropic bodies to study
	a piezoelectric cantilever. In order to find the general solution of a
	density functionally gradient piezoelectric cantilever, we have used
	the inverse method (i.e. the Airy stress function method). We have
	obtained the stress and induction functions in the form of polynomials
	as well as the general solution of the beam. Based on this general
	solution, we have deduced the solutions of the cantilever under
	different loading conditions. Furthermore, as applications of this
	general solution in engineering, we have studied the tip deflection and
	blocking force of a piezoelectric cantilever actuator. Finally, we have
	addressed a method to determine the density distribution profile for a
	given piezoelectric material
['plane strain theory', 'transversely isotropic bodies', 'inverse method', 'airy stressfunction', 'polynomials', 'loading conditions', 'piezoelectric cantileveractuator', 'density distribution profile', 'piezoelectric material']
['gradient piezoelectric cantilever', 'general solution', 'plane strain theory', 'airy stress function method', 'piezoelectric cantilever', 'isotropic body', 'different loading condition', 'solution', 'inverse method', 'piezoelectric cantilever actuator']

Recording quantum properties of light in a long-lived atomic spin state:
	towards quantum memory
We report an experiment on mapping a quantum state of light onto the ground
	state spin of an ensemble of Cs atoms with the lifetime of 2 ms.
	Recording of one of the two quadrature phase operators of light is
	demonstrated with vacuum and squeezed states of light. The sensitivity
	of the mapping procedure at the level of approximately 1 photon/sec per
	Hz is shown. The results pave the road towards complete (storing both
	quadrature phase observables) quantum memory for Gaussian states of
	light. The experiment also sheds new light on fundamental limits of
	sensitivity of the magneto-optical resonance method
['light quantum properties recording', 'long-lived atomic spin state', 'quantummemory', 'ground state spin', 'ensemble', 'two quadrature phase operators', 'vacuum states', 'squeezed states', 'mapping procedure', 'magnetoopticalresonance method', '2 ms', 'cs']
['quadrature phase observable', 'optical resonance method', 'quadrature phase operator', 'atomic spin state', 'quantum memory', 'light', 'cs atom', 'quantum state', 'state spin', 'quantum property']

Comprehensive encoding and decoupling solution to problems of decoherence and
	design in solid-state quantum computing
Proposals for scalable quantum computing devices suffer not only from
	decoherence due to the interaction with their environment, but also
	from severe engineering constraints. Here we introduce a practical
	solution to these major concerns, addressing solid-state proposals in
	particular. Decoherence is first reduced by encoding a logical qubit
	into two qubits, then completely eliminated by an efficient set of
	decoupling pulse sequences. The same encoding removes the need for
	single-qubit operations, which pose a difficult design constraint. We
	further show how the dominant decoherence processes can be identified
	empirically, in order to optimize the decoupling pulses
['solid-state quantum computing', 'decoherence', 'logical qubit encoding', 'pulsesequence decoupling', 'engineering constraints', 'decoupling pulseoptimization', 'scalable quantum computing devices', 'exchange hamiltonian']
['scalable quantum computing device', 'severe engineering constraint', 'comprehensive encoding', 'state quantum', 'solution', 'solid', 'state proposal', 'difficult design constraint', 'major concern', 'logical qubit']

Social percolation and the influence of mass media
In the marketing model of Solomon and Weisbuch, people buy a product only if
	their neighbours tell them of its quality, and if this quality is
	higher than their own quality expectations. Now we introduce additional
	information from the mass media, which is analogous to the ghost field
	in percolation theory. The mass media shift the percolative phase
	transition observed in the model, and decrease the time after which the
	stationary state is reached
['social percolation', 'mass media influence', 'solomon-weisbuch marketing model', 'quality expectations', 'ghost field', 'percolative phase transition', 'stationary state', 'customers', 'cinema', 'external field']
['mass medium', 'marketing model', 'quality expectation', 'ghost field', 'social percolation', 'percolation theory', 'stationary state', 'percolative phase', 'quality', 'model']

Estimating long-range dependence: finite sample properties and confidence
	intervals
A major issue in financial economics is the behavior of asset returns over long
	horizons. Various estimators of long-range dependence have been
	proposed. Even though some have known asymptotic properties, it is
	important to test their accuracy by using simulated series of different
	lengths. We test R/S analysis, detrended fluctuation analysis and
	periodogram regression methods on samples drawn from Gaussian white
	noise. The DFA statistics turns out to be the unanimous winner.
	Unfortunately, no asymptotic distribution theory has been derived for
	this statistics so far. We were able, however, to construct empirical
	(i.e. approximate) confidence intervals for all three methods. The
	obtained values differ largely from heuristic values proposed by some
	authors for the R/S statistics and are very close to asymptotic values
	for the periodogram regression method
['long-range dependence', 'finite sample properties', 'confidence intervals', 'financial economics', 'asset returns', 'long horizons', 'asymptoticproperties', 'detrended fluctuation analysis', 'periodogram regressionmethods', 'gaussian white noise', 'heuristic values']
['periodogram regression method', 'asymptotic distribution theory', 'finite sample property', 'range dependence', 'financial economic', 'asset return', 'various estimator', 'major issue', 'long', 'asymptotic property']

Simulation of evacuation processes using a bionics-inspired cellular automaton
	model for pedestrian dynamics
We present simulations of evacuation processes using a recently introduced
	cellular automaton model for pedestrian dynamics. This model applies a
	bionics approach to describe the interaction between the pedestrians
	using ideas from chemotaxis. Here we study a rather simple situation,
	namely the evacuation from a large room with one or two doors. It is
	shown that the variation of the model parameters allows to describe
	different types of behaviour, from regular to panic. We find a
	non-monotonic dependence of the evacuation times on the coupling
	constants. These times depend on the strength of the herding behaviour,
	with minimal evacuation times for some intermediate values of the
	couplings, i.e., a proper combination of herding and use of knowledge
	about the shortest way to the exit
['evacuation processes simulation', 'chemotaxis', 'nonmonotonic dependence', 'couplingconstants', 'herding behaviour', 'bionics-inspired cellular automatonmodel', 'pedestrian dynamics']
['minimal evacuation time', 'evacuation process', 'cellular automaton model', 'pedestrian dynamic', 'cellular automaton', 'evacuation time', 'different type', 'large room', 'monotonic dependence', 'model parameter']

Dynamical transition to periodic motions of a recurrent bus induced by nonstops
We study the dynamical behavior of a recurrent bus on a circular route with
	many bus stops when the recurrent bus passes some bus stops without
	stopping. The recurrent time (one period) is described in terms of a
	nonlinear map. It is shown that the recurrent bus exhibits the complex
	periodic behaviors. The dynamical transitions to periodic motions occur
	by increasing nonstops. The periodic motions depend on the property of
	an attractor of the nonlinear map. The period n of the attractor varies
	sensitively with the number of nonstops
['dynamical transition', 'periodic motions', 'recurrent bus', 'nonstops', 'circularroute', 'recurrent time', 'nonlinear map', 'complex periodic behaviors', 'attractor']
['recurrent bus', 'periodic motion', 'dynamical transition', 'dynamical behavior', 'circular route', 'nonlinear map', 'recurrent time', 'nonstop', 'periodic behavior', 'bus']

The two populations' cellular automata model with predation based on the Penna
	model
In Penna's (1995) single-species asexual bit-string model of biological ageing,
	the Verhulst factor has too strong a restraining effect on the
	development of the population. Danuta Makowiec gave an improved model
	based on the lattice, where the restraining factor of the four
	neighbours take the place of the Verhulst factor. Here, we discuss the
	two populations' Penna model with predation on the planar lattice of
	two dimensions. A cellular automata model containing movable wolves and
	sheep has been built. The results show that both the quantity of the
	wolves and the sheep fluctuate in accordance with the law that one
	quantity increases while the other one decreases
['cellular automata model', 'population', 'penna model', 'single-species asexualbit-string model', 'biological ageing', 'verhulst factor', 'restrainingeffect', 'lattice', 'wolves', 'sheep', 'fluctuation', 'lotka-volterra model', 'predation']
['cellular automata model', 'specie asexual bit', 'cellular automaton model', 'verhulst factor', 'population', 'improved model', 'danuta makowiec', 'predation', 'penna', 'biological ageing']

Option pricing from path integral for non-Gaussian fluctuations. Natural
	martingale and application to truncated Levy distributions
Within a path integral formalism for non-Gaussian price fluctuations, we set up
	a simple stochastic calculus and derive a natural martingale for option
	pricing from the wealth balance of options, stocks, and bonds. The
	resulting formula is evaluated for truncated Levy distributions
['option pricing', 'path integrals', 'stochastic calculus', 'stocks', 'bonds', 'nongaussianfluctuations', 'natural martingale', 'truncated levy distributions']
['simple stochastic calculus', 'path integral', 'gaussian price fluctuation', 'levy distribution', 'path integral formalism', 'gaussian fluctuation', 'option pricing', 'option', 'non', 'natural martingale']

Quantum market games
We propose a quantum-like description of markets and economics. The approach
	has roots in the recently developed quantum game theory
['quantum market games', 'economics', 'quantum game theory', 'quantum strategies', 'financial markets']
['quantum game theory', 'quantum market game', 'like description', 'root', 'approach', 'economic']

On the emergence of rules in neural networks
A simple associationist neural network learns to factor abstract rules (i.e.,
	grammars) from sequences of arbitrary input symbols by inventing
	abstract representations that accommodate unseen symbol sets as well as
	unseen but similar grammars. The neural network is shown to have the
	ability to transfer grammatical knowledge to both new symbol
	vocabularies and new grammars. Analysis of the state-space shows that
	the network learns generalized abstract structures of the input and is
	not simply memorizing the input strings. These representations are
	context sensitive, hierarchical, and based on the state variable of the
	finite-state machines that the neural network has learned.
	Generalization to new symbol sets or grammars arises from the spatial
	nature of the internal representations used by the network, allowing
	new symbol sets to be encoded close to symbol sets that have already
	been learned in the hidden unit space of the network. The results are
	counter to the arguments that learning algorithms based on weight
	adaptation after each exemplar presentation (such as the long term
	potentiation found in the mammalian nervous system) cannot in principle
	extract symbolic knowledge from positive examples as prescribed by
	prevailing human linguistic theory and evolutionary psychology
['associationist neural network', 'learns', 'abstract rules', 'neural network', 'state-space', 'symbolic knowledge', 'cognitive neurosciences', 'associationist learning']
['new symbol set', 'factor abstract rule', 'neural network', 'associationist neural network', 'simple associationist neural', 'symbol set', 'similar grammar', 'new symbol', 'input symbol', 'unseen symbol']

Streaming, disruptive interference and power-law behavior in the exit dynamics
	of confined pedestrians
We analyze the exit dynamics of pedestrians who are initially confined in a
	room. Pedestrians are modeled as cellular automata and compete to
	escape via a known exit at the soonest possible time. A pedestrian
	could move forward, backward, left or right within each iteration time
	depending on adjacent cell vacancy and in accordance with simple rules
	that determine the compulsion to move and physical capability relative
	to his neighbors. The arching signatures of jamming were observed and
	the pedestrians exited in bursts of various sizes. Power-law behavior
	is found in the burst-size frequency distribution for exit widths w
	greater than one cell dimension (w > 1). The slope of the power-law
	curve varies with w from -1.3092 (w = 2) to -1.0720 (w = 20). Streaming
	which is a diffusive behavior, arises in large burst sizes and is more
	likely in a single-exit room with w = 1 and leads to a counterintuitive
	result wherein an average exit throughput Q is obtained that is higher
	than with w = 2, 3, or 4. For a two-exit room (w = 1), Q is not greater
	than twice the yield of a single-exit room. If the doors are not
	separated far enough (< 4w), Q becomes even significantly less due
	to a collective slow-down that emerges among pedestrians crossing in
	each other's path (disruptive interference effect). For the same w and
	door number, Q is also higher with relaxed pedestrians than with
	anxious ones
['streaming', 'cellular automata', 'iteration time', 'adjacent cell vacancy', 'archingsignatures', 'jamming', 'burst-size frequency distribution', 'collectiveslow-down', 'self-organised criticality', 'disruptive interference', 'power-law behavior', 'exit dynamics', 'confined pedestrians']
['average exit throughput q', 'exit dynamic', 'law behavior', 'size frequency distribution', 'large burst size', 'physical capability relative', 'adjacent cell vacancy', 'soon possible time', 'disruptive interference', 'pedestrian']

The influence of tollbooths on highway traffic
We study the effects of tollbooths on the traffic flow. The highway traffic is
	simulated by the Nagel-Schreckenberg model. Various types of toll
	collection are examined, which can be characterized either by a waiting
	time or a reduced speed. A first-order phase transition is observed.
	The phase separation results a saturated flow, which is observed as a
	plateau region in the fundamental diagram. The effects of lane
	expansion near the tollbooth are examined. The full capacity of a
	highway can be restored. The emergence of vehicle queuing is studied.
	Besides the numerical results, we also obtain analytical expressions
	for various quantities. The numerical simulations can be well described
	by the analytical formulas. We also discuss the influence on the travel
	time and its variance. The tollbooth increases the travel time but
	decreases its variance. The differences between long- and
	short-distance travelers are also discussed
['highway traffic', 'tollbooths', 'nagel-schreckenberg model', 'toll collection', 'waiting time', 'reduced speed', 'first-order phase transition', 'saturatedflow', 'lane expansion', 'vehicle queuing', 'numerical simulations']
['order phase transition', 'highway traffic', 'tollbooth', 'fundamental diagram', 'various type', 'analytical formula', 'numerical simulation', 'traffic flow', 'plateau region', 'saturated flow']

The Bagsik Oscillator without complex numbers
We argue that the analysis of the so-called Bagsik Oscillator, recently
	published by Piotrowski and Sladkowski (2001), is erroneous due to: (1)
	the incorrect banking data used and (2) the application of statistical
	mechanism apparatus to processes that are totally deterministic
['bagsik oscillator', 'noncomplex numbers', 'incorrect banking data', 'statisticalmechanism apparatus', 'game theory', 'deterministic processes']
['bagsik oscillator', 'complex number', 'incorrect banking datum', 'mechanism apparatus', 'erroneous', 'sladkowski', 'statistical', 'process', 'piotrowski', 'application']

The variance of firm growth rates: the 'scaling' puzzle
Recent evidence suggests that a power-law relationship exists between a firm's
	size and the variance of its growth rate. The flatness of the relation
	is regarded as puzzling, in that it suggests that large firms are not
	much more stable than small firms. It has been suggested that the
	powerlaw nature of the relationship reflects the presence of some form
	of correlation of growth rates across the firm's constituent
	businesses. Here, it is shown that a model of independent businesses
	which allows for the fact that these businesses vary in size, as
	modelled by a simple 'partitions of integers' model, provides a good
	representation of what is observed empirically
['firm growth rates', 'scaling puzzle', 'power-law', 'flatness', 'correlation', 'constituent businesses', 'partitions of integers model', 'sizedistribution', 'corporate growth']
['growth rate', 'firm growth rate', 'recent evidence', 'law relationship', 'large firm', 'variance', 'small firm', 'powerlaw nature', 'size', 'firm']

Antipersistent Markov behavior in foreign exchange markets
A quantitative check of efficiency in US dollar/Deutsche mark exchange rates is
	developed using high-frequency (tick by tick) data. The antipersistent
	Markov behavior of log-price fluctuations of given size implies, in
	principle, the possibility of a statistical forecast. We introduce and
	measure the available information of the quote sequence, and we show
	how it can be profitable following a particular trading rule
['antipersistent markov behavior', 'foreign exchange markets', 'efficiency', 'usdollar', 'deutsche mark', 'exchange rates', 'high-frequency data', 'log-pricefluctuations', 'statistical forecast', 'quote sequence', 'trading rule', 'shannon entropy', 'forecasting']
['mark exchange rate', 'particular trading rule', 'markov behavior', 'foreign exchange market', 'quantitative check', 'us dollar', 'price fluctuation', 'statistical forecast', 'quote sequence', 'available information']

Stock market dynamics
We elucidate on several empirical statistical observations of stock market
	returns. Moreover, we find that these properties are recurrent and are
	also present in invariant measures of low-dimensional dynamical
	systems. Thus, we propose that the returns are modeled by the first
	Poincare return time of a low-dimensional chaotic trajectory. This
	modeling, which captures the recurrent properties of the return
	fluctuations, is able to predict well the evolution of the observed
	statistical quantities. In addition, it explains the reason for which
	stocks present simultaneously dynamical properties and high
	uncertainties. In our analysis, we use data from the S&P 500 index and
	the Brazilian stock Telebras
['stock market returns', 'empirical statistical observations', 'invariant measures', 'low-dimensional dynamical systems', 'first poincare return time', 'low-dimensional chaotic trajectory', 'statistical quantities', 'brazilianstock', 'econophysics']
['dimensional chaotic trajectory', 'brazilian stock telebras', 'empirical statistical observation', 'stock market dynamic', 'poincare return time', 'stock market', 'dimensional dynamical', 'invariant measure', 'recurrent property', 'property']

Application of nonlinear time series analysis techniques to high-frequency
	currency exchange data
In this work we have applied nonlinear time series analysis to high-frequency
	currency exchange data. The time series studied are the exchange rates
	between the US Dollar and 18 other foreign currencies from within and
	without the Euro zone. Our goal was to determine if their dynamical
	behaviours were in some way correlated. The nonexistence of
	stationarity called for the application of recurrence quantification
	analysis as a tool for this analysis, and is based on the definition of
	several parameters that allow for the quantification of recurrence
	plots. The method was checked using the European Monetary System
	currency exchanges. The results show, as expected, the high correlation
	between the currencies that are part of the Euro, but also a strong
	correlation between the Japanese Yen, the Canadian Dollar and the
	British Pound. Singularities of the series are also demonstrated taking
	into account historical events, in 1996, in the Euro zone
['nonlinear time series', 'high-frequency currency exchange data', 'exchange rates', 'us dollar', 'foreign currencies', 'euro zone', 'stationarity', 'recurrencequantification analysis', 'recurrence plots', 'european monetary system', 'japanese yen', 'canadian dollar', 'british pound', 'historical events', 'econophysics', 'nonlinear dynamics']
['nonlinear time series analysis technique', 'currency exchange datum', 'euro zone', 'currency exchange', 'high', 'application', '-PRON- goal', 'european monetary system', 'recurrence quantification', 'foreign currency']

Modeling daily realized futures volatility with singular spectrum analysis
Using singular spectrum analysis (SSA), we model the realized volatility and
	logarithmic standard deviations of two important futures return series.
	The realized volatility and logarithmic standard deviations are
	constructed following the methodology of Andersen et al. [J. Am. Stat.
	Ass. 96 (2001) 42-55] using intra-day transaction data. We find that
	SSA decomposes the volatility series quite well and effectively
	captures both the market trend (accounting for about 34-38% of the
	total variance in the series) and, more importantly, a number of
	underlying market periodicities. Reliable identification of any
	periodicities is extremely important for options pricing and risk
	management and we believe that SSA can be a useful addition to the
	financial practitioners' toolbox
['daily realized futures volatility', 'singular spectrum analysis', 'ssa', 'logarithmicstandard deviations', 'return series', 'intraday transaction data', 'markettrend', 'market periodicities', 'risk management', 'options pricing', 'financial practitioners', 'econophysics', 'asset return']
['logarithmic standard deviation', 'singular spectrum analysis', 'day transaction datum', 'andersen et al', 'ssa', 'volatility', 'future volatility', 'important future', 'j. am', 'series']

Phase control of higher-order squeezing of a quantum field
In a recent experiment [Phys. Rev. Lett. 88 (2002) 023601], phase-dependent
	photon statistics in a c.w. system has been observed in the mixing of a
	coherent field with a two-photon source. Their system has the advantage
	over other atomic transition-based fluorescent systems. In this paper,
	we examine further the squeezing properties of higher-order quantum
	fluctuations in one of the quadrature components of the combined field
	in this system. We demonstrate that efficient and lasting higher-order
	squeezing effects could be observed with proper choice of the relative
	phase between the pump and coherent fields. This nonclassical feature
	is attributed to a constructive two-photon interference. Relationship
	between the second- and higher-order squeezing of the field is
	discussed
['phase control', 'higher-order squeezing', 'quantum field', 'phase-dependent photonstatistics', 'coherent field mixing', 'atomic transition-based fluorescentsystems', 'quantum fluctuations', 'two-photon interference']
['order squeezing', 'coherent field', 'quantum field', 'rev. lett', 'recent experiment', '-PRON- system', 'photon statistic', 'fluorescent system', 'photon source', 'atomic transition']

Modeling self-consistent multi-class dynamic traffic flow
In this study, we present a systematic self-consistent multiclass multilane
	traffic model derived from the vehicular Boltzmann equation and the
	traffic dispersion model. The multilane domain is considered as a
	two-dimensional space and the interaction among vehicles in the domain
	is described by a dispersion model. The reason we consider a multilane
	domain as a two-dimensional space is that the driving behavior of road
	users may not be restricted by lanes, especially motorcyclists. The
	dispersion model, which is a nonlinear Poisson equation, is derived
	from the car-following theory and the equilibrium assumption. Under the
	concept that all kinds of users share the finite section, the density
	is distributed on a road by the dispersion model. In addition, the
	dynamic evolution of the traffic flow is determined by the systematic
	gas-kinetic model derived from the Boltzmann equation. Multiplying
	Boltzmann equation by the zeroth, first- and second-order moment
	functions, integrating both side of the equation and using chain rules,
	we can derive continuity, motion and variance equation, respectively.
	However, the second-order moment function, which is the square of the
	individual velocity, is employed by previous researches does not have
	physical meaning in traffic flow
['self-consistent multiclass dynamic traffic flow modeling', 'multilane trafficmodel', 'vehicular boltzmann equation', 'traffic dispersion model', 'roadusers', 'nonlinear poisson equation', 'car-following theory', 'dynamicevolution', 'variance equation', 'motion equation', 'poisson equation']
['class dynamic traffic flow', 'dispersion model', 'traffic dispersion model', 'vehicular boltzmann equation', 'boltzmann equation', 'traffic flow', 'consistent multiclass multilane', 'dimensional space', 'nonlinear poisson equation', 'multilane domain']

Mixture of experts classification using a hierarchical mixture model
A three-level hierarchical mixture model for classification is presented that
	models the following data generation process: (1) the data are
	generated by a finite number of sources (clusters), and (2) the
	generation mechanism of each source assumes the existence of individual
	internal class-labeled sources (subclusters of the external cluster).
	The model estimates the posterior probability of class membership
	similar to a mixture of experts classifier. In order to learn the
	parameters of the model, we have developed a general training approach
	based on maximum likelihood that results in two efficient training
	algorithms. Compared to other classification mixture models, the
	proposed hierarchical model exhibits several advantages and provides
	improved classification performance as indicated by the experimental
	results
['hierarchical mixture model', 'classification', 'data generation process', 'bayesclassifier', 'experts classifier', 'posterior probability of classmembership']
['level hierarchical mixture model', 'hierarchical mixture model', 'data generation process', 'general training approach', 'expert classification', 'classification mixture model', 'class membership', 'expert classifier', 'internal class', 'external cluster']

eMarketing: restaurant Web sites that click
A number of global companies have adopted electronic commerce as a means of
	reducing transaction related expenditures, connecting with current and
	potential customers, and enhancing revenues and profitability. If a
	restaurant is to have an Internet presence, what aspects of the
	business should be highlighted? Food service companies that have
	successfully ventured onto the web have employed assorted web-based
	technologies to create a powerful marketing tool of unparalleled
	strength. Historically, it has been difficult to create a set of
	criteria against which to evaluate website effectiveness. As
	practitioners consider additional resources for website development,
	the effectiveness of e-marketing investment becomes increasingly
	important. Care must be exercised to ensure that the quality of the
	site adheres to high standards and incorporates evolving technology, as
	appropriate. Developing a coherent website strategy, including an
	effective website design, are proving critical to an effective web
	presence
['e-marketing', 'restaurant web sites', 'electronic commerce', 'internet presence', 'foodservice companies', 'revenues', 'profitability']
['powerful marketing tool', 'effective website design', 'restaurant web site', 'food service company', 'coherent website strategy', 'electronic commerce', 'potential customer', 'internet presence', 'global company', 'assorted web']

Exploring developments in Web based relationship marketing within the hotel
	industry
This paper provides a content analysis study of the application of World Wide
	Web marketing by the hotel industry. There is a lack of historical
	perspective on industry related Web marketing applications and this
	paper attempts to resolve this with a two-year follow-up case study of
	the changing use of the Web to develop different types of
	relationships. Specifically, the aims are: (1) to identify key changes
	in the way hotels are using the Web; (2) to look for evidence of the
	adoption of a relationship marketing (RM) model as a strategy for the
	development of hotel Web sites and the use of new technologies; and,
	(3) To investigate the use of multimedia in hotel Web sites. The
	development and strategic exploitation of the Internet has transformed
	the basis of marketing. Using the evidence from a Web content survey
	this study reveals the way relationships are being created and managed
	within the hotel industry by its use of the Web as a marketing tool.
	The authors have collected evidence by means of a descriptive study on
	the way hotels build and create relationships with their Web presence
	delivering multimedia information as well as channel and interactive
	means of communication. In addition a strategic framework is offered as
	the means to describe the mechanism and orientation of Web based
	marketing by hotels. The study utilizes a model by Gilbert (1996) as a
	means of developing a measurement instrument to allow a content
	analysis of the current approach by hotels to the development of Web
	sites. The results indicate hotels are aware of the new uses of Web
	technology and are promoting hotel products in the global electronic
	market in new and sophisticated ways
['web based relationship marketing', 'hotel industry', 'world wide web marketing', 'hotel web sites', 'multimedia', 'web content survey', 'global electronicmarket']
['hotel web site', 'relationship marketing', 'content analysis study', 'hotel industry', 'web marketing', 'way hotel', 'development', 'web', 'marketing application', 'paper attempt']

Online auctions: dynamic pricing and the lodging industry
The traditional channels of distribution for overnight accommodation are
	rapidly being displaced by Web site scripting, online intermediaries,
	and specialty brokers. Businesses that pioneered Internet usage relied
	on it as a sales and marketing alternative to predecessor product
	distribution channels. As such, Web sites replace the traditional
	trading model to the Internet. Web-enabled companies are popular
	because the medium renders the process faster, less costly, highly
	reliable, and secure. Auction-based models impact business models by
	converting the price setting mechanism from supplier-centric to
	market-centric and transforming the trading model from "one to many" to
	"many to many." Historically, pricing was based on the cost of
	production plus a margin of profit. Traditionally, as products and
	services move through the supply chain, from the producer to the
	consumer, various intermediaries added their share of profit to the
	price. As Internet based mediums of distribution become more prevalent,
	traditional pricing models are being supplanted with dynamic pricing. A
	dynamic pricing model represents a flexible system that changes prices
	not only from product to product, but also from customer to customer
	and transaction to transaction. Many industry leaders are skeptical of
	the long run impact of online auctions on lodging industry profit
	margins, despite the fact pricing theory suggests that an increase in
	the flow of information results in efficient market pricing. The future
	of such endeavors remains promising, but controversial
['online auctions', 'dynamic pricing', 'lodging industry', 'overnight accommodations', 'web site scripting', 'online intermediaries', 'specialty brokers', 'internetusage', 'sales', 'marketing', 'trading model', 'business models', 'price settingmechanism', 'supply chain']
['dynamic pricing', 'online auction', 'web site', 'trading model', 'site scripting', 'online intermediary', 'specialty broker', 'internet usage', 'traditional channel', 'overnight accommodation']

Affine invariants of convex polygons
In this correspondence, we prove that the affine invariants, for image
	registration and object recognition, proposed recently by Yang and
	Cohen (see ibid., vol.8, no.7, p.934-46, July 1999) are algebraically
	dependent. We show how to select an independent and complete set of the
	invariants. The use of this new set leads to a significant reduction of
	the computing complexity without decreasing the discrimination power
['affine invariants', 'convex polygons', 'algebraically dependent. invariants', 'complexity reduction', 'image registration', 'object recognition', 'convexquadruplet', 'feature vector']
['discrimination power', 'significant reduction', 'object recognition', 'convex polygon', 'new set', 'complete set', 'affine invariant', 'invariant', 'affine', 'p.934']

Real-time implementation of a new low-memory SPIHT image coding algorithm using
	DSP chip
Among all algorithms based on wavelet transform and zerotree quantization, Said
	and Pearlman's (1996) set partitioning in hierarchical trees (SPIHT)
	algorithm is well-known for its simplicity and efficiency. This paper
	deals with the real-time implementation of SPIHT algorithm using DSP
	chip. In order to facilitate the implementation and improve the codec's
	performance, some relative issues are thoroughly discussed, such as the
	optimization of program structure to speed up the wavelet
	decomposition. SPIHT's high memory requirement is a major drawback for
	hardware implementation. In this paper, we modify the original SPIHT
	algorithm by presenting two new concepts-number of error bits and
	absolute zerotree. Consequently, the memory cost is significantly
	reduced. We also introduce a new method to control the coding process
	by number of error bits. Our experimental results show that the
	implementation meets common requirement of real-time video coding and
	is proven to be a practical and efficient DSP solution
['spiht algorithm', 'real-time implementation', 'wavelet transform', 'zerotreequantization', 'codec', 'wavelet decomposition', 'number of error bits', 'absolute zerotree', 'dsp chip', 'set partitioning in hierarchical trees', 'memory cost reduction', 'video coding']
['memory spiht image coding algorithm', 'time implementation', 'high memory requirement', 'error bit', 'real', 'new low', 'dsp chip', 'wavelet transform', 'implementation', 'hierarchical tree']

Efficient computation of local geometric moments
Local moments have attracted attention as local features in applications such
	as edge detection and texture segmentation. The main reason for this is
	that they are inherently integral-based features, so that their use
	reduces the effect of uncorrelated noise. The computation of local
	moments, when viewed as a neighborhood operation, can be interpreted as
	a convolution of the image with a set of masks. Nevertheless, moments
	computed inside overlapping windows are not independent and convolution
	does not take this fact into account. By introducing a matrix
	formulation and the concept of accumulation moments, this paper
	presents an algorithm which is computationally much more efficient than
	convolving and yet as simple
['local geometric moments computation', 'local features', 'edge detection', 'texturesegmentation', 'integral-based features', 'neighborhood operation', 'imageconvolution', 'overlapping windows', 'matrix formulation', 'accumulationmoments', 'computationally efficient algorithm', 'image analysis']
['local geometric moment', 'efficient computation', 'main reason', 'texture segmentation', 'local feature', 'local moment', 'edge detection', 'efficient', 'uncorrelated noise', 'neighborhood operation']

Adaptive image denoising using scale and space consistency
This paper proposes a new method for image denoising with edge preservation,
	based on image multiresolution decomposition by a redundant wavelet
	transform. In our approach, edges are implicitly located and preserved
	in the wavelet domain, whilst image noise is filtered out. At each
	resolution level, the image edges are estimated by gradient magnitudes
	(obtained from the wavelet coefficients), which are modeled
	probabilistically, and a shrinkage function is assembled based on the
	model obtained. Joint use of space and scale consistency is applied for
	better preservation of edges. The shrinkage functions are combined to
	preserve edges that appear simultaneously at several resolutions, and
	geometric constraints are applied to preserve edges that are not
	isolated. The proposed technique produces a filtered version of the
	original image, where homogeneous regions appear separated by
	well-defined edges. Possible applications include image
	presegmentation, and image denoising
['adaptive image denoising', 'scale consistency', 'space consistency', 'edgepreservation', 'image multiresolution decomposition', 'redundant wavelettransform', 'image edges', 'gradient magnitudes', 'shrinkage function', 'geometric constraints', 'edge enhancement']
['image multiresolution decomposition', 'shrinkage function', 'scale consistency', 'good preservation', 'joint use', 'preserve edge', 'geometric constraint', 'wavelet coefficient', 'gradient magnitude', 'edge preservation']

Tracking nonparameterized object contours in video
We propose a new method for contour tracking in video. The inverted distance
	transform of the edge map is used as an edge indicator function for
	contour detection. Using the concept of topographical distance, the
	watershed segmentation can be formulated as a minimization. This new
	viewpoint gives a way to combine the results of the watershed algorithm
	on different surfaces. In particular, our algorithm determines the
	contour as a combination of the current edge map and the contour,
	predicted from the tracking result in the previous frame. We also show
	that the problem of background clutter can be relaxed by taking the
	object motion into account. The compensation with object motion allows
	to detect and remove spurious edges in background. The experimental
	results confirm the expected advantages of the proposed method over the
	existing approaches
['contour tracking', 'nonparameterized object contours', 'edge indicator function', 'topographical distance', 'watershed segmentation', 'minimization', 'background clutter', 'object motion', 'motion analysis', 'video', 'inverteddistance transform', 'edge map', 'motion estimation', 'edge detection']
['current edge map', 'edge indicator function', 'edge map', 'object motion', 'watershed segmentation', 'topographical distance', 'contour detection', 'different surface', 'inverted distance', 'watershed algorithm']

Multilayered image representation: application to image compression
The main contribution of this work is a new paradigm for image representation
	and image compression. We describe a new multilayered representation
	technique for images. An image is parsed into a superposition of
	coherent layers: piecewise smooth regions layer, textures layer, etc.
	The multilayered decomposition algorithm consists in a cascade of
	compressions applied successively to the image itself and to the
	residuals that resulted from the previous compressions. During each
	iteration of the algorithm, we code the residual part in a lossy way:
	we only retain the most significant structures of the residual part,
	which results in a sparse representation. Each layer is encoded
	independently with a different transform, or basis, at a different
	bitrate, and the combination of the compressed layers can always be
	reconstructed in a meaningful way. The strength of the multilayer
	approach comes from the fact that different sets of basis functions
	complement each others: some of the basis functions will give
	reasonable account of the large trend of the data, while others will
	catch the local transients, or the oscillatory patterns. This
	multilayered representation has a lot of beautiful applications in
	image understanding, and image and video coding. We have implemented
	the algorithm and we have studied its capabilities
['image compression', 'multilayered representation', 'image representation', 'piecewisesmooth regions layer', 'textures layer', 'multilayered decompositionalgorithm', 'residual part', 'sparse representation', 'basis functions', 'wavelet transforms', 'cosine transforms', 'transform coding']
['multilayered decomposition algorithm', 'smooth region layer', 'image representation', 'new multilayered representation', 'image compression', 'multilayered representation', 'image', 'main contribution', 'new paradigm', 'coherent layer']

Combining spatial and scale-space techniques for edge detection to provide a
	spatially adaptive wavelet-based noise filtering algorithm
New methods for detecting edges in an image using spatial and scale-space
	domains are proposed. A priori knowledge about geometrical
	characteristics of edges is used to assign a probability factor to the
	chance of any pixel being on an edge. An improved double thresholding
	technique is introduced for spatial domain filtering. Probabilities
	that pixels belong to a given edge are assigned based on pixel
	similarity across gradient amplitudes, gradient phases and edge
	connectivity. The scale-space approach uses dynamic range compression
	to allow wavelet correlation over a wider range of scales. A
	probabilistic formulation is used to combine the results obtained from
	filtering in each domain to provide a final edge probability image
	which has the advantages of both spatial and scale-space domain
	methods. Decomposing this edge probability image with the same wavelet
	as the original image permits the generation of adaptive filters that
	can recognize the characteristics of the edges in all wavelet detail
	and approximation images regardless of scale. These matched filters
	permit significant reduction in image noise without contributing to
	edge distortion. The spatially adaptive wavelet noise-filtering
	algorithm is qualitatively and quantitatively compared to a frequency
	domain and two wavelet based noise suppression algorithms using both
	natural and computer generated noisy images
['spatial techniques', 'scale-space techniques', 'edge detection', 'spatially adaptivewavelet-based noise filtering algorithm', 'a priori knowledge', 'geometrical characteristics', 'probability factor', 'double thresholdingtechnique', 'spatial domain filtering', 'pixel similarity', 'gradientamplitudes', 'gradient phases', 'edge connectivity', 'dynamic rangecompression', 'wavelet correlation', 'probabilistic formulation', 'final edgeprobability image', 'adaptive filters', 'approximation images', 'matchedfilters', 'image noise', 'spatially adaptive wavelet noise-filteringalgorithm', 'noise suppression']
['final edge probability image', 'edge probability image', 'dynamic range compression', 'spatial domain filtering', 'noise filtering algorithm', 'improved double thresholding', 'adaptive wavelet', 'scale', 'new method', 'permit significant reduction']

Computational capacity of an odorant discriminator: the linear separability of
	curves
We introduce and study an artificial neural network inspired by the
	probabilistic receptor affinity distribution model of olfaction. Our
	system consists of N sensory neurons whose outputs converge on a single
	processing linear threshold element. The system's aim is to model
	discrimination of a single target odorant from a large number p of
	background odorants within a range of odorant concentrations. We show
	that this is possible provided p does not exceed a critical value p/sub
	c/ and calculate the critical capacity alpha c=p/sub c//N. The critical
	capacity depends on the range of concentrations in which the
	discrimination is to be accomplished. If the olfactory bulb may be
	thought of as a collection of such processing elements, each
	responsible for the discrimination of a single odorant, our study
	provides a quantitative analysis of the potential computational
	properties of the olfactory bulb. The mathematical formulation of the
	problem we consider is one of determining the capacity for linear
	separability of continuous curves, embedded in a large-dimensional
	space. This is accomplished here by a numerical study, using a method
	that signals whether the discrimination task is realizable, together
	with a finite-size scaling analysis
['artificial neural network', 'receptor affinity distribution', 'olfaction', 'linearthreshold element', 'sensory neurons', 'linear separability', 'odorantdiscriminator']
['olfactory bulb', 'discrimination task', 'numerical study', 'processing element', 'continuous curve', 'potential computational', 'mathematical formulation', 'single odorant', 'quantitative analysis', 'sub c//n.']

Lossy to lossless object-based coding of 3-D MRI data
We propose a fully three-dimensional (3-D) object-based coding system
	exploiting the diagnostic relevance of the different regions of the
	volumetric data for rate allocation. The data are first decorrelated
	via a 3-D discrete wavelet transform. The implementation via the
	lifting steps scheme allows to map integer-to-integer values, enabling
	lossless coding, and facilitates the definition of the object-based
	inverse transform. The coding process assigns disjoint segments of the
	bitstream to the different objects, which can be independently accessed
	and reconstructed at any up-to-lossless quality. Two fully 3-D coding
	strategies are considered: embedded zerotree coding (EZW-3D) and
	multidimensional layered zero coding (MLZC), both generalized for
	region of interest (ROI)-based processing. In order to avoid artifacts
	along region boundaries, some extra coefficients must be encoded for
	each object. This gives rise to an overheading of the bitstream with
	respect to the case where the volume is encoded as a whole. The amount
	of such extra information depends on both the filter length and the
	decomposition depth. The system is characterized on a set of head
	magnetic resonance images. Results show that MLZC and EZW-3D have
	competitive performances. In particular, the best MLZC mode outperforms
	the others state-of-the-art techniques on one of the datasets for which
	results are available in the literature
['lossy object-based coding', 'lossless object-based coding', '3-d mri data', 'diagnostic relevance', 'volumetric data', 'rate allocation', '3-d discretewavelet transform', 'lifting steps scheme', 'integer-to-integer values', 'object-based inverse transform', 'disjoint segments', 'bitstream', 'embeddedzerotree coding', 'ezw-3d', 'multidimensional layered zero coding', 'nmzq', 'region of interest-based processing', 'roi-based processing', 'regionboundaries', 'filter length', 'decomposition depth', 'head magnetic resonanceimages']
['good mlzc mode', 'discrete wavelet transform', 'magnetic resonance image', 'object', 'different object', 'zerotree coding', 'lossless quality', 'disjoint segment', 'extra coefficient', 'coding process']

Optimal linear control in stabilizer design
The most common method of improving stability of the power system is the
	synthesis of the turbine and generator control systems, because of the
	high effectiveness and relatively low cost of these elements. The
	synthesis and construction of the effective synchronous generator and
	turbine controller is a very difficult task. This paper describes the
	seven step mu -synthesis approach to PSS design enabling the
	synchronous generator to remain stable over a wide range of system
	operating conditions
['mu -synthesis approach', 'pss design', 'optimal linear control', 'synchronousgenerator control system synthesis', 'turbine control system synthesis']
['step mu -synthesis approach', 'effective synchronous generator', 'optimal linear control', 'generator control system', 'synchronous generator', 'power system', 'common method', 'low cost', 'turbine controller', 'stabilizer design']

Verifying resonant grounding in distribution systems
The authors describe RESFAL, a software tool that can check on the behavior of
	distribution network resonant grounding systems with regard to
	compensation coil tuning and to fault detection
['resfal software tool', 'resonant grounding systems', 'compensation coil tuning', 'fault detection', 'computer simulation', 'power distribution systems']
['compensation coil tuning', 'distribution network resonant', 'software tool', 'distribution system', 'resonant grounding', 'system', 'resfal', 'author', 'behavior', 'regard']

Power electronics spark new simulation challenges
This article discusses some of the changes that have taken place in power
	systems and explores some of the inherent requirements for simulation
	technologies in order to keep up with this rapidly changing
	environment. The authors describe how energy utilities are realizing
	that, with the appropriate tools, they can train and sustain engineers
	who can maintain a great insight into system dynamics
['power system computer simulation', 'power electronics', 'simulation challenges', 'simulation technologies', 'electric utilities']
['new simulation challenge', 'power electronic', 'inherent requirement', 'power', 'energy utility', 'appropriate tool', 'system', 'order', 'author', 'technology']

Deriving model parameters from field test measurements [generator control
	simulation]
A major component of any power system simulation is the generating plant. The
	purpose of DeriveAssist is to speed up the parameter derivation process
	and to allow engineers less versed in parameter matching and
	identification to get involved in the process of power plant electric
	generator modelling
['deriveassist', 'parameter derivation process', 'parameter matching', 'parameteridentification', 'power system simulation', 'turbine/governor', 'power systemstability analysis', 'computer simulation', 'generator parameter derivationprocess', 'steady-state parameters derivation', 'control simulation']
['power plant electric', 'parameter derivation process', 'power system simulation', 'field test measurement', 'major component', 'generator control', 'model parameter', 'deriveassist', 'identification', 'engineer']

Prospective on computer applications in power
The so-called "deregulation" and restructuring of the electric power industry
	have made it very difficult to keep up with industry changes and have
	made it much more difficult to envision the future. In this article,
	current key issues and major developments of the past few years are
	reviewed to provide perspective, and prospects for future computer
	applications in power are suggested. Technology changes are occurring
	at an exponential rate. The interconnected bulk electric systems are
	becoming integrated with vast networked information systems. This
	article discusses the skills that will be needed by future power
	engineers to keep pace with these developments and trends
['electric power industry deregulation', 'computer applications', 'electricityindustry restructuring', 'technology changes', 'interconnected bulkelectric systems', 'networked information systems']
['vast networked information system', 'past few year', 'interconnected bulk electric system', 'current key issue', 'electric power industry', 'computer application', 'industry change', 'application', 'major development', 'future computer']

Control centers are here to stay
Despite changes with different structures, market rules, and uncertainties, a
	control center must always be in place to maintain the security,
	reliability, and quality of electric service. This article focuses on
	the energy management system (EMS) control center, identifying the
	major functions that have become standard components of every
	application software package. The two most important control center
	functions, security control and load-following control, guarantee the
	continuity of electric service, which after all, is the end-product of
	the utility business. New technology trends in the design of control
	center infrastructures are emerging in the liberalized environment of
	the energy market. An example of a control center infrastructure is
	described. The article ends with a concern for the security of the
	control center itself
['ems control centers', 'energy management system', 'standard components', 'applicationsoftware package', 'security control', 'load-following control', 'electricservice continuity', 'control center infrastructures', 'liberalizedenvironment', 'energy market']
['control center', 'control center infrastructure', 'new technology trend', 'important control center', 'market rule', 'different structure', 'electric service', 'application software package', 'energy management system', 'major function']

Virus hunting
We all appreciate the need for, and hopefully we have all deployed, anti-virus
	software. The good news is that AV software has come a long way fast.
	Four or so years ago it was true to write that AV software could not
	detect Trojan Horses and similar intrusion attempts. Now it can and
	does. McAfee's VirusScan, for example, goes one further; it detects
	viruses, worms and Trojan Horses and deploys itself as a firewall to
	filter data packets, control access to Internet resources, activate
	rule sets for specific applications, in general to protect against
	hackers. But like so much software, we use it with little thought as to
	how it came to do its job. Behind the scenes there is an army of top
	notch programmers trying to stay ahead of the baddies who, at the last
	count, had produced some 60,000 viruses
['anti-virus software', 'programmers', 'worms', 'trojan horses']
['filter datum packet', 'similar intrusion attempt', 'av software', 'trojan horses', 'virus', 'long way', 'virus hunting', 'specific application', 'good news', 'rule set']

Integration is key - an introduction to enterprise application integration
	(EAI) technology
Over the past few years, numerous organisations have invested in the latest
	software applications to drive their business forward. But many are now
	finding that these systems are becoming redundant on their own. The key
	to staying ahead of the competition in today's current climate is now
	to integrate all of these systems, says Justin Opie, Portfolio Director
	at Imark Communications
['enterprise application integration', 'imark communications']
['past few year', 'enterprise application integration', 'key', 'software application', 'numerous organisation', 'system', 'introduction', 'technology', 'eai', 'current climate']

Managing system risk
Companies are increasingly required to provide assurance that their systems are
	secure and conform to commercial security standards. Senior business
	managers are ultimately responsible for the security of their corporate
	systems and for the implications in the event of a failure. Businesses
	will be exposed to unquantified security risks unless they have a
	formal risk management framework in place to enable risks to be
	identified, evaluated and managed. Failure to assess and manage risks
	can lead to a business suffering serious financial impacts, commercial
	embarrassment and fines or sanctions from regulators. This is both a
	key responsibility and opportunity for Management Services
	Practitioners
['risk management framework', 'commercial security standards', 'it projects']
['formal risk management framework', 'commercial security standard', 'unquantified security risk', 'system risk', 'senior business', 'system', 'business', 'failure', 'financial impact', 'company']

On optimality in auditory information processing
We study limits for the detection and estimation of weak sinusoidal signals in
	the primary part of the mammalian auditory system using a stochastic
	Fitzhugh-Nagumo model and an action-recovery model for synaptic
	depression. Our overall model covers the chain from a hair cell to a
	point just after the synaptic connection with a cell in the cochlear
	nucleus. The information processing performance of the system is
	evaluated using so-called phi -divergences from statistics that
	quantify "dissimilarity" between probability measures and are
	intimately related to a number of fundamental limits in statistics and
	information theory (IT). We show that there exists a set of parameters
	that can optimize several important phi -divergences simultaneously and
	that this set corresponds to a constant quiescent firing rate (QFR) of
	the spiral ganglion neuron. The optimal value of the QFR is frequency
	dependent but is essentially independent of the amplitude of the signal
	(for small amplitudes). Consequently, optimal processing according to
	several standard IT criteria can be accomplished for this model if and
	only if the parameters are "tuned" to values that correspond to one and
	the same QFR. This offers a new explanation for the QFR and can provide
	new insight into the role played by several other parameters of the
	peripheral auditory system
['weak sinusoidal signals', 'mammalian auditory system', 'stochastic fitzhugh-nagumomodel', 'action-recovery model', 'peripheral auditory system', 'quiescentfiring rate', 'spiral ganglion neuron', 'brain']
['weak sinusoidal signal', 'auditory information processing', 'auditory system', 'information processing', 'phi -divergence', 'overall model', 'nagumo model', 'recovery model', 'hair cell', '-PRON- overall']

Electrical facility construction work for information network structuring by
	the use of sewage conduits
To confront the advent of the advanced information society, there has been a
	pressing demand for the adjustment of the communications infrastructure
	and the structuring of the information network by utilizing the sewage
	conduits. The City of Tokyo is promoting a project by the name of the
	sewer optical fiber teleway (SOFT) network plan. According to this
	plan, the total distance of the optical fiber network laid in the sewer
	conduits is scheduled to reach about 470 km by the end of March 2000.
	At the final stage, this distance will reach 800 km as a whole. We
	completed the construction work for the information control facilities
	scattered in 11 places inclusive of the Treatment Site S, with the
	intention to adjust and extend the information transmission network
	laid through the above-mentioned optical fiber network, to be used
	exclusively by the Bureau of Sewerage. This construction work is
	described in the paper
['electrical facility construction work', 'information network structuring', 'sewageconduits', 'communications infrastructure', 'tokyo', 'sewer optical fiberteleway network plan', 'information control facilities', 'treatment site s', 'information transmission network', 'bureau of sewerage', 'asynchronoustransmission mode switches', 'atm switches']
['sewer optical fiber teleway', 'electrical facility construction work', 'information network structuring', 'optical fiber network', 'advanced information society', 'information network', 'construction work', 'sewage conduit', 'communication infrastructure', 'pressing demand']

A framework for evaluating the data-hiding capacity of image sources
An information-theoretic model for image watermarking and data hiding is
	presented in this paper. Previous theoretical results are used to
	characterize the fundamental capacity limits of image watermarking and
	data-hiding systems. Capacity is determined by the statistical model
	used for the host image, by the distortion constraints on the data
	hider and the attacker, and by the information available to the data
	hider, to the attacker, and to the decoder. We consider autoregressive,
	block-DCT, and wavelet statistical models for images and compute
	data-hiding capacity for compressed and uncompressed host-image
	sources. Closed-form expressions are obtained under sparse-model
	approximations. Models for geometric attacks and distortion measures
	that are invariant to such attacks are considered
['data-hiding capacity', 'image sources', 'information-theoretic model', 'watermarking', 'capacity limits', 'statistical model', 'distortion constraints', 'autoregressive statistical models', 'block-dct statistical models', 'wavelet statistical models', 'compressed host-image sources', 'uncompressedhost-image sources', 'closed-form expressions', 'sparse-modelapproximations', 'geometric attacks', 'distortion measures']
['wavelet statistical model', 'fundamental capacity limit', 'hiding capacity', 'image watermarking', 'previous theoretical result', 'datum hiding', 'image source', 'theoretic model', 'datum', 'statistical model']

Geometrically invariant watermarking using feature points
This paper presents a new approach for watermarking of digital images providing
	robustness to geometrical distortions. The weaknesses of classical
	watermarking methods to geometrical distortions are outlined first.
	Geometrical distortions can be decomposed into two classes: global
	transformations such as rotations and translations and local
	transformations such as the StirMark attack. An overview of existing
	self-synchronizing schemes is then presented. Theses schemes can use
	periodical properties of the mark, invariant properties of transforms,
	template insertion, or information provided by the original image to
	counter geometrical distortions. Thereafter, a new class of
	watermarking schemes using the image content is presented. We propose
	an embedding and detection scheme where the mark is bound with a
	content descriptor defined by salient points. Three different types of
	feature points are studied and their robustness to geometrical
	transformations is evaluated to develop an enhanced detector. The
	embedding of the signature is done by extracting feature points of the
	image and performing a Delaunay tessellation on the set of points. The
	mark is embedded using a classical additive scheme inside each triangle
	of the tessellation. The detection is done using correlation properties
	on the different triangles. The performance of the presented scheme is
	evaluated after JPEG compression, geometrical attack and
	transformations. Results show that the fact that the scheme is robust
	to these different manipulations. Finally, in our concluding remarks,
	we analyze the different perspectives of such content-based
	watermarking scheme
['geometrically invariant watermarking', 'feature points', 'digital images', 'geometrical distortions', 'global transformations', 'rotations', 'translations', 'local transformations', 'stirmark attack', 'self-synchronizing schemes', 'periodical properties', 'invariantproperties', 'transforms', 'template insertion', 'image content', 'embedding', 'detection scheme', 'content descriptor', 'feature extraction', 'delaunaytessellation', 'additive scheme', 'correlation properties', 'jpegcompression', 'geometrical attack']
['feature point', 'geometrical distortion', 'classical additive scheme', 'watermarking scheme', 'watermarking method', 'counter geometrical distortion', 'new approach', 'digital image', 'stirmark attack', 'theses scheme']

Color plane interpolation using alternating projections
Most commercial digital cameras use color filter arrays to sample red, green,
	and blue colors according to a specific pattern. At the location of
	each pixel only one color sample is taken, and the values of the other
	colors must be interpolated using neighboring samples. This color plane
	interpolation is known as demosaicing; it is one of the important tasks
	in a digital camera pipeline. If demosaicing is not performed
	appropriately, images suffer from highly visible color artifacts. In
	this paper we present a new demosaicing technique that uses
	inter-channel correlation effectively in an alternating-projections
	scheme. We have compared this technique with six state-of-the-art
	demosaicing techniques, and it outperforms all of them, both visually
	and in terms of mean square error
['color plane interpolation', 'alternating projections', 'digital cameras', 'demosaicing', 'color filter arrays', 'color artifacts', 'inter-channelcorrelation']
['most commercial digital camera', 'color plane interpolation', 'color filter array', 'color plane', 'new demosaicing technique', 'digital camera pipeline', 'visible color artifact', 'specific pattern', 'blue color', 'color sample']

A comparison of computational color constancy Algorithms. II. Experiments with
	image data
For pt.I see ibid., vol. 11, no.9, p.972-84 (2002). We test a number of the
	leading computational color constancy algorithms using a comprehensive
	set of images. These were of 33 different scenes under 11 different
	sources representative of common illumination conditions. The
	algorithms studied include two gray world methods, a version of the
	Retinex method, several variants of Forsyth's (1990) gamut-mapping
	method, Cardei et al.'s (2000) neural net method, and Finlayson et
	al.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel
	and Finlayson 2000). We discuss a number of issues in applying color
	constancy ideas to image data, and study in depth the effect of
	different preprocessing strategies. We compare the performance of the
	algorithms on image data with their performance on synthesized data.
	All data used for this study are available online at
	http://www.cs.sfu.ca/~color/data, and implementations for most of the
	algorithms are also available (http://www.cs.sfu.ca/~color/code).
	Experiments with synthesized data (part one of this paper) suggested
	that the methods which emphasize the use of the input data statistics,
	specifically color by correlation and the neural net algorithm, are
	potentially the most effective at estimating the chromaticity of the
	scene illuminant. Unfortunately, we were unable to realize comparable
	performance on real images. Here exploiting pixel intensity proved to
	be more beneficial than exploiting the details of image chromaticity
	statistics, and the three-dimensional (3-D) gamut-mapping algorithms
	gave the best performance
['computational color constancy algorithms', 'images', 'image data', 'illuminationconditions', 'gray world methods', 'retinex method', 'gamut-mapping method', 'neural net method', 'color by correlation method', 'preprocessingstrategies', 'synthesized data', 'input data statistics', 'chromaticity', 'scene illuminant', 'pixel intensity']
['computational color constancy', 'color constancy algorithms', 'image datum', 'neural net', 'finlayson et', 'et al', 'net method', 'correlation method', 'world method', 'illumination condition']

A comparison of computational color constancy algorithms. I: Methodology and
	experiments with synthesized data
We introduce a context for testing computational color constancy, specify our
	approach to the implementation of a number of the leading algorithms,
	and report the results of three experiments using synthesized data.
	Experiments using synthesized data are important because the ground
	truth is known, possible confounds due to camera characterization and
	pre-processing are absent, and various factors affecting color
	constancy can be efficiently investigated because they can be
	manipulated individually and precisely. The algorithms chosen for close
	study include two gray world methods, a limiting case of a version of
	the Retinex method, a number of variants of Forsyth's (1990)
	gamut-mapping method, Cardei et al.'s (2000) neural net method, and
	Finlayson et al.'s color by correlation method (Finlayson et al. 1997,
	2001; Hubel and Finlayson 2000) . We investigate the ability of these
	algorithms to make estimates of three different color constancy
	quantities: the chromaticity of the scene illuminant, the overall
	magnitude of that illuminant, and a corrected, illumination invariant,
	image. We consider algorithm performance as a function of the number of
	surfaces in scenes generated from reflectance spectra, the relative
	effect on the algorithms of added specularities, and the effect of
	subsequent clipping of the data. All data is available on-line at
	http://www.cs.sfu.ca/~color/data, and implementations for most of the
	algorithms are also available (http://www.cs.sfu.ca/~color/code)
['computational color constancy algorithms', 'synthesized data', 'gray world methods', 'retinex method', 'gamut-mapping method', 'neural net method', 'color bycorrelation method', 'chromaticity', 'scene illuminant', 'illuminationinvariant image', 'algorithm performance', 'reflectance spectra', 'specularities', 'clipping']
['finlayson et al', 'computational color constancy', 'color constancy algorithm', 'algorithm', 'neural net method', 'gray world method', 'cardei et al', 'datum', 'experiment', 'number']

Quality image metrics for synthetic images based on perceptual color
	differences
Due to the improvement of image rendering processes, and the increasing
	importance of quantitative comparisons among synthetic color images, it
	is essential to define perceptually based metrics which enable to
	objectively assess the visual quality of digital simulations. In
	response to this need, this paper proposes a new methodology for the
	determination of an objective image quality metric, and gives an answer
	to this problem through three metrics. This methodology is based on the
	LLAB color space for perception of color in complex images, a
	modification of the CIELab1976 color space. The first metric proposed
	is a pixel by pixel metric which introduces a local distance map
	between two images. The second metric associates, to a pair of images,
	a global value. Finally, the third metric uses a recursive subdivision
	of the images to obtain an adaptative distance map, rougher but less
	expensive to compute than the first method
['quality image metrics', 'synthetic images', 'perceptual color differences', 'imagerendering', 'color images', 'perceptually based metrics', 'visual quality', 'digital simulations', 'llab color space', 'cielab1976 color space', 'pixel bypixel metric', 'local distance map', 'global value', 'recursive subdivision', 'adaptative distance map']
['objective image quality metric', 'synthetic color image', 'quality image metric', 'image rendering process', 'llab color space', 'cielab1976 color space', 'second metric associate', 'local distance map', 'perceptual color', 'synthetic image']

Exact controllability of shells in minimal time
We prove an exact controllability result for thin cups using the Fourier method
	and recent improvements of Ingham (1936) type theorems
['controllability', 'shells', 'minimal time', 'thin cups', 'partial differentialequations', 'young modulus', 'hilbert space', 'fourier method', 'ingham typetheorems']
['exact controllability result', 'exact controllability', 'minimal time', 'recent improvement', 'fourier method', 'thin cup', 'type theorem', 'ingham', 'shell']

A friction compensator for pneumatic control valves
A procedure that compensates for static friction (stiction) in pneumatic
	control valves is presented. The compensation is obtained by adding
	pulses to the control signal. The characteristics of the pulses are
	determined from the control action. The compensator is implemented in
	industrial controllers and control systems, and the industrial
	experiences show that the procedure reduces the control error during
	stick-slip motion significantly compared to standard control without
	stiction compensation
['friction compensator', 'pneumatic control valves', 'static friction compensation', 'stiction compensation', 'industrial controllers', 'control error reduction', 'stick-slip motion', 'standard control']
['pneumatic control valve', 'control valve', 'static friction', 'friction compensator', 'procedure', 'control system', 'control signal', 'control action', 'industrial controller', 'compensator']

Performance comparison between PID and dead-time compensating controllers
This paper is intended to answer the question: "When can a simple dead-time
	compensator be expected to perform better than a PID?". The performance
	criterion used is the integrated absolute error (IAE). It is compared
	for PI and PID controllers and a simple dead-time compensator (DTC)
	when a step load disturbance is applied at the plant input. Both stable
	and integrating processes are considered. For a fair comparison the
	controllers should provide equal robustness in some sense. Here, as a
	measure of robustness, the H/sub infinity / norm of the sum of the
	absolute values of the sensitivity function and the complementary
	sensitivity function is used. Performance of the DTC's is given also as
	a function of dead-time margin (D/sub M/)
['performance comparison', 'pid controllers', 'dead-time compensating controllers', 'performance criterion', 'integrated absolute error', 'iae', 'pi controllers', 'dead-time compensator', 'dtc', 'step load disturbance', 'stable processes', 'integrating processes', 'equal robustness', 'complementary sensitivityfunction', 'dead-time margin', 'absolute value sum h/sub infinity / norm']
['step load disturbance', 'time compensating controller', 'simple dead', 'performance', 'performance comparison', 'dead', 'sensitivity function', 'pid', 'absolute error', 'time compensator']

Waiting for the wave to crest [wavelength services]
Wavelength services have been hyped ad nauseam for years. But despite their
	quick turn-up time and impressive margins, such services have yet to
	live up to the industry's expectations. The reasons for this lukewarm
	reception are many, not the least of which is the confusion that still
	surrounds the technology, but most industry observers are still
	convinced that wavelength services with ultimately flourish
['wavelength services', 'fiber optic networks', 'looking glass networks', 'pointeastresearch']
['wavelength service', 'ad nauseam', 'impressive margin', 'quick turn', 'service', 'industry observer', 'industry', 'reception', 'lukewarm', 'reason']

Adaptive state feedback control for a class of linear systems with unknown
	bounds of uncertainties
The problem of adaptive robust stabilization for a class of linear time-varying
	systems with disturbance and nonlinear uncertainties is considered. The
	bounds of the disturbance and uncertainties are assumed to be unknown,
	being even arbitrary. For such uncertain dynamical systems, the
	adaptive robust state feedback controller is obtained. And the
	resulting closed-loop systems are asymptotically stable in theory.
	Moreover, an adaptive robust state feedback control scheme is given.
	The scheme ensures the closed-loop systems exponentially practically
	stable and can be used in practical engineering. Finally, simulations
	show that the control scheme is effective
['robust stabilization', 'adaptive stabilization', 'linear time-varying systems', 'nonlinear uncertainties', 'closed-loop systems', 'uncertain dynamicalsystems', 'state feedback', 'adaptive controller', 'robust control', 'uncertainsystems']
['adaptive robust state feedback control scheme', 'adaptive state feedback control', 'adaptive robust state feedback controller', 'adaptive robust stabilization', 'uncertain dynamical system', 'linear system', 'class', 'nonlinear uncertainty', 'linear time', 'loop system']

Preintegration lateral inhibition enhances unsupervised learning
A large and influential class of neural network architectures uses
	postintegration lateral inhibition as a mechanism for competition. We
	argue that these algorithms are computationally deficient in that they
	fail to generate, or learn, appropriate perceptual representations
	under certain circumstances. An alternative neural network architecture
	is presented here in which nodes compete for the right to receive
	inputs rather than for the right to generate outputs. This form of
	competition, implemented through preintegration lateral inhibition,
	does provide appropriate coding properties and can be used to learn
	such representations efficiently. Furthermore, this architecture is
	consistent with both neuroanatomical and neuropsychological data. We
	thus argue that preintegration lateral inhibition has computational
	advantages over conventional neural network architectures while
	remaining equally biologically plausible
['neural network architectures', 'postintegration lateral inhibition', 'competition', 'preintegration lateral inhibition', 'neural network', 'unsupervisedlearning']
['preintegration lateral inhibition', 'postintegration lateral inhibition', 'neural network architecture', 'appropriate perceptual representation', 'conventional neural network architecture', 'alternative neural network architecture', 'appropriate coding property', 'influential class', 'unsupervised learning', 'competition']

Generalized predictive control for non-uniformly sampled systems
In this paper, we study digital control systems with non-uniform updating and
	sampling patterns, which include multirate sampled-data systems as
	special cases. We derive lifted models in the state-space domain. The
	main obstacle for generalized predictive control (GPC) design using the
	lifted models is the so-called causality constraint. Taking into
	account this design constraint, we propose a new GPC algorithm, which
	results in optimal causal control laws for the non-uniformly sampled
	systems. The solution applies immediately to multirate sampled-data
	systems where rates are integer multiples of some base period
['generalized predictive control design', 'nonuniformly sampled systems', 'digitalcontrol systems', 'nonuniform updating patterns', 'nonuniform samplingpatterns', 'multirate sampled-data systems', 'state-space models', 'gpc', 'causality constraint', 'optimal causal control laws', 'integer multiples']
['optimal causal control law', 'digital control system', 'generalized predictive control', 'new gpc algorithm', 'predictive control', 'non', 'system', 'special case', 'data system', 'uniform updating']

A simple graphic approach for observer decomposition
Based upon the proposition that the roles of inputs and outputs in a physical
	system and those in the corresponding output-injection observer do not
	really have to be consistent, a systematic procedure is developed in
	this work to properly divide a set of sparse system models and
	measurement models into a number of independent subsets with the help
	of a visual aid. Several smaller sub-observers can then be constructed
	accordingly to replace the original one. The size of each sub-observer
	may be further reduced by strategically selecting one or more appended
	states. These techniques are shown to be quite effective in relieving
	on-line computation load of the output-injection observers and also in
	identifying detectable sub-systems
['graphic approach', 'observer decomposition', 'output-injection observer', 'sparsesystem models', 'measurement models', 'independent subsets', 'sub-observers', 'online computation load', 'detectable subsystems']
['line computation load', 'several small sub', 'simple graphic approach', 'sparse system model', 'injection observer', 'output', 'observer', 'observer decomposition', 'systematic procedure', 'independent subset']

A new subspace identification approach based on principal component analysis
Principal component analysis (PCA) has been widely used for monitoring complex
	industrial processes with multiple variables and diagnosing process and
	sensor faults. The objective of this paper is to develop a new subspace
	identification algorithm that gives consistent model estimates under
	the errors-in-variables (EIV) situation. In this paper, we propose a
	new subspace identification approach using principal component
	analysis. PCA naturally falls into the category of EIV formulation,
	which resembles total least squares and allows for errors in both
	process input and output. We propose to use PCA to determine the system
	observability subspace, the matrices and the system order for an EIV
	formulation. Standard PCA is modified with instrumental variables in
	order to achieve consistent estimates of the system matrices. The
	proposed subspace identification method is demonstrated using a
	simulated process and a real industrial process for model
	identification and order determination. For comparison the MOESP
	algorithm and N4SID algorithm are used as benchmarks to demonstrate the
	advantages of the proposed PCA based subspace model identification
	(SMI) algorithm
['subspace identification approach', 'principal component analysis', 'pca', 'complexindustrial process monitoring', 'process fault diagnosis', 'sensor faultdiagnosis', 'errors-in-variables situation', 'eiv situation', 'totalleast-squares approximation', 'system observability subspace', 'consistentsystem matrix estimates', 'moesp algorithm', 'n4sid algorithm', 'subspacemodel identification', 'smi']
['new subspace identification approach', 'principal component analysis', 'pca', 'consistent model estimate', 'industrial process', 'principal component', 'total least square', 'multiple variable', 'sensor fault', 'identification algorithm']

Nonlinear modeling and adaptive fuzzy control of MCFC stack
To improve availability and performance of fuel cells, the operating
	temperature of the molten carbonate fuel cells (MCFC) stack should be
	controlled within a specified range. However, most existing models of
	MCFC are not ready to be applied in synthesis. In the paper, a radial
	basis function neural networks identification model of a MCFC stack is
	developed based on the input-output sampled data. An adaptive fuzzy
	control procedure for the temperature of the MCFC stack is also
	developed. The parameters of the fuzzy control system are regulated by
	back-propagation algorithm, and the rule database of the fuzzy system
	is also adaptively adjusted by the nearest-neighbor-clustering
	algorithm. Finally using the neural networks model of MCFC stack, the
	simulation results of the control algorithm are presented. The results
	show the effectiveness of the proposed modeling and design procedures
	for the MCFC stack based on neural networks identification and the
	novel adaptive fuzzy control
['nonlinear modeling', 'adaptive fuzzy control', 'mcfc stack', 'fuel cells', 'moltencarbonate fuel cells stack', 'radial basis function neural networksidentification model', 'input-output sampled data', 'backpropagationalgorithm', 'rule database', 'nearest-neighbor-clustering algorithm']
['mcfc stack', 'basis function neural network identification model', 'adaptive fuzzy control', 'molten carbonate fuel cell', 'existing model', 'mcfc', 'fuzzy control system', 'temperature', 'control procedure', 'stack']

New paradigms for interactive 3D volume segmentation
We present a new virtual reality-based interaction metaphor for semi-automatic
	segmentation of medical 3D volume data. The mouse-based, manual
	initialization of deformable surfaces in 3D represents a major
	bottleneck in interactive segmentation. In our multi-modal system we
	enhance this process with additional sensory feedback. A 3D haptic
	device is used to extract the centreline of a tubular structure. Based
	on the obtained path a cylinder with varying diameter is generated,
	which in turn is used as the initial guess for a deformable surface
['interactive 3d volume segmentation', 'virtual reality', 'interaction metaphor', 'medical image segmentation', 'mouse', 'deformable surfaces', 'interactivesegmentation', 'multi-modal system', 'sensory feedback', '3d haptic device', 'tubular structure', 'varying diameter cylinder', 'deformable surface', 'haptic interaction']
['medical 3d volume datum', 'interactive 3d volume segmentation', 'new virtual reality', 'deformable surface', 'new paradigms', 'additional sensory feedback', 'interaction metaphor', 'interactive segmentation', 'modal system', '3d haptic']

State-of-the-art in orthopaedic surgical navigation with a focus on medical
	image modalities
This paper presents a review of surgical navigation systems in orthopaedics and
	categorizes these systems according to the image modalities that are
	used for the visualization of surgical action. Medical images used to
	be an essential part of surgical education and documentation as well as
	diagnosis and operation planning over many years. With the recent
	introduction of navigation techniques in orthopaedic surgery, a new
	field of application has been opened. Today surgical navigation systems
	- also known as image-guided surgery systems - are available for
	various applications in orthopaedic surgery. They visualize the
	position and orientation of surgical instruments as graphical overlays
	onto a medical image of the operated anatomy on a computer monitor.
	Preoperative image data such as computed tomography scans or intra
	operatively generated images (for example, ultrasonic, endoscopic or
	fluoroscopic images) are suitable for this purpose. A new category of
	medical images termed 'surgeon-defined anatomy' has been developed that
	exclusively relies upon the usage of navigation technology. Points on
	the anatomy are digitized interactively by the surgeon and are used to
	build up an abstract geometrical model of the bony structures to be
	operated on. This technique may be used when no other image data is
	available or appropriate for a given application
['orthopaedic surgical navigation', 'medical image modalities', 'surgical actionvisualization', 'medical image processing', 'surgical education', 'image-guided surgery systems', 'surgical instruments', 'graphical overlays', 'computer monitor', 'computed tomography scans', 'intra operativelygenerated images', 'surgeon-defined anatomy', 'abstract geometrical model', 'bony structures', 'image registration']
['surgical navigation system', 'today surgical navigation system', 'medical image', 'orthopaedic surgical navigation', 'orthopaedic surgery', 'preoperative image datum', 'surgical education', 'operation planning', 'navigation technique', 'surgery system']

Lung metastasis detection and visualization on CT images: a knowledge-based
	method
A solution to the problem of lung metastasis detection on computed tomography
	(CT) scans of the thorax is presented. A knowledge-based top-down
	approach for image interpretation is used. The method is inspired by
	the manner in which a radiologist and radiotherapist interpret CT
	images before radiotherapy is planned. A two-dimensional followed by a
	three-dimensional analysis is performed. The algorithm first detects
	the thorax contour, the lungs and the ribs, which further help the
	detection of metastases. Thus, two types of tumors are detected:
	nodules and metastases located at the lung extremities. A method to
	visualize the anatomical structures segmented is also presented. The
	system was tested on 20 patients (988 total images) from the Oncology
	Department of La Chaux-de-Fonds Hospital and the results show that the
	method is reliable as a computer-aided diagnostic tool for clinical
	purpose in an oncology department
['lung metastasis detection', 'data visualization', 'ct images', 'computed tomography', 'knowledge-based top-down approach', 'two-dimensional analysis', 'three-dimensional analysis', 'computer-aided diagnostic tool', 'oncology', 'medical imaging', 'knowledge representation', 'thorax', 'image interpretation']
['lung metastasis detection', 'algorithm first detect', 'radiotherapist interpret ct', 'method', 'ct image', 'knowledge', 'image', 'image interpretation', 'dimensional analysis', 'thorax contour']

The creation of a high-fidelity finite element model of the kidney for use in
	trauma research
A detailed finite element model of the human kidney for trauma research has
	been created directly from the National Library of Medicine Visible
	Human Female (VHF) Project data set. An image segmentation and organ
	reconstruction software package has been developed and employed to
	transform the 2D VHF images into a 3D polygonal representation.
	Nonuniform rational B-spline (NURBS) surfaces were then mapped to the
	polygonal surfaces, and were finally utilized to create a robust 3D
	hexahedral finite element mesh within a commercially available meshing
	software. The model employs a combined viscoelastic and hyperelastic
	material model to successfully simulate the behaviour of biological
	soft tissues. The finite element model was then validated for use in
	biomechanical research
['high-fidelity finite element model', 'kidney', 'trauma research', 'national libraryof medicine', 'visible human female project', 'medical data set', 'imagesegmentation', 'organ reconstruction', 'physically based animation', 'software package', '3d polygonal representation', '2d vhf images', 'nonuniform rational b-spline surfaces', 'nurbs', 'polygonal surfaces', '3dhexahedral finite element mesh', 'viscoelastic model', 'hyperelasticmaterial model', 'biological soft tissues', 'biomechanical research']
['hexahedral finite element mesh', 'detailed finite element model', 'fidelity finite element model', 'finite element model', 'nonuniform rational b', 'trauma research', 'reconstruction software package', '3d polygonal representation', '2d vhf image', 'use']

Building 3D anatomical scenes on the Web
We propose a new service for building user-defined 3D anatomical structures on
	the Web. The Web server is connected to a database storing more than
	1000 3D anatomical models reconstructed from the Visible Human. Users
	may combine existing models as well as planar oblique slices in order
	to create their own structured anatomical scenes. Furthermore, they may
	record sequences of scene construction and visualization actions. These
	actions enable the server to construct high-quality video animations,
	downloadable by the user. Professionals and students in anatomy,
	medicine and related disciplines are invited to use the server and
	create their own anatomical scenes
['3d anatomical scenes', 'world wide web', 'user-defined 3d anatomical structures', 'web server', 'database', '3d anatomical models', 'visible human', 'planaroblique slices', 'structured anatomical scenes', 'volume visualization', 'surface reconstruction', 'applet-based rendering engine', 'java', 'visualization', 'scene construction', 'high-quality video animation']
['building 3d anatomical scene', '3d anatomical structure', '3d anatomical model', 'user', 'structured anatomical scene', 'planar oblique slice', 'server', 'web server', 'new service', 'quality video animation']

A survey of interactive mesh-cutting techniques and a new method for
	implementing generalized interactive mesh cutting using virtual tools
In our experience, mesh-cutting methods can be distinguished by how their
	solutions address the following major issues: definition of the cut
	path, primitive removal and re-meshing, number of new primitives
	created, when re-meshing is performed, and representation of the
	cutting tool. Many researchers have developed schemes for interactive
	mesh cutting with the goals of reducing the number of new primitives
	created, creating new primitives with good aspect ratios, avoiding a
	disconnected mesh structure between primitives in the cut path, and
	representing the path traversed by the tool as accurately as possible.
	The goal of this paper is to explain how, by using a very simple
	framework, one can build a generalized cutting scheme. This method
	allows for any arbitrary cut to be made within a virtual object, and
	can simulate cutting surface, layered surface or tetrahedral objects
	using a virtual scalpel, scissors, or loop cautery tool. This method
	has been implemented in a real-time, haptic-rate surgical simulation
	system allowing arbitrary cuts to be made on high-resolution
	patient-specific models
['generalized interactive mesh cutting', 'virtual tools', 'cut path definition', 're-meshing', 'cutting tool', 'disconnected mesh structure', 'virtual object', 'tetrahedral objects', 'layered surface', 'real-time system', 'haptic-ratesurgical simulation system', 'high-resolution patient-specific models', 'rendering', 'haptic interfaces']
['rate surgical simulation', 'disconnected mesh structure', 'loop cautery tool', 'interactive mesh cutting', 'good aspect ratio', 'new primitive', 'mesh cutting', 'virtual tool', 'interactive mesh', 'method']

Correction to construction of panoramic image mosaics with global and local
	alignment
For original paper see ibid., vol. 36, no. 2, p. 101-30 (2000). The authors had
	given a method for the construction of panoramic image mosaics with
	global and local alignment. Unfortunately a mistake had led to an
	incorrect equation which whilst making little difference in many cases,
	for faster (and assured) convergence, the correct formulae given here
	should be used
['panoramic image mosaics', 'global alignment', 'local alignment', 'resampled image']
['panoramic image mosaic', 'construction', 'global', 'original paper', 'local', 'alignment', 'local alignment', 'little difference', 'incorrect equation', 'p.']

Scale-invariant segmentation of dynamic contrast-enhanced perfusion MR images
	with inherent scale selection
Selection of the best set of scales is problematic when developing
	signal-driven approaches for pixel-based image segmentation. Often,
	different possibly conflicting criteria need to be fulfilled in order
	to obtain the best trade-off between uncertainty (variance) and
	location accuracy. The optimal set of scales depends on several
	factors: the noise level present in the image material, the prior
	distribution of the different types of segments, the class-conditional
	distributions associated with each type of segment as well as the
	actual size of the (connected) segments. We analyse, theoretically and
	through experiments, the possibility of using the overall and
	class-conditional error rates as criteria for selecting the optimal
	sampling of the linear and morphological scale spaces. It is shown that
	the overall error rate is optimized by taking the prior class
	distribution in the image material into account. However, a uniform
	(ignorant) prior distribution ensures constant class-conditional error
	rates. Consequently, we advocate for a uniform prior class distribution
	when an uncommitted, scale-invariant segmentation approach is desired.
	Experiments with a neural net classifier developed for segmentation of
	dynamic magnetic resonance (MR) images, acquired with a paramagnetic
	tracer, support the theoretical results. Furthermore, the experiments
	show that the addition of spatial features to the classifier, extracted
	from the linear or morphological scale spaces, improves the
	segmentation result compared to a signal-driven approach based solely
	on the dynamic MR signal. The segmentation results obtained from the
	two types of features are compared using two novel quality measures
	that characterize spatial properties of labelled images
['scale-invariant segmentation', 'dynamic contrast-enhanced perfusion mr images', 'inherent scale selection', 'pixel-based image segmentation', 'noise level', 'class-conditional error rates', 'optimal sampling', 'experiments', 'neuralnet classifier', 'dynamic magnetic resonance images', 'paramagnetic tracer', 'quality measures', 'labelled images', 'class-conditional distributions']
['morphological scale space', 'invariant segmentation', 'image material', 'scale selection', 'good set', 'conditional error', 'error rate', 'mr image', 'inherent scale', 'dynamic contrast']

Innovative phase unwrapping algorithm: hybrid approach
We present a novel algorithm based on a hybrid of the global and local
	treatment of a wrapped map. The proposed algorithm is especially
	effective for the unwrapping of speckle-coded interferogram contour
	maps. In contrast to earlier unwrapping algorithms by region, we
	propose a local discontinuity-restoring criterion to serve as the
	preprocessor or postprocessor of our hybrid algorithm, which makes the
	unwrapping by region much easier and more efficient. With this hybrid
	algorithm, a robust, stable, and especially time effective phase
	unwrapping can be achieved. Additionally, the criterion and limitation
	of this hybrid algorithm are fully described. The robustness,
	stability, and speed of this hybrid algorithm are also studied. The
	proposed algorithm can be easily upgraded with minor modifications to
	solve the unwrapping problem of maps with phase inconsistency. Both
	numerical simulation and experimental applications demonstrate the
	effectiveness of the proposed algorithm
['phase unwrapping algorithm', 'global treatment', 'local treatment', 'wrapped map', 'speckle-coded interferogram contour maps', 'unwrapping algorithms', 'localdiscontinuity-restoring criterion', 'postprocessor', 'hybrid algorithm', 'robust stable time effective phase unwrapping', 'unwrapping problem', 'phase inconsistency', 'numerical simulation', 'interferogram analysis', 'light interferometry']
['hybrid algorithm', 'time effective phase', 'algorithm', 'novel algorithm', 'local discontinuity', 'interferogram contour', 'hybrid approach', 'innovative phase', 'hybrid', 'map']

Strain contouring using Gabor filters: principle and algorithm
Moire interferometry is a powerful technique for high sensitivity in-plane
	deformation contouring. However, from an engineering viewpoint, the
	derivatives of displacement, i.e., strain, are the desired parameter.
	Thus there is a need to differentiate the displacement field. Optical
	and digital methods have been proposed for this differentiation.
	Optical methods provide contours that still need to be quantified,
	while digital methods suffer from drawbacks inherent in the digital
	differentiation process. We describe a novel approach of strain
	segmentation for the moire pattern using a multichannel Gabor filter.
	Appropriate filter design allows for user-specific segmentation, which
	is essentially in engineering design and analysis
['strain contouring', 'gabor filters', 'algorithm', 'moire interferometry', 'highsensitivity in-plane deformation contouring', 'displacement', 'displacementfield', 'digital methods', 'optical methods', 'differentiation', 'digitaldifferentiation process', 'strain segmentation', 'multichannel gaborfilter', 'filter design', 'user-specific segmentation', 'engineering design', 'engineering analysis', 'image segmentation', 'spatial filters']
['gabor filter', 'digital method', 'engineering viewpoint', 'deformation contouring', 'high sensitivity', 'powerful technique', 'moire interferometry', 'appropriate filter design', 'displacement field', 'optical method']

Novel denoising algorithm for obtaining a superresolved position estimation
We present a new algorithm that uses the randomness of the noise pattern to
	achieve high positioning accuracy by applying a modified averaging
	operation. Using the suggested approach, noise sensitivity of the
	positioning accuracy can be significantly reduced. This new improved
	algorithm can improve the performances of tracking systems used for
	military as well as civil applications. The concept is demonstrated
	theoretically as well as by optical experiment
['denoising algorithm', 'superresolved position estimation', 'noise patternrandomness', 'high positioning accuracy', 'modified averaging operation', 'noise sensitivity', 'tracking systems', 'military applications', 'civilapplications', 'optical experiment']
['achieve high positioning accuracy', 'noise pattern', 'new algorithm', 'noise sensitivity', 'position estimation', 'algorithm', 'new improved', 'tracking system', 'civil application', 'randomness']

Adaptive filtering for noise reduction in hue saturation intensity color space
Even though the hue saturation intensity (HSI) color model has been widely used
	in color image processing and analysis, the conversion formulas from
	the RGB color model to HSI are nonlinear and complicated in comparison
	with the conversion formulas of other color models. When an RGB image
	is degraded by random Gaussian noise, this nonlinearity leads to a
	nonuniform noise distribution in HSI, making accurate image analysis
	more difficult. We have analyzed the noise characteristics of the HSI
	color model and developed an adaptive spatial filtering method to
	reduce the magnitude of noise and the nonuniformity of noise variance
	in the HSI color space. With this adaptive filtering method, the filter
	kernel for each pixel is dynamically adjusted, depending on the values
	of intensity and saturation. In our experiments we have filtered the
	saturation and hue components and generated edge maps from color
	gradients. We have found that by using the adaptive filtering method,
	the minimum error rate in edge detection improves by approximately 15%
['adaptive filtering', 'noise reduction', 'hue saturation intensity color space', 'color image processing', 'color image analysis', 'rgb color model', 'randomgaussian noise', 'nonuniform noise distribution', 'accurate image analysis', 'adaptive spatial filtering method', 'nonuniformity', 'noise variance', 'hsicolor space', 'filter kernel', 'pixel', 'saturation', 'intensity', 'generatededge maps', 'color gradients', 'edge detection', 'minimum error rate']
['hue saturation intensity color space', 'color model', 'adaptive spatial filtering method', 'adaptive filtering', 'adaptive filtering method', 'accurate image analysis', 'rgb color model', 'conversion formula', 'nonuniform noise distribution', 'color image processing']

Optical recognition of three-dimensional objects with scale invariance using a
	classical convergent correlator
We present a real-time method for recognizing three-dimensional (3-D) objects
	with scale invariance. The 3-D information of the objects is codified
	in deformed fringe patterns using the Fourier transform profilometry
	technique and is correlated using a classical convergent correlator.
	The scale invariance property is achieved using two different
	approaches: the Mellin radial harmonic decomposition and the
	logarithmic radial harmonic filter. Thus, the method is invariant for
	changes in the scale of the 3-D target within a defined interval of
	scale factors. Experimental results show the utility of the proposed
	method
['optical recognition', '3d object recognition', 'scale invariance', 'classicalconvergent correlator', 'real-time method', '3-d information', 'deformedfringe patterns', 'fourier transform profilometry technique', 'scaleinvariance property', 'mellin radial harmonic decomposition', 'logarithmicradial harmonic filter', 'invariant', 'scale factors']
['classical convergent correlator', 'logarithmic radial harmonic filter', 'scale invariance', 'mellin radial harmonic decomposition', 'scale invariance property', 'time method', 'optical recognition', 'dimensional object', 'fringe pattern', 'method']

Fully automatic algorithm for region of interest location in camera calibration
We present an automatic method for region of interest (ROI) location in camera
	calibration used in computer vision inspection. An intelligent ROI
	location algorithm based on the Radon transform is developed to
	automate the calibration process. The algorithm remains robust even if
	the anchor target has a notable rotation angle in the target plane.
	This method functions well although the anchor target is not carefully
	positioned. Several improvement methods are studied to avoid the
	algorithm's huge time/space consumption problem. The algorithm runs
	about 100 times faster if these improvement methods are applied. Using
	this method fully automatic camera calibration is achieved without
	human interactive ROI specification. Experiments show that this
	algorithm can help to calibrate the intrinsic parameters of the zoom
	lens and the camera parameters quickly and automatically
['fully automatic algorithm', 'interest location', 'camera calibration', 'region ofinterest location', 'computer vision inspection', 'roi location algorithm', 'radon transform', 'calibration process', 'rotation angle', 'time/spaceconsumption problem', 'fully automatic camera calibration', 'humaninteractive specification', 'intrinsic parameters', 'zoom lens', 'cameraparameters']
['human interactive roi specification', 'space consumption problem', 'automatic camera calibration', 'several improvement method', 'notable rotation angle', 'computer vision inspection', 'camera calibration', 'anchor target', 'automatic method', 'algorithm']

Autofocus system for microscope
A technique is developed for microscope autofocusing, which is called the
	eccentric light beam approach with high resolution, wide focusing
	range, and compact construction. The principle is described. The
	theoretical formula of the eccentric light beam approach deduced can be
	applied not only to an object lens whose objective plane is just at the
	focal plane, but also to an object lens whose objective plane is not at
	the focal plane. The experimental setup uses a semiconductor laser
	device as the light source. The laser beam that enters into the
	microscope is eccentric with the main light axis. A defocused signal is
	acquired by a symmetrical silicon photocell for the change of the
	reflected light position caused by differential amplification and
	processed by a microprocessor. Then the electric signal is
	power-amplified and drives a dc motor, which moves a fine working
	platform to an automatic focus of the microscope. The result of the
	experiments shows a +or-0.1- mu m precision of autofocusing for a range
	of +or-500- mu m defocusing. The system has high reliability and can
	meet the requirements of various accurate micro measurement systems
['autofocus system', 'microscope autofocusing', 'eccentric light beam approach', 'object lens', 'objective plane', 'semiconductor laser', 'main light axis', 'defocused signal', 'symmetrical silicon photocell', 'reflected lightposition', 'differential amplification', 'microprocessor', 'power-amplifiedelectric signal', 'dc motor', 'fine working platform', 'high reliability', 'micro measurement systems']
['object lens whose objective plane', 'eccentric light beam approach', 'microscope', 'symmetrical silicon photocell', 'main light axis', 'autofocus system', 'focal plane', 'compact construction', 'mu m', 'theoretical formula']

Design and implementation of a 3-D mapping system for highly irregular shaped
	objects with application to semiconductor manufacturing
The basic technology for a robotic system is developed to automate the packing
	of polycrystalline silicon nuggets into fragile fused silica crucible
	in Czochralski (melt pulling) semiconductor wafer production. The
	highly irregular shapes of the nuggets and the packing constraints make
	this a difficult and challenging task. It requires the delicate
	manipulation and packing of highly irregular polycrystalline silicon
	nuggets into a fragile fused silica crucible. For this application, a
	dual optical 3-D surface mapping system that uses active laser
	triangulation has been developed and successfully tested. One part of
	the system measures the geometry profile of a nugget being packed and
	the other the profile of the nuggets already in the crucible. A
	resolution of 1 mm with 15-KHz sampling frequency is achieved. Data
	from the system are used by the packing algorithm, which determines
	optimal nugget placement. The key contribution is to describe the
	design and implementation of an efficient and robust 3-D imaging system
	to map highly irregular shaped objects using conventional components in
	context of real commercial manufacturing processes
['3d mapping system', 'highly irregular shaped objects', 'semiconductormanufacturing', 'robotic system', 'polycrystalline silicon nuggets', 'fragilefused silica crucible', 'sampling frequency', 'packing algorithm', 'opticalnugget placement', 'robust 3-d imaging system', 'irregular shaped objects', 'commercial manufacturing processes', 'czochralski semiconductor waferproduction', 'dual optical 3d surface mapping system', 'highly irregularpolycrystalline silicon nuggets', 'active laser triangulation']
['fragile fused silica crucible', 'semiconductor wafer production', 'mapping system', 'polycrystalline silicon nugget', 'irregular polycrystalline silicon', 'surface mapping system', 'system', 'design', 'implementation', 'robotic system']

Effective moving cast shadow detection for monocular color traffic image
	sequences
For an accurate scene analysis using monocular color traffic image sequences, a
	robust segmentation of moving vehicles from the stationary background
	is generally required. However, the presence of moving cast shadow may
	lead to an inaccurate vehicle segmentation, and as a result, may lead
	to further erroneous scene analysis. We propose an effective method for
	the detection of moving cast shadow. By observing the characteristics
	of cast shadow in the luminance, chrominance, gradient density, and
	geometry domains, a combined probability map, called a shadow
	confidence score (SCS), is obtained. From the edge map of the input
	image, each edge pixel is examined to determine whether it belongs to
	the vehicle region based on its neighboring SCSs. The cast shadow is
	identified as those regions with high SCSs, which are outside the
	convex hull of the selected vehicle edge pixels. The proposed method is
	tested on 100 vehicle images taken under different lighting conditions
	(sunny and cloudy), viewing angles (roadside and overhead), vehicle
	sizes (small, medium, and large), and colors (similar to the road and
	not). The results indicate that an average error rate of around 14% is
	obtained while the lowest error rate is around 3% for large vehicles
['effective moving cast shadow detection', 'monocular color traffic imagesequences', 'accurate scene analysis', 'robust segmentation', 'movingvehicles', 'stationary background', 'moving cast shadow', 'inaccurate vehiclesegmentation', 'erroneous scene analysis', 'luminance', 'chrominance', 'gradient density', 'geometry domains', 'combined probability map', 'shadowconfidence score', 'input image', 'cast shadow', 'convex hull', 'selectedvehicle edge pixels', 'lighting conditions', 'vehicle images', 'sunny', 'cloudy', 'viewing angles', 'vehicle sizes', 'average error rate', 'imagesegmentation']
['monocular color traffic image', 'cast shadow', 'cast shadow detection', 'traffic image sequence', 'accurate scene analysis', 'inaccurate vehicle segmentation', 'scene analysis', 'edge pixel', 'robust segmentation', 'stationary background']

Estimation of error in curvature computation on multi-scale free-form surfaces
A novel technique for multi-scale curvature computation on a free-form 3-D
	surface is presented. This is achieved by convolving local
	parametrisations of the surface with 2-D Gaussian filters iteratively.
	In our technique, semigeodesic coordinates are constructed at each
	vertex of the mesh. Smoothing results are shown for 3-D surfaces with
	different shapes indicating that surface noise is eliminated and
	surface details are removed gradually. A number of evolution properties
	of 3-D surfaces are described. Next, the surface Gaussian and mean
	curvature values are estimated accurately at multiple scales which are
	then mapped to colours and displayed directly on the surface. The
	performance of the technique when selecting different directions as an
	arbitrary direction for the geodesic at each vertex are also presented.
	The results indicate that the error observed for the estimation of
	Gaussian and mean curvatures is quite low after only one iteration.
	Furthermore, as the surface is smoothed iteratively, the error is
	further reduced. The results also show that the estimation error of
	Gaussian curvature is less than that of mean curvature. Our experiments
	demonstrate that estimation of smoothed surface curvatures are very
	accurate and not affected by the arbitrary direction of the first
	geodesic line when constructing semigeodesic coordinates. Our technique
	is independent of the underlying triangulation and is also more
	efficient than volumetric diffusion techniques since 2-D rather than
	3-D convolutions are employed. Finally, the method presented here is a
	generalisation of the Curvature Scale Space method for 2-D contours.
	The CSS method has outperformed comparable techniques within the MPEG-7
	evaluation framework. As a result, it has been selected for inclusion
	in the MPEG-7 package of standards
['multi-scale curvature computation', 'free-form 3d surface', 'localparametrisations', '2d gaussian filters', 'surface noise', 'evolutionproperties', 'surface gaussian values', 'mean curvature values', 'semigeodesic coordinates', 'underlying triangulation', 'volumetricdiffusion techniques', 'convolutions', 'curvature scale space method', 'mpeg-7 evaluation framework']
['curvature scale space method', 'scale curvature computation', 'semigeodesic coordinate', 'volumetric diffusion technique', 'arbitrary direction', 'mean curvature', 'surface', 'multiple scale', 'curvature value', 'surface detail']

Restoration of broadband imagery steered with a liquid-crystal optical phased
	array
In many imaging applications, it is highly desirable to replace mechanical
	beam-steering components (i.e., mirrors and gimbals) with a
	nonmechanical device. One such device is a nematic liquid crystal
	optical phased array (LCOPA). An LCOPA can implement a blazed phase
	grating to steer the incident light. However, when a phase grating is
	used in a broadband imaging system, two adverse effects can occur.
	First, dispersion will cause different incident wavelengths arriving at
	the same angle to be steered to different output angles, causing
	chromatic aberrations in the image plane. Second, the device will steer
	energy not only to the first diffraction order, but to others as well.
	This multiple-order effect results in multiple copies of the scene
	appearing in the image plane. We describe a digital image restoration
	technique designed to overcome these degradations. The proposed
	postprocessing technique is based on a Wiener deconvolution filter. The
	technique, however, is applicable only to scenes containing objects
	with approximately constant reflectivities over the spectral region of
	interest. Experimental results are presented to demonstrate the
	effectiveness of this technique
['broadband imagery', 'liquid-crystal optical phased array steering', 'imagingapplications', 'mechanical beam-steering components', 'mirrors', 'gimbals', 'nonmechanical device', 'nematic liquid crystal optical phased array', 'blazed phase grating', 'incident light steering', 'broadband imagingsystem', 'dispersion', 'incident wavelengths', 'output angles', 'chromaticaberrations', 'image plane', 'optical phased array', 'first diffractionorder', 'multiple-order effect', 'halogen lamp', 'multiple copies', 'digitalimage restoration technique', 'postprocessing technique', 'wienerdeconvolution filter', 'approximately constant reflectivities', 'spectralregion of interest']
['wiener deconvolution filter', 'digital image restoration', 'different output angle', 'order effect result', 'different incident wavelength', 'broadband imaging system', 'nematic liquid crystal', 'image plane', 'nonmechanical device', 'steering component']

One-step digit-set-restricted modified signed-digit adder using an incoherent
	correlator based on a shared content-addressable memory
An efficient one-step digit-set-restricted modified signed-digit (MSD) adder
	based on symbolic substitution is presented. In this technique, carry
	propagation is avoided by introducing reference digits to restrict the
	intermediate carry and sum digits to {1,0} and {0,1}, respectively. The
	proposed technique requires significantly fewer minterms and simplifies
	system complexity compared to the reported one-step MSD addition
	techniques. An incoherent correlator based on an optoelectronic shared
	content-addressable memory processor is suggested to perform the
	addition operation. In this technique, only one set of minterms needs
	to be stored, independent of the operand length
['one-step digit-set-restricted modified signed-digit adder', 'incoherentcorrelator', 'shared content-addressable memory', 'symbolic substitution', 'reference digits', 'intermediate carry', 'sum digits', 'minterms', 'systemcomplexity', 'optoelectronic shared content-addressable memory processor', 'addition operation', 'operand length']
['step digit', 'addressable memory', 'addressable memory processor', 'digit adder', 'step msd addition', 'intermediate carry', 'symbolic substitution', 'reference digit', 'set', 'system complexity']

Two-step integral imaging for orthoscopic three-dimensional imaging with
	improved viewing resolution
We present a two-step integral imaging system to obtain 3-D orthoscopic real
	images. By adopting a nonstationary micro-optics technique, we
	demonstrate experimentally the potential usefulness of two-step
	integral imaging
['two-step integral imaging', 'resolution improved viewing', 'two-step integralimaging system', '3-d orthoscopic real images', 'nonstationary micro-opticstechnique', '3-d image reconstruction', 'liquid crystal light valve', 'display device', 'lclv', 'pickup lenslet array']
['step integral imaging', 'integral imaging', 'step integral', 'dimensional imaging', 'orthoscopic real', 'optics technique', 'nonstationary micro', 'orthoscopic', 'potential usefulness', 'resolution']

Diffraction limit for a circular mask with a periodic rectangular apertures
	array
A mask with periodic apertures imaging system is adopted very widely and plays
	a leading role in modern technology for uses such as pinhole cameras,
	coded imaging systems, optical information processing, etc. because of
	its high resolution, its infinite depth of focus, and its usefulness
	over a broad frequency spectra ranging from visible light to X-rays and
	gamma rays. While the masks with periodic apertures investigated in the
	literature are limited only to far-field diffraction, they do not take
	the shift of apertures within the mask into consideration. Therefore
	the derivation of the far-field diffraction for a single aperture
	cannot be applied to a mask with periodic apertures. The far-field
	diffraction formula modified for a multiaperture mask has been proposed
	in the past, the analysis remains too complicated to offer some
	practical guidance for mask design. We study a circular mask with
	periodic rectangular apertures and develop an easier way to interpret
	it. First, the near-field diffraction intensity of a circular aperture
	is calculated by means of Lommel's function. Then the convolution of
	the circular mask diffraction with periodic rectangular apertures is
	put together, and we can present a simple mathematical tool to analyze
	the mask properties including the intensity distribution, blurring
	aberration, and the criterion of defining the far- or near-field
	diffraction. This concept can also be expanded to analyze different
	types of masks with the arbitrarily shaped apertures
['diffraction limit', 'circular mask', 'periodic rectangular apertures array', 'pinholecameras', 'coded imaging systems', 'optical information processing', 'highresolution', 'infinite depth of focus', 'broad frequency spectra', 'visiblelight', 'x rays', 'gamma rays', 'periodic apertures', 'far-field diffraction', 'mask', 'single aperture', 'far-field diffraction formula', 'multiaperturemask', 'periodic rectangular apertures', 'convolution', 'circular maskdiffraction', 'near-field diffraction', 'arbitrarily shaped apertures']
['periodic rectangular aperture', 'circular mask', 'periodic aperture', 'field diffraction', 'mask', 'information processing', 'modern technology', 'infinite depth', 'optical information', 'visible light']

Binocular model for figure-ground segmentation in translucent and occluding
	images
A Fourier-based solution to the problem of figure-ground segmentation in short
	baseline binocular image pairs is presented. Each image is modeled as
	an additive composite of two component images that exhibit a spatial
	shift due to the binocular parallax. The segmentation is accomplished
	by decoupling each Fourier component in one of the resultant additive
	images into its two constituent phasors, allocating each to its
	appropriate object-specific spectrum, and then reconstructing the
	foreground and background using the inverse Fourier transform. It is
	shown that the foreground and background shifts can be computed from
	the differences of the magnitudes and phases of the Fourier transform
	of the binocular image pair. While the model is based on translucent
	objects, it also works with occluding objects
['binocular model', 'figure-ground segmentation', 'translucent images', 'occludingimages', 'images', 'image segmentation', 'fourier-based solution', 'shortbaseline binocular image pairs', 'component images', 'spatial shift', 'binocular parallax', 'fourier component decoupling', 'phasors', 'object-specific spectrum', 'foreground', 'background', 'inverse fouriertransform', 'binocular image pair', 'translucent objects', 'occluding objects']
['baseline binocular image pair', 'binocular image pair', 'ground segmentation', 'inverse fourier transform', 'a fourier', 'binocular model', 'specific spectrum', 'figure', 'translucent', 'component image']

Multispectral color image capture using a liquid crystal tunable filter
We describe the experimental setup of a multispectral color image acquisition
	system consisting of a professional monochrome CCD camera and a tunable
	filter in which the spectral transmittance can be controlled
	electronically. We perform a spectral characterization of the
	acquisition system taking into account the acquisition noise. To
	convert the camera output signals to device-independent color data, two
	main approaches are proposed and evaluated. One consists in applying
	regression methods to convert from the K camera outputs to a
	device-independent color space such as CIEXYZ or CIELAB. Another method
	is based on a spectral model of the acquisition system. By inverting
	the model using a principal eigenvector approach, we estimate the
	spectral reflectance of each pixel of the imaged surface
['multispectral color image capture', 'liquid crystal tunable filter', 'multispectralcolor image acquisition system', 'monochrome ccd camera', 'tunable filter', 'spectral transmittance', 'spectral characterization', 'acquisition system', 'acquisition noise', 'camera output signals', 'device-independent colordata', 'regression methods', 'camera outputs', 'independent color space', 'ciexyz', 'cielab', 'spectral model', 'principal eigenvector approach', 'spectral reflectance', 'imaged surface', 'pixel']
['professional monochrome ccd camera', 'multispectral color image acquisition', 'multispectral color image capture', 'liquid crystal tunable filter', 'acquisition system', 'independent color datum', 'camera output signal', 'acquisition noise', 'k camera output', 'independent color space']

Iterative regularized least-mean mixed-norm image restoration
We develop a regularized mixed-norm image restoration algorithm to deal with
	various types of noise. A mixed-norm functional is introduced, which
	combines the least mean square (LMS) and the least mean fourth (LMF)
	functionals, as well as a smoothing functional. Two regularization
	parameters are introduced: one to determine the relative importance of
	the LMS and LMF functionals, which is a function of the kurtosis, and
	another to determine the relative importance of the smoothing
	functional. The two parameters are chosen in such a way that the
	proposed functional is convex, so that a unique minimizer exists. An
	iterative algorithm is utilized for obtaining the solution, and its
	convergence is analyzed. The novelty of the proposed algorithm is that
	no knowledge of the noise distribution is required, and the relative
	contributions of the LMS, the LMF, and the smoothing functionals are
	adjusted based on the partially restored image. Experimental results
	demonstrate the effectiveness of the proposed algorithm
['iterative regularized least-mean mixed-norm image restoration', 'noise', 'mixed-norm functional', 'least mean square functionals', 'mean fourthfunctionals', 'smoothing functional', 'regularization parameters', 'kurtosis', 'convex functional', 'unique minimizer', 'iterative algorithm', 'convergence', 'noise distribution', 'partially restored image']
['norm image restoration algorithm', 'norm image restoration', 'relative importance', 'smoothing functional', 'functional', 'lmf functional', 'lms', 'iterative', 'mixed', 'unique minimizer exist']

Motion estimation using modified dynamic programming
A new method for computing precise estimates of the motion vector field of
	moving objects in a sequence of images is proposed. Correspondence
	vector-field computation is formulated as a matching optimization
	problem for multiple dynamic images. The proposed method is a heuristic
	modification of dynamic programming applied to the 2-D optimization
	problem. Motion-vector-field estimates using real movie images
	demonstrate good performance of the algorithm in terms of dynamic
	motion analysis
['modified dynamic programming', 'motion estimation', 'precise estimates', 'motionvector field', 'moving objects', 'image sequence', 'vector-field computation', 'matching optimization problem', 'multiple dynamic images', 'heuristicmodification', 'dynamic programming', '2-d optimization problem', 'motionvector field estimates', 'real movie images', 'algorithm', 'dynamic motionanalysis']
['dynamic programming', 'demonstrate good performance', 'multiple dynamic image', 'motion vector field', 'real movie image', 'precise estimate', 'new method', 'field computation', 'motion estimation', 'proposed method']

Centroid detection based on optical correlation
We propose three correlation-based methods to simultaneously detect the
	centroids of multiple objects in an input scene. The first method is
	based on the modulus of the moment function, the second method is based
	on squaring the moment function, and the third method works with a
	single intensity filter. These methods are invariant to changes in the
	position, orientation, and scale of the object and result in good
	noise-smoothing performance. We use spatial light modulators (SLMs) to
	directly implement the input of the image and filter information for
	the purpose of these approaches. We present results showing simulations
	from different approaches and provide comparisons between
	optical-correlation- and digital-moment-based methods. Experimental
	results corresponding to an optical correlator using SLMs for the
	centroid detection are also presented
['optical correlation', 'centroid detection', 'correlation-based methods', 'centroids', 'multiple objects', 'input scene', 'moment function modulus', 'moment functionsquaring', 'single intensity filter', 'position', 'orientation', 'scale', 'noise-smoothing performance', 'spatial light modulators', 'digital-moment-based methods', 'optical correlator']
['centroid detection', 'moment function', 'optical correlation', 'spatial light modulator', 'second method', 'input scene', 'single intensity filter', 'method', 'multiple object', 'smoothing performance']

Block truncation image bit plane coding
Block truncation coding (BTC) is a successful image compression technique due
	to its simple and fast computational burden. The bit rate is fixed to
	2.0 bits/pixel, whose performance is moderate in terms of compression
	ratio compared to other compression schemes such as discrete cosine
	transform (DCT), vector quantization (VQ), wavelet transform coding
	(WTC), etc. Two kinds of overheads are required for BTC coding: bit
	plane and quantization values, respectively. A new technique is
	presented to reduce the bit plane overhead. Conventional bit plane
	overhead is 1.0 bits/pixel; we decrease it to 0.734 bits/pixel while
	maintaining the same decoded quality as absolute moment BTC (AMBTC)
	does for the "Lena" image. Compared to other published bit plane coding
	strategies, the proposed method outperforms all of the existing methods
['image bit plane coding', 'block truncation coding', 'image compression technique', 'bit rate', 'performance', 'compression ratio', 'bit plane overhead', 'decodedquality', 'absolute moment btc', 'ambtc', 'quantization values', 'lena image']
['successful image compression technique', 'truncation image bit plane', 'bit plane', 'block truncation coding', 'fast computational burden', 'bit rate', 'discrete cosine', 'vector quantization', 'compression scheme', 'conventional bit plane']

Plenoptic image editing
This paper presents a new class of interactive image editing operations
	designed to maintain consistency between multiple images of a physical
	3D scene. The distinguishing feature of these operations is that edits
	to any one image propagate automatically to all other images as if the
	(unknown) 3D scene had itself been modified. The modified scene can
	then be viewed interactively from any other camera viewpoint and under
	different scene illuminations. The approach is useful first as a
	power-assist that enables a user to quickly modify many images by
	editing just a few, and second as a means for constructing and editing
	image-based scene representations by manipulating a set of photographs.
	The approach works by extending operations like image painting,
	scissoring, and morphing so that they alter a scene's plenoptic
	function in a physically-consistent way, thereby affecting scene
	appearance from all viewpoints simultaneously. A key element in
	realizing these operations is a new volumetric decomposition technique
	for reconstructing an scene's plenoptic function from an incomplete set
	of camera viewpoints
['interactive image editing operations', 'multiple images', 'physical 3d scene', 'modified scene', 'camera viewpoint', 'image-based scene representations', 'image painting', 'scissoring', 'morphing', 'plenoptic function', 'volumetricdecomposition technique', 'plenoptic image editing']
['interactive image editing operation', 'new volumetric decomposition technique', 'plenoptic image editing', '3d scene', 'different scene illumination', 'camera viewpoint', 'distinguishing feature', 'scene', 'new class', 'multiple image']

Elimination of zero-order diffraction in digital holography
A simple method to suppress the zero-order diffraction in the reconstructed
	image of digital holography is presented. In this method, the Laplacian
	of a detected hologram is used instead of the hologram itself for
	numerical reconstruction by computing the discrete Fresnel integral.
	This method can significantly improve the image quality and give better
	resolution and higher accuracy of the reconstructed image. The main
	advantages of this method are its simplicity in experimental
	requirements and convenience in data processing
['zero-order diffraction suppression', 'reconstructed image', 'laplacian', 'detectedhologram', 'numerical image reconstruction', 'discrete fresnel integral', 'image quality', 'image resolution', 'accuracy', 'data processing', 'image.processing', 'digital holography']
['order diffraction', 'digital holography', 'discrete fresnel integral', 'simple method', 'method', 'reconstructed image', 'numerical reconstruction', 'high accuracy', 'image quality', 'hologram']

Efficient two-level image thresholding method based on Bayesian formulation and
	the maximum entropy principle
An efficient method for two-level thresholding is proposed based on the Bayes
	formula and the maximum entropy principle, in which no assumptions of
	the image histogram are made. An alternative criterion is derived based
	on maximizing entropy and used for speeding up the searching algorithm.
	Five forms of conditional probability distributions-simple, linear,
	parabola concave, parabola convex, and S-function-are employed and
	compared to each other for optimal threshold determination. The effect
	of precision on optimal threshold determination is discussed and a
	trade-off precision epsilon =0.001 is selected experimentally. Our
	experiments demonstrate that the proposed method achieves a significant
	improvement in speed from 26 to 57 times faster than the exhaustive
	search method
['two-level image thresholding method', 'bayesian formulation', 'maximum entropyprinciple', 'image histogram', 'entropy', 'searching algorithm', 'conditionalprobability distributions', 'parabola concave', 'parabola convex', 's-function', 'optimal threshold determination', 'trade-off precision', 'imagesegmentation', 'image thresholding']
['level image thresholding method', 'maximum entropy principle', 'optimal threshold determination', 'conditional probability distribution', 'level thresholding', 'efficient method', 'alternative criterion', 'bayesian formulation', 'image histogram', 'parabola convex']

Adaptive digital watermarking using fuzzy logic techniques
Digital watermarking has been proposed for copyright protection in our digital
	society. We propose an adaptive digital watermarking scheme based on
	the human visual system model and a fuzzy logic technique. The fuzzy
	logic approach is employed to obtain the different strengths and
	lengths of a watermark by the local characteristics of the image in our
	proposed scheme. In our experiments, this scheme provides a more robust
	and imperceptible watermark
['adaptive digital watermarking', 'fuzzy logic techniques', 'copyright protection', 'digital society', 'human visual system model', 'local characteristics', 'imperceptible watermark', 'robust watermark', 'image processing']
['fuzzy logic technique', 'human visual system model', 'adaptive digital', 'digital watermarking', 'copyright protection', 'scheme', 'different strength', 'logic approach', 'local characteristic', 'digital']

Optical encoding of color three-dimensional correlation
Three-dimensional (3D) correlation of color images, considering the color
	distribution as the third dimension, has been shown to be useful for
	color pattern recognition tasks. Nevertheless, 3D correlation cannot be
	directly performed on an optical correlator, that can only process
	two-dimensional (2D) signals. We propose a method to encode 3D
	functions onto 2D ones in such a way that the Fourier transform and
	correlation of these signals, that can be optically performed, encode
	the 3D Fourier transform and correlation of the 3D signals. The theory
	for the encoding is given and experimental results obtained in an
	optical correlator are shown
['optical encoding', 'color three-dimensional correlation', '3d correlation', 'colorimages', 'color distribution', 'color pattern recognition tasks', 'opticalcorrelator', '3d function encoding', 'fourier transform', '3d fouriertransform']
['color pattern recognition task', 'optical correlator', 'color image', 'correlation', 'dimensional correlation', '2d one', 'optical encoding', 'encode 3d', 'fourier transform', '3d fourier transform']

Search for efficient solutions of multi-criterion problems by target-level
	method
The target-level method is considered for solving continuous multi-criterion
	maximization problems. In the first step, the decision-maker specifies
	a target-level point (the desired criterion values); then in the set of
	vector evaluations we seek points that are closest to the target point
	in the Chebyshev metric. The vector evaluations obtained in this way
	are in general weakly efficient. To identify the efficient evaluations,
	the second step maximizes the sum of the criteria on the set generated
	in step 1. We prove the relationship between the evaluations and
	decisions obtained by the proposed procedure, on the one hand, and the
	efficient (weakly efficient) evaluations and decisions, on the other
	hand. If the Edgeworth-Pareto hull of the set of vector evaluations is
	convex, the set of efficient vector evaluations can be approximated by
	the proposed method
['multi-criterion problems', 'target-level method', 'continuous multi-criterionmaximization problems', 'target-level point', 'chebyshev metric', 'edgeworth-pareto hull']
['vector evaluation', 'chebyshev metric', 'target point', 'second step', 'efficient evaluation', 'criterion value', 'target', 'level point', 'efficient', 'maximization problem']

Computer processing of data on mental impairments during the acute period of
	concussion
The article presents results of computer processing of experimental information
	obtained from patients during the acute period of concussion. A number
	of computational procedures are described
['computer processing', 'mental impairments', 'acute period of concussion', 'computational procedures']
['computer processing', 'acute period', 'mental impairment', 'experimental information', 'concussion', 'computational procedure', 'result', 'article', 'patient', 'number']

Regularization of linear regression problems
The study considers robust estimation of linear regression parameters by the
	regularization method, the pseudoinverse method, and the Bayesian
	method allowing for correlations and errors in the data. Regularizing
	algorithms are constructed and their relationship with pseudoinversion,
	the Bayesian approach, and BLUE is investigated
['linear regression problems regularization', 'robust estimation', 'linear regressionparameters', 'pseudoinverse method', 'bayesian method', 'pseudoinversion', 'bayesian approach', 'blue']
['linear regression parameter', 'linear regression problem', 'pseudoinverse method', 'regularization method', 'robust estimation', 'bayesian', 'method', 'regularization', 'bayesian approach', 'datum']

Choice from a three-element set: some lessons of the 2000 presidential campaign
	in the United States
We consider the behavior of four choice rules - plurality voting, approval
	voting, Borda count, and self-consistent choice - when applied to
	choose the best option from a three-element set. It is assumed that the
	two main options are preferred by a large majority of the voters, while
	the third option gets a very small number of votes and influences the
	election outcome only when the two main options receive a close number
	of votes. When used to rate the main options, Borda count and
	self-consistent choice contain terms that allow both for the "strength
	of preferences" of the voters and the rating of the main candidates by
	voters who vote for the third option. In this way, it becomes possible
	to determine more reliably the winner when plurality voting or approval
	voting produce close results
['three-element set', '2000 presidential campaign', 'plurality voting', 'approvalvoting', 'borda count', 'self-consistent choice']
['element set', 'borda count', 'consistent choice', 'main option', 'plurality voting', 'good option', 'presidential campaign', 'united states', 'large majority', 'choice rule']

An inverse problem for a model of a hierarchical structure
We consider the inverse problem for the identification of the coefficient in a
	parabolic equation. The model is applied to describe the functioning of
	a hierarchical structure; it is also relevant for heat-conduction
	theory. Unique solvability of the inverse problem is proved
['inverse problem', 'hierarchical structure', 'parabolic equation', 'heat-conductiontheory', 'unique solvability']
['inverse problem', 'hierarchical structure', 'parabolic equation', 'model', 'unique solvability', 'coefficient', 'theory', 'conduction', 'identification', 'functioning']

Self-calibration from image derivatives
This study investigates the problem of estimating camera calibration parameters
	from image motion fields induced by a rigidly moving camera with
	unknown parameters, where the image formation is modeled with a linear
	pinhole-camera model. The equations obtained show the flow to be
	separated into a component due to the translation and the calibration
	parameters and a component due to the rotation and the calibration
	parameters. A set of parameters encoding the latter component is
	linearly related to the flow, and from these parameters the calibration
	can be determined. However, as for discrete motion, in general it is
	not possible to decouple image measurements obtained from only two
	frames into translational and rotational components. Geometrically, the
	ambiguity takes the form of a part of the rotational component being
	parallel to the translational component, and thus the scene can be
	reconstructed only up to a projective transformation. In general, for
	full calibration at least four successive image frames are necessary,
	with the 3D rotation changing between the measurements. The geometric
	analysis gives rise to a direct self-calibration method that avoids
	computation of optical flow or point correspondences and uses only
	normal flow measurements. New constraints on the smoothness of the
	surfaces in view are formulated to relate structure and motion directly
	to image derivatives, and on the basis of these constraints the
	transformation of the viewing geometry between consecutive images is
	estimated. The calibration parameters are then estimated from the
	rotational components of several flow fields. As the proposed technique
	neither requires a special set up nor needs exact correspondence it is
	potentially useful for the calibration of active vision systems which
	have to acquire knowledge about their intrinsic parameters while they
	perform other tasks, or as a tool for analyzing image sequences in
	large video databases
['camera calibration parameters', 'image motion fields', 'rigidly moving camera', 'image formation', 'linear pinhole-camera model', 'calibration parameters', 'image measurements', 'translational components', 'rotational components', 'direct self-calibration method', 'optical flow', 'point correspondences', 'normal flow measurements', 'active vision systems', 'image sequences', 'largevideo databases', 'depth distortion']
['image derivative', 'rotational component', 'camera model', 'image formation', 'motion field', 'unknown parameter', 'image motion', 'camera calibration', 'calibration', 'vision system']

Inverse problems for a mathematical model of ion exchange in a compressible ion
	exchanger
A mathematical model of ion exchange is considered, allowing for ion exchanger
	compression in the process of ion exchange. Two inverse problems are
	investigated for this model, unique solvability is proved, and
	numerical solution methods are proposed. The efficiency of the proposed
	methods is demonstrated by a numerical experiment
['inverse problems', 'mathematical model', 'ion exchange', 'compressible ion exchanger', 'ion exchanger compression', 'unique solvability', 'numerical solutionmethods']
['inverse problem', 'mathematical model', 'ion exchange', 'compressible ion', 'numerical solution method', 'ion exchanger', 'unique solvability', 'model', 'exchanger', 'numerical experiment']

Application of multiprocessor systems for computation of jets
The article describes the implementation of methods for numerical solution of
	gas-dynamic problems on a wide class of multiprocessor systems,
	conventionally characterized as "cluster" systems. A standard
	data-transfer interface - the so-called message passing interface - is
	used for parallelization of application algorithms among processors.
	Simulation of jets escaping into a low-pressure region is chosen as a
	computational example
['multiprocessor systems', 'computation of jets', 'gas-dynamic problems', 'clustersystems', 'data-transfer interface', 'message passing interface', 'low-pressure region']
['multiprocessor system', 'numerical solution', 'dynamic problem', 'wide class', 'computational example', 'pressure region', 'application algorithm', 'transfer interface', 'jet', 'application']

Hybrid simulation of space plasmas: models with massless fluid representation
	of electrons. IV. Kelvin-Helmholtz instability
For pt.III. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000).
	This is a survey of the literature on hybrid simulation of the
	Kelvin-Helmholtz instability. We start with a brief review of the
	theory: the simplest model of the instability - a transition layer in
	the form of a tangential discontinuity; compressibility of the medium;
	finite size of the velocity shear region; pressure anisotropy. We then
	describe the electromagnetic hybrid model (ions as particles and
	electrons as a massless fluid) and the main numerical schemes. We
	review the studies on two-dimensional and three-dimensional hybrid
	simulation of the process of particle mixing across the magnetopause
	shear layer driven by the onset of a Kelvin-Helmholtz instability. The
	article concludes with a survey of literature on hybrid simulation of
	the Kelvin-Helmholtz instability in finite-size objects: jets moving
	across the magnetic field in the middle of the field reversal layer;
	interaction between a magnetized plasma flow and a cylindrical plasma
	source with zero own magnetic field
['hybrid simulation', 'space plasmas', 'massless fluid representation', 'kelvin-helmholtz instability', 'transition layer', 'tangentialdiscontinuity', 'pressure anisotropy', 'electromagnetic hybrid model', 'three-dimensional hybrid simulation', 'magnetopause shear layer', 'fieldreversal layer', 'magnetized plasma flow', 'cylindrical plasma source']
['hybrid simulation', 'helmholtz instability', 'magnetized plasma flow', 'main numerical scheme', 'massless fluid representation', 'field reversal layer', 'electromagnetic hybrid model', 'space plasma', 'massless fluid', 'velocity shear region']

Limits for computational electromagnetics codes imposed by computer
	architecture
The algorithmic complexity of the innermost loops that determine the complexity
	of algorithms in computational electromagnetics (CEM) codes are
	analyzed according to their operation count and the impact of the
	underlying computer hardware. As memory chips are much slower than
	arithmetic processors, codes that involve a high data movement compared
	to the number of arithmetic operations are executed comparatively
	slower. Hence, matrix-matrix multiplications are much faster than
	matrix-vector multiplications. It is seen that it is not sufficient to
	compare only the complexity, but also the actual performance of
	algorithms to judge on faster execution. Implications involve FDTD
	loops, LU factorizations, and iterative solvers for dense matrices. Run
	times on two reference platforms, namely an Athlon 900 MHz and an HP PA
	8600 processor, verify the findings
['computational electromagnetics codes', 'computer architecture', 'algorithmiccomplexity', 'innermost loops', 'cem codes', 'operation count', 'computerhardware', 'memory chips', 'data movement', 'matrix-matrix multiplications', 'matrix-vector multiplications', 'fdtd loops', 'lu factorizations', 'iterativesolvers', 'dense matrices']
['high datum movement', 'underlying computer hardware', 'computational electromagnetic code', 'computational electromagnetic', 'innermost loop', 'algorithmic complexity', 'complexity', 'operation count', 'arithmetic processor', 'memory chip']

Three-dimensional geometrical optics code for indoor propagation
This paper presents a program, GO 3D, for computing the fields of a transmitter
	in an indoor environment using geometrical optics. The program uses an
	"image tree" data structure to construct the images needed to compute
	all the rays carrying fields above a preset "threshold" value, no
	matter how many reflections are needed. The paper briefly describes the
	input file required to define wall construction, the floor plan, the
	transmitter, and the receiver locations. A case study consisting of a
	long corridor with a small room on one side is used to demonstrate the
	features of the GO 3D program
['three-dimensional geometrical optics', '3d geometrical optics code', 'indoorpropagation', 'image tree data structure', 'image construction', 'wallconstruction', 'floor plan', 'transmitter', 'receiver locations', 'ray tracing', 'data visualisation']
['dimensional geometrical optic code', 'go 3d', 'datum structure', 'indoor environment', 'image tree', 'indoor propagation', 'transmitter', 'input file', 'paper briefly', 'floor plan']

Building a better game through dynamic programming: a Flip analysis
Flip is a solitaire board game produced by craft woodworkers. We analyze Flip
	and suggest modifications to the rules to make the game more
	marketable. In addition to being an interesting application of dynamic
	programming, this case shows the use of operations research in
	managerial decision making
['dynamic programming', 'flip analysis', 'operations research', 'managerial decisionmaking', 'solitaire board game', 'craft woodworkers']
['solitaire board game', 'managerial decision making', 'flip analysis', 'craft woodworker', 'dynamic programming', 'good game', 'interesting application', 'programming', 'flip', 'dynamic']

Designing and delivering a university course - a process (or operations)
	management perspective
With over 30 years of academic experience in both engineering and management
	faculties, involving trial and error experimentation in teaching as
	well as reading relevant literature and observing other instructors in
	action, the author has accumulated a number of ideas, regarding the
	preparation and delivery of a university course, that should be of
	interest to other instructors. This should be particularly the case for
	those individuals who have had little or no teaching experience (e.g.
	those whose graduate education was recently completed at
	research-oriented institutions providing little guidance with respect
	to teaching). A particular perspective is used to convey the ideas,
	namely one of viewing the preparation and delivery of a course as two
	major processes that should provide outputs or outcomes that are of
	value to a number of customers, in particular, students
['university course delivery', 'management perspective', 'academic experience', 'management faculties', 'engineering faculties', 'search-orientedinstitutions']
['university course', 'management perspective', 'academic experience', 'error experimentation', 'relevant literature', 'major process', 'particular perspective', 'little guidance', 'graduate education', 'teaching experience']

A generalized PERT/CPM implementation in a spreadsheet
This paper describes the implementation of the traditional PERT/CPM algorithm
	for finding the critical path in a project network in a spreadsheet.
	The problem is of importance due to the recent shift of attention to
	using the spreadsheet environment as a vehicle for delivering
	management science/operations research (MS/OR) techniques to end-users
['generalized pert/cpm implementation', 'spreadsheet', 'critical path', 'ms/ortechniques']
['operation research', 'management science', 'spreadsheet environment', 'project network', 'recent shift', 'critical path', 'cpm algorithm', 'cpm implementation', 'traditional pert', 'spreadsheet']

An object-oriented version of SIMLIB (a simple simulation package)
This paper introduces an object-oriented version of SIMLIB (an
	easy-to-understand discrete-event simulation package). The
	object-oriented version is preferable to the original procedural
	language versions of SIMLIB in that it is easier to understand and
	teach simulation from an object point of view. A single-server queue
	simulation is demonstrated using the object-oriented SIMLIB
['object-oriented version', 'simlib', 'discrete-event simulation', 'teach simulation']
['event simulation package', 'object', 'simple simulation package', 'simlib', 'version', 'easy', 'paper', 'language version', 'discrete', 'object point']

The maximum possible EVPI
In this paper we calculate the maximum expected value of perfect information
	(EVPI) for any probability distribution for the states of the world.
	This maximum EVPI is an upper bound for the EVPI with given
	probabilities and thus an upper bound for any partial information about
	the states of the world
['decision analysis', 'expected value of perfect information', 'operations research', 'management science', 'probability distribution', 'optimisation']
['maximum possible evpi', 'upper bound', 'evpi', 'probability distribution', 'perfect information', 'maximum evpi', 'state', 'world', 'probability', 'partial information']

Geotensity: combining motion and lighting for 3D surface reconstruction
This paper is about automatically reconstructing the full 3D surface of an
	object observed in motion by a single static camera. Based on the two
	paradigms, structure from motion and linear intensity subspaces, we
	introduce the geotensity constraint that governs the relationship
	between four or more images of a moving object. We show that it is
	possible in theory to solve for 3D Lambertian surface structure for the
	case of a single point light source and propose that a solution exists
	for an arbitrary number point light sources. The surface may or may not
	be textured. We then give an example of automatic surface
	reconstruction of a face under a point light source using arbitrary
	unknown object motion and a single fixed camera
['full 3d surface', 'single static camera', 'linear intensity subspaces', 'geotensityconstraint', '3d lambertian surface structure', 'single point light source', 'arbitrary number point light sources', 'automatic surface reconstruction', 'point light source', 'linear image subspaces', 'structure-from-motion']
['arbitrary number point light source', 'single point light source', 'linear intensity subspace', '3d lambertian surface structure', 'single static camera', '3d surface reconstruction', 'motion', '3d surface', 'geotensity', 'geotensity constraint']

Who Wants To Be A Millionaire(R): The classroom edition
This paper introduces a version of the internationally popular television game
	show Who Wants To Be A Millionaire(R) that has been created for use in
	the classroom using Microsoft PowerPoint(R). A suggested framework for
	its classroom use is presented, instructions on operating and editing
	the classroom version of Who Wants To Be A Millionaire(R) are provided,
	and sample feedback from students who have played the classroom version
	of Who Wants To Be A Millionaire(R) is offered
['classroom', 'who wants to be a millionaire(r)', 'classroom version', 'undergraduatebusiness students', 'student contestants']
['popular television game', 'who', 'millionaire(r', 'classroom version', 'classroom edition', 'paper', 'classroom use', 'sample feedback', 'classroom', 'microsoft powerpoint(r']

Who wants to see a $million error?
Inspired by the popular television show "Who Wants to Be a Millionaire?", this
	case discusses the monetary decisions contestants face on a game
	consisting of 15 increasingly difficult multiple choice questions.
	Since the game continues as long as a contestant answers correctly,
	this case, at its core, is one of sequential decision analysis,
	amenable to analysis via stochastic dynamic programming. The case is
	also suitable for a course dealing with single decision analysis,
	allowing for discussion of utility theory and Bayesian probability
	revision. In developing a story line for the case, the author has
	sprinkled in much background material on probability and statistics.
	This material is placed in a historical context, illuminating some of
	the influential scholars involved in the development of these subjects
	as well as the birth of operations research and the management sciences
['operations research', 'game theory', 'decision analysis', 'stochastic dynamicprogramming', 'educational course', 'statistics', 'probabilistic models']
['difficult multiple choice question', 'single decision analysis', 'stochastic dynamic programming', 'monetary decision contestant', 'sequential decision analysis', 'case', 'popular television', 'contestant answer', 'game', 'bayesian probability']

Blitzograms - interactive histograms
As computers become ever faster, more and more procedures that were once viewed
	as iterative will continue to become instantaneous. The blitzogram is
	the application of this trend to histograms, which the author hopes
	will lead to a better tacit understanding of probability distributions
	among both students and managers. And this is not just an academic
	exercise. Commercial Monte Carlo simulation packages like @RISK and
	Crystal Ball, and my INSIGHT.xla are widely available
['blitzogram', 'histograms', 'probability distributions', 'mba', 'operations research', 'management science', 'statistics']
['commercial monte carlo simulation package', 'good tacit understanding', 'interactive histogram', 'probability distribution', 'histogram', 'crystal ball', 'computer', 'trend', 'procedure', 'instantaneous']

Teaching management science with spreadsheets: From decision models to decision
	support
The 1990s were a decade of enormous change for management science (MS)
	educators. While the outlook at the beginning of the decade was
	somewhat bleak, the renaissance in MS education brought about by the
	use of spreadsheets as the primary delivery vehicle for quantitative
	modeling techniques has resulted in a much brighter future. This paper
	takes inventory of the current state of MS education and suggests some
	promising new directions in the area of decision support systems for MS
	educators to consider for the future
['management science', 'ms education', 'spreadsheets', 'quantitative modeling', 'decisionsupport systems']
['management science', 'ms education', 'decision support system', 'decision model', 'primary delivery vehicle', 'promising new direction', 'enormous change', 'modeling technique', 'spreadsheet', 'bright future']

Teaching modeling in management science
This essay discusses how we can most effectively teach Management Science to
	students in MBA or similar programs who will be, at best, part-time
	practitioners of these arts. I take as a working hypothesis the radical
	proposition that the heart of Management Science itself is not the
	impressive array of tools that have been built up over the years
	(optimization, simulation, decision analysis, queuing, and so on) but
	rather the art of reasoning logically with formal models. I believe it
	is necessary with this group of students to teach basic modeling
	skills, and in fact it is only when such students have these basic
	skills as a foundation that they are prepared to acquire the more
	sophisticated skills needed to employ Management Science. In this paper
	I present a hierarchy of modeling skills, from numeracy skills through
	sophisticated Management Science skills, as a framework within which to
	plan courses for the occasional practitioner
['management science', 'modeling', 'numeracy skills', 'formal models', 'decision analysis']
['management science', 'similar program', 'impressive array', 'decision analysis', 'formal model', 'student', 'sophisticated management science skill', 'sophisticated skill', 'modeling skill', 'practitioner']

Causes of the decline of the business school management science course
The business school management science course is suffering serious decline. The
	traditional model- and algorithm-based course fails to meet the needs
	of MBA programs and students. Poor student mathematical preparation is
	a reality, and is not an acceptable justification for poor teaching
	outcomes. Management science Ph.D.s are often poorly prepared to teach
	in a general management program, having more experience and interest in
	algorithms than management. The management science profession as a
	whole has focused its attention on algorithms and a narrow subset of
	management problems for which they are most applicable. In contrast,
	MBA's rarely encounter problems that are suitable for straightforward
	application of management science tools, living instead in a world
	where problems are ill-defined, data is scarce, time is short, politics
	is dominant, and rational "decision makers" are non-existent. The root
	cause of the profession's failure to address these issues seems to be
	(in Russell Ackoff's words) a habit of professional introversion that
	caused the profession to be uninterested in what MBA's really do on the
	job and how management science can help them
['business school management science course', 'mba programs', 'mba students', 'management science', 'profession']
['business school management science course', 'poor student mathematical preparation', 'mba', 'management science ph', 'general management program', 'management science profession', 'management science tool', 'poor teaching', 'mba program', 'traditional model-']

Gifts to a science academic librarian
Gifts, by their altruistic nature, perfectly fit into the environment of
	universities and academic libraries. As a university's community and
	general public continue to donate materials, libraries accept donations
	willingly, both in-kind and monetary. Eight steps of gift processing
	are listed in the paper. Positive and negative aspects of gift
	acceptance are discussed. Gifts bring value for academic libraries.
	Gifts can be considered additional routes to contribute to library
	collections without direct purchases, options to add money to the
	library budget, and the cement of social relationships. But,
	unfortunately, large donations are time-consuming, labor-intensive and
	costly to process. Great amounts of staff time and processing space are
	two main negative aspects that cause concern and put the value of gift
	acceptance under consideration by librarians. Some strategies in
	handling gifts are recommended. To be effective, academic science
	librarians need to approach gifts as an investment. Librarians are not
	to be forced by moral and public notions and should be able to make
	professional decisions in evaluating proposed collections
['science academic librarian', 'academic libraries', 'donations', 'gift processing', 'library collections', 'budget', 'staff time', 'professional decisions', 'research libraries', 'acquisitions', 'gift books']
['main negative aspect', 'science academic librarian', 'gift', 'academic library', 'negative aspect', 'general public', 'gift processing', 'additional route', 'direct purchase', 'social relationship']

Four factors influencing the fair market value of out-of print books. 2
Fot pt.1 see ibid., p.71-8 (2002). Data from the fifty-six titles examined
	qualitatively in the Patterson study are examined quantitatively. In
	addition to the four factors of edition, condition, dust jacket, and
	autograph that were hypothesized to influence the value of a book, four
	other factors for which information was available in the data were
	examined
['out-of-print books', 'quantitative analysis', 'fair market value', 'pricing', 'economics', 'publisher']
['fair market value', 'factor', 'fot pt.1', 'datum', 'print book', 'patterson study', 'dust jacket', 'book', 'p.71', 'title']

Four factors influencing the fair market value of out-of-print books.1
Four factors (edition, condition, dust jacket, and autograph) that are
	hypothesized to influence the value of books are identified and linked
	to basic economic principles, which are explained. A sample of
	fifty-six titles is qualitatively examined to test the hypothesis
['fair market value', 'out-of-print books', 'economic principles', 'pricing']
['basic economic principle', 'fair market value', 'dust jacket', 'print books.1', 'factor', 'autograph', 'condition', 'edition', 'title', 'sample']

Acquiring materials in the history of science, technology, and medicine
This article provides detailed advice on acquiring new, out-of-print, and rare
	materials in the history of science, technology, and medicine for the
	beginner in these fields. The focus is on the policy formation, basic
	reference tools, and methods of collection development and acquisitions
	that are the necessary basis for success in this endeavor
['out-of-print books', 'special collections', 'library acquisitions', 'science', 'technology', 'medicine', 'rare materials', 'policy formation', 'basic referencetools', 'collection development']
['necessary basis', 'collection development', 'reference tool', 'material', 'detailed advice', 'history', 'policy formation', 'science', 'technology', 'medicine']

Information architecture: looking ahead
It may be a bit strange to consider where the field of information architecture
	(IA) is headed. After all, many would argue that it's too new to be
	considered as a field at all, or that it is mislabeled, and by no means
	is there a widely accepted definition of what information architecture
	actually is. Practicing information architects probably number in the
	thousands, and this vibrant group is already building various forms of
	communal infrastructure, ranging from an IA journal and a
	self-organizing "library" of resources to a passel of local
	professional groups and degree-granting academic programs. So the
	profession has achieved a beachhead that will enable it to stabilize
	and perhaps even grow during these difficult times
['information architecture', 'information architects', 'communal infrastructure', 'local professional groups', 'degree-granting academic programs']
['information architecture', 'bit strange', 'difficult time', 'academic program', 'professional group', 'ia journal', 'communal infrastructure', 'vibrant group', 'field', 'ia']

Decisions, decisions, decisions: a tale of special collections in the small
	academic library
A case study of a special collections department in a small academic library
	and how its collections have been acquired and developed over the years
	is described. It looks at the changes that have occurred in the
	academic environment and what effect, if any, these changes may have
	had on the department and how it has adapted to them. It raises
	questions about development and acquisitions policies and procedures
['special collections', 'small academic library', 'case study', 'acquisitions policies', 'out-of-print books', 'university library']
['small academic library', 'special collection department', 'special collection', 'academic library', 'case study', 'decision', 'academic environment', 'change', 'tale', 'effect']

Acquisitions in the James Ford Bell Library
This article presents basic acquisitions philosophy and approaches in a noted
	special collection, with commentary on "just saying no" and on how the
	electronic revolution has changed the acquisition of special
	collections materials
['james ford bell library', 'library acquisitions philosophy', 'out-of-print books', 'university library', 'special collections', 'electronic revolution']
['james ford bell library', 'basic acquisition philosophy', 'special collection', 'commentary', 'special', 'electronic revolution', 'approach', 'collection material', 'article']

Underground poetry, collecting poetry, and the librarian
A powerful encounter with underground poetry and its important role in poetry,
	literature, and culture is discussed. The acquisitions difficulties
	encountered in the unique publishing world of underground poetry are
	introduced. Strategies for acquiring underground poetry for library
	collections are proposed, including total immersion and local focus,
	with accompanying action
['librarian', 'underground poetry', 'publishing', 'library collections', 'out-of-printbooks', 'special collections', 'literature', 'culture']
['underground poetry', 'powerful encounter', 'important role', 'unique publishing world', 'acquisition difficulty', 'poetry', 'local focus', 'total immersion', 'librarian', 'culture']

On emotion and bounded rationality: reply to Hanoch
The author refers to the comment made by Hanoch (see ibid. vol.49 (2000)) on
	his model of bounded rationality and the role of the Yerkes-Dodson law
	and emotional arousal in it. The author points out that Hanoch's
	comment, however, conspicuously fails to challenge - much less
	contradict - the central hypothesis of his paper. In addition, several
	of Hanoch's criticisms are based on a wrong characterization of the
	positions
['emotion', 'bounded rationality', 'yerkes-dodson law', 'decision-making', 'psychology']
['bounded rationality', 'wrong characterization', 'central hypothesis', 'emotional arousal', 'hanoch', 'dodson law', 'author', 'comment', 'vol.49', 'ibid']

The effects of emotions on bounded rationality: a comment on Kaufman
Bruce Kaufman's article (1999), "Emotional arousal as a source of bounded
	rationality", objective is to present an additional source of bounded
	rationality, one that is not due to cognitive constraints, but to high
	emotional arousal. In doing so, Kaufman is following a long tradition
	of thinkers who have contrasted emotion with reason, claiming, for the
	most part, that emotions are a violent force hindering rational
	thinking. This paper aims to challenge Kaufman's unidimensional idea
	regarding the connection between high emotional arousal and decision
	making
['bounded rationality', 'decision making', 'rational thinking', 'psychology', 'emotion', 'yerkes-dodson law']
['emotional arousal', 'additional source', 'cognitive constraint', 'long tradition', 'violent force', 'bruce kaufman', 'kaufman', 'bounded rationality', 'bounded', 'rationality']

Emotion and self-control
A biology-based model of choice is used to examine time-inconsistent
	preferences and the problem of self-control. Emotion is shown to be the
	biological substrate of choice, in that emotional systems assign value
	to 'goods' in the environment and also facilitate the learning of
	expectations regarding alternative options for acquiring those goods. A
	third major function of the emotional choice systems is motivation.
	Self-control is shown to be the result of a problem with the inhibition
	of the motive force of emotion, where this inhibition is necessary for
	higher level deliberation
['choice model', 'inhibition', 'learning', 'time-inconsistent preferences', 'self-control', 'emotional choice systems', 'emotion']
['self', 'control', 'emotional choice system', 'high level deliberation', 'major function', 'emotional system', 'problem', 'alternative option', 'biology', 'biological substrate']

Product and process innovations in the life cycle of an industry
Filson (2001) uses industry-level data on firm numbers, price, quantity and
	quality along with an equilibrium model of industry evolution to
	estimate the nature and effects of quality and cost improvements in the
	personal computer industry and four other new industries. This paper
	studies the personal computer industry in more detail and shows that
	the model explains some peculiar patterns that cannot be explained by
	previous life-cycle models. The model estimates are evaluated using
	historical studies of the evolution of the personal computer industry
	and patterns that require further model development are described
['technological change', 'life-cycle models', 'industry dynamics', 'personal computermarket', 'microelectronics', 'equilibrium model', 'industry evolution', 'pcindustry', 'production cost']
['personal computer industry', 'industry evolution', 'cost improvement', 'firm number', 'new industry', 'equilibrium model', 'life cycle', 'level datum', 'process innovation', 'peculiar pattern']

A comparison of the discounted utility model and hyperbolic discounting models
	in the case of social and private intertemporal preferences for health
Whilst there is substantial evidence that hyperbolic discounting models
	describe intertemporal preferences for monetary outcomes better than
	the discounted utility (DU) model, there is only very limited evidence
	in the context of health outcomes. This study elicits private and
	social intertemporal preferences for non-fatal changes in health.
	Specific functional forms of the DU model and three hyperbolic models
	are fitted. The results show that the stationarity axiom is violated,
	and that the hyperbolic models fit the data better than the DU model.
	Intertemporal preferences for private and social decisions are found to
	be very similar
['discounted utility model', 'hyperbolic discounting models', 'intertemporalpreferences', 'health outcomes', 'private decisions', 'social decisions']
['hyperbolic discounting model', 'specific functional form', 'private intertemporal preference', 'monetary outcome good', 'social intertemporal preference', 'intertemporal preference', 'substantial evidence', 'du model', 'utility model', 'hyperbolic model']

Modeling the labor market as an evolving institution: model ARTEMIS
A stylized French labor market is modeled as an endogenously evolving
	institution. Boundedly rational firms and individuals strive to
	decrease the cost or increase utility. The labor market is coordinated
	by a search process and decentralized setting of hiring standards, but
	intermediaries can speed up matching. The model reproduces the dynamics
	of the gross flows and spectacular changes in mobility patterns of some
	demographic groups when the oil crisis in the 1970's occurred, notably
	the sudden decline of the integration in good jobs. The internal labor
	markets of large firms are shown to increase unemployment if the
	secondary (temporary or bad) jobs do not exist
['artemis model', 'french labor market', 'endogenously evolving institution', 'simulation model', 'jobs', 'endogenous intermediary', 'spectacular changes', 'mobility patterns', 'demographic groups']
['stylized french labor market', 'boundedly rational firm', 'labor market', 'model artemis', 'search process', 'oil crisis', 'demographic group', 'mobility pattern', 'spectacular change', 'sudden decline']

The ultimate control group
Empirical research on the organization of firms requires that firms be
	classified on the basis of their control structures. This should be
	done in a way that can potentially be made operational. It is easy to
	identify the ultimate controller of a hierarchical organization, and
	the literature has largely focused on this case. However, many
	organizational structures mix hierarchy with collective choice
	procedures such as voting, or use circular structures under which
	superiors are accountable to their subordinates. The author develops
	some analytic machinery that can be used to map the authority
	structures of such organizations, and show that under mild restrictions
	there is a well-defined ultimate control group. The results are
	consistent with intuitions about the nature of control in familiar
	economic settings
['ultimate control group', 'hierarchical organization', 'organizational structures', 'authority structures', 'committees', 'control rights', 'firm organization']
['ultimate control group', 'empirical research', 'control structure', 'hierarchical organization', 'collective choice', 'circular structure', 'organizational structure', 'ultimate controller', 'analytic machinery', 'structure']

Information architecture for bilingual Web sites
Creating an information architecture for a bilingual Web site presents
	particular challenges beyond those that exist for single and
	multilanguage sites. This article reports work in progress on the
	development of a content-based bilingual Web site to facilitate the
	sharing of resources and information between Speech and Language
	Therapists. The development of the information architecture is based on
	a combination of two aspects: an abstract structural analysis of
	existing bilingual Web designs focusing on the presentation of
	bilingual material, and a bilingual card-sorting activity conducted
	with potential users. Issues for bilingual developments are discussed,
	and some observations are made regarding the use of card-sorting
	activities
['information architecture', 'content-based bilingual web site', 'speech therapists', 'language therapists', 'bilingual card-sorting activity', 'bilingualdevelopments', 'world wide web']
['bilingual web site', 'information architecture', 'bilingual web design', 'abstract structural analysis', 'particular challenge', 'multilanguage site', 'information', 'development', 'bilingual card', 'bilingual material']

Modularity in technology and organization
The paper is an attempt to raid both the literature on modular design and the
	literature on property rights to create the outlines of a modularity
	theory of the firm. Such a theory will look at firms, and other
	organizations, in terms of the partitioning of rights-understood as
	protected spheres of authority-among cooperating parties. It will
	assert that organizations reflect nonmodular structures, that is,
	structures in which decision rights, rights of alienation, and residual
	claims to income do not all reside in the same hands
['modularity', 'technology', 'organization', 'property rights', 'partitioning of rights', 'authority', 'cooperating parties', 'nonmodular structures', 'decision rights', 'rights of alienation', 'transaction costs']
['decision right', 'organization', 'property right', 'modularity', 'nonmodular structure', 'modular design', 'right', 'literature', 'theory', 'firm']

Designing a new urban Internet
The parallel between designing a Web site and the construction of a building is
	a familiar one, but how often do we think of the Internet as having
	parks and streets? It would be absurd to say that the Internet could
	ever take the place of real, livable communities; however, it is safe
	to say that the context for using the Internet is on a path of change.
	As the Internet evolves beyond a simple linkage of disparate Web sites
	and applications, the challenge for Information Architects is
	establishing a process by which to structure, organize, and design
	networked environments. The principles that guide New Urbanism can
	offer much insight into networked electronic environment design. At the
	core of every New Urbanism principle is the idea of "wholeness"-of
	making sure that neighborhoods and communities are knit together in a
	way that supports civic activities, economic development, efficient
	ecosystems, aesthetic beauty, and human interaction
['web site', 'internet', 'information architects', 'private-public sector cooperation', 'global information networks', 'networked environments', 'networkedelectronic environment design', 'communities']
['new urbanism principle', 'new urban internet', 'electronic environment design', 'disparate web site', 'offer much insight', 'web site', 'internet', 'livable community', 'simple linkage', 'internet evolf']

Three-dimensional optimum design of the cooling lines of injection moulds based
	on boundary element design sensitivity analysis
A three-dimensional numerical simulation using the boundary element method is
	proposed, which can predict the cavity temperature distributions in the
	cooling stage of injection moulding. Then, choosing the radii and
	positions of cooling lines as design variables, the boundary integral
	sensitivity formulations are deduced. For the optimum design of cooling
	lines, the squared difference between the objective temperature and
	temperature of the cavity is taken as the objective function. Based on
	the optimization techniques with design sensitivity analysis, an
	iterative algorithm to reach the minimum value of the objective
	function is introduced, which leads to the optimum design of cooling
	lines at the same time
['injection moulding', '3d numerical simulation', 'boundary element method', 'cavitytemperature distributions', 'cooling stage', 'boundary integral sensitivityanalysis', 'iterative algorithm', 'heat conduction', 'objective function', 'optimization']
['boundary element design sensitivity analysis', 'optimum design', 'dimensional optimum design', 'cavity temperature distribution', 'boundary element method', 'dimensional numerical simulation', 'line', 'injection moulding', 'cooling stage', 'sensitivity formulation']

Managing safety and strategic stocks to improve materials requirements planning
	performance
This paper provides a methodology for managing safety and strategic stocks in
	materials requirements planning (MRP) environments to face uncertainty
	in market demand. A set of recommended guidelines suggest where to
	position, how to dimension and when to replenish both safety and
	strategic stocks. Trade-offs between stock positioning and dimensioning
	and between stock positioning and replenishment order triggering are
	outlined. The study reveals also that most of the decisions are system
	specific, so that they should be evaluated in a quantitative manner
	through simulation. A case study is reported, where the benefits from
	adopting the new proposed methodology lie in achieving the target
	service level even under peak demand conditions, with the value of
	safety stocks as a whole growing only by about 20 per cent
['mrp', 'materials requirements planning', 'market demand', 'strategic stocks', 'safetystocks', 'inventory management', 'variance control', 'stock replenishment', 'service level', 'peak demand']
['strategic stock', 'peak demand condition', 'material requirement planning', 'material requirement', 'safety', 'stock positioning', 'market demand', 'replenishment order', 'case study', 'quantitative manner']

Innovative manufacture of impulse turbine blades for wave energy power
	conversion
An innovative approach to the manufacture of impulse turbine blades using rapid
	prototyping, fused decomposition modelling (FDM), is presented. These
	blades were designed and manufactured by the Wave Energy Research Team
	(WERT) at the University of Limerick for the experimental analysis of a
	0.6 m impulse turbine with fixed guide vanes for wave energy power
	conversion. The computer aided design/manufacture (CAD/CAM) package
	Pro-Engineer 2000i was used for three-dimensional solid modelling of
	the individual blades. A detailed finite element analysis of the blades
	under centrifugal loads was performed using Pro-Mechanica. based on
	this analysis and FDM machine capabilities, blades were redesigned.
	Finally, Pro-E data were transferred to an FDM machine for the
	manufacture of turbine blades. The objective of this paper is to
	present the innovative method used to design, modify and manufacture
	blades in a time and cost effective manner using a concurrent
	engineering approach
['cad/cam', 'impulse turbine blades', 'wave energy power conversion', 'fuseddecomposition modelling', 'rapid prototyping', 'manufacturing', 'concurrentengineering', 'university of limerick', 'solid modelling', 'finite elementanalysis']
['impulse turbine blade', 'wave energy power', 'detailed finite element analysis', 'wave energy research team', 'fused decomposition modelling', 'm impulse turbine', 'turbine blade', 'dimensional solid modelling', 'manufacture', 'innovative manufacture']

Evaluation of combined dispatching and routeing strategies for a flexible
	manufacturing system
This paper deals with the evaluation of combined dispatching and routeing
	strategies on the performance of a flexible manufacturing system. Three
	routeing policies - no alternative routings, alternative routeing
	dynamics and alternative routeing plans - are considered with four
	dispatching rules with finite buffer capacity. In addition, the effect
	of changing part mix ratios is also discussed. The performance measures
	considered are makespan, average machine utilization, average flow time
	and average delay at local input buffers. Simulation results indicate
	that the alternative routings dynamic policy gives the best results in
	three performance measures except for average delay at local input
	buffers. Further, the effect of changing part mix ratios is not
	significant
['alternative routings', 'flexible manufacturing system', 'fms', 'dispatching rules', 'finite buffer capacity', 'part mix ratios', 'average flow time']
['alternative routing dynamic policy', 'local input buffer', 'average flow time', 'finite buffer capacity', 'average machine utilization', 'alternative routeing plan', 'flexible manufacturing system', 'mix ratio', 'manufacturing system', 'evaluation']

An intelligent fuzzy decision system for a flexible manufacturing system with
	multi-decision points
This paper describes an intelligent fuzzy decision support system for real-time
	scheduling and dispatching of parts in a flexible manufacturing system
	(FMS), with alternative routing possibilities for all parts. A fuzzy
	logic approach is developed to improve the system performance by
	considering multiple performance measures and at multiple decision
	points. The characteristics of the system status, instead of parts, are
	fed back to assign priority to the parts waiting to be processed. A
	simulation model is developed and it is shown that the proposed
	intelligent fuzzy decision support system keeps all performance
	measures at a good level. The proposed intelligent system is a
	promising tool for dealing with scheduling FMSs, in contrast to
	traditional rules
['flexible manufacturing system', 'fms', 'fuzzy logic', 'multiple decision points', 'intelligent decision support system', 'real-time system', 'scheduling', 'simulation']
['intelligent fuzzy decision support system', 'flexible manufacturing system', 'intelligent fuzzy decision system', 'part', 'multiple performance measure', 'decision point', 'system performance', 'logic approach', 'multiple decision', 'system status']

A design to cost system for innovative product development
Presents a prototype object-oriented and rule-based system for product cost
	modelling and design for automation at an early design stage. The
	developed system comprises a computer aided design (CAD) solid
	modelling system, a material selection module, a knowledge-based system
	(KBS), a process optimization module, a design for assembly module, a
	cost estimation module and a user interface. Two manufacturing
	processes, namely machining and injection moulding processes, were
	considered in the developed system. The main function of the system,
	besides estimating the product cost, is to generate initial process
	planning, including the generation and selection of machining
	processes, their sequence and their machining parameters, and to
	recommend the most economical assembly technique for a product and
	provide design improvement suggestions based on a design feasibility
	technique. In addition, a feature-by-feature cost estimation report is
	generated using the proposed system to highlight the features of high
	manufacturing cost. Two case studies were used to validate the
	developed system
['design to cost system', 'innovative product development', 'object-orientedrule-based system', 'product cost modelling', 'design for automation', 'computer aided design solid modelling system', 'material selectionmodule', 'knowledge-based system', 'process optimization module', 'design forassembly module', 'cost estimation module', 'user interface', 'machining', 'injection moulding', 'process planning', 'feature-by-feature costestimation report', 'fuzzy logic', 'object-oriented programming', 'concurrentengineering']
['feature cost estimation report', 'provide design improvement suggestion', 'injection moulding process', 'cost estimation module', 'economical assembly technique', 'process optimization module', 'material selection module', 'early design stage', 'innovative product development', 'system']

Re-examining the machining frictional boundary conditions using fractals
Presents experimental evidence for the existence of non-Euclidean contact
	geometry at the tool-chip interface in the machining of aluminium
	alloy, which challenges conventional assumptions. The geometry of
	contact at the tool rake face is modelled using fractals and a
	dimension is computed for its description. The variation in the fractal
	dimension with the cutting speed is explored
['machining frictional boundary conditions', 'fractals', 'noneuclidean contactgeometry', 'tool-chip interface', 'aluminium alloy', 'contact geometry', 'toolrake face', 'cutting speed', 'al']
['machining frictional boundary condition', 'presents experimental evidence', 'fractal', 'euclidean contact', 'tool rake face', 'chip interface', 'geometry', 'conventional assumption', 'contact', 'non']

Layer-based machining: recent development and support structure design
There is growing interest in additive and subtractive shaping theories that are
	synthesized to integrate the layered manufacturing process and material
	removal process. Layer-based machining has emerged as a promising
	method for integrated additive and subtractive shaping theory. In the
	paper, major layer-based machining systems are reviewed and compared
	according to characteristics of stock layers, numerical control
	machining configurations, stacking operations, input format and raw
	materials. Support structure, a major issue in machining-based systems
	which has seldom been addressed in previous research, is investigated
	in the paper with considerations of four situations: floating overhang,
	cantilever, vaulted overhang and ceiling. Except for the floating
	overhang where a support structure should not be overlooked, the
	necessity for support structures for the other three situations is
	determined by stress and deflection analysis. This is demonstrated by
	the machining of a large castle model
['layer-based machining', 'support structure design', 'additive shaping theories', 'subtractive shaping theories', 'layered manufacturing process', 'materialremoval process', 'stock layers', 'numerical control machiningconfigurations', 'stacking operations', 'input format', 'raw materials', 'floating overhang', 'cantilever', 'vaulted overhang', 'ceiling', 'stress', 'deflection analysis']
['support structure', 'layered manufacturing process', 'large castle model', 'support structure design', 'shaping theory', 'machining', 'removal process', 'machining system', 'machining configuration', 'major layer']

World's biggest battery helps to stabilise Alaska
In this paper, the author describes a battery energy storage system which is
	under construction to provide voltage compensation in support of
	Alaska's 138 kV Northern Intertie
['power system stabilisation', 'battery energy storage system', 'voltagecompensation', 'usa', 'interconnected power systems', '138 kv', '77 mw']
['battery energy storage system', 'big battery', 'alaska', 'voltage compensation', 'author', 'paper', 'kv northern intertie', 'construction', 'support', 'world']

Information interaction: providing a framework for information architecture
Information interaction is the process that people use in interacting with the
	content of an information system. Information architecture is a
	blueprint and navigational aid to the content of information-rich
	systems. As such information architecture performs an important
	supporting role in information interactivity. This article elaborates
	on a model of information interactivity that crosses the "no-man's
	land" between user and computer articulating a model that includes
	user, content and system, illustrating the context for information
	architecture
['information interaction', 'navigational aid', 'information-rich systems', 'information interactivity']
['information interaction', 'information architecture', 'process that people', 'information', 'information interactivity', 'information system', 'navigational aid', 'content', 'system', 'architecture']

All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers
The authors present a novel all-optical logic NOR gate using two-cascaded
	semiconductor optical. amplifiers (SOAs) in a counterpropagating
	feedback configuration. This configuration accentuates the gain
	nonlinearity due to the mutual gain modulation of the two SOAs. The
	all-optical NOR gate feasibility has been demonstrated delivering an
	extinction ratio higher than 12 dB over a wide range of wavelength
['all-optical logic nor gate', 'two-cascaded semiconductor optical amplifiers', 'soa', 'counterpropagating feedback configuration', 'gain nonlinearity', 'mutualgain modulation', 'extinction ratio', 'wide wavelength range']
['optical logic', 'extinction ratio high', 'mutual gain modulation', 'semiconductor optical amplifier', 'semiconductor optical', 'gate', 'feedback configuration', 'gate feasibility', 'novel', 'configuration']

Prospects for quantitative computed tomography imaging in the presence of
	foreign metal bodies using statistical image reconstruction
X-ray computed tomography (CT) images of patients bearing metal intracavitary
	applicators or other metal foreign objects exhibit severe artifacts
	including streaks and aliasing. We have systematically evaluated via
	computer simulations the impact of scattered radiation, the
	polyenergetic spectrum, and measurement noise on the performance of
	three reconstruction algorithms: conventional filtered backprojection
	(FBP), deterministic iterative deblurring, and a new iterative
	algorithm, alternating minimization (AM), based on a CT detector model
	that includes noise, scatter, and polyenergetic spectra. Contrary to
	the dominant view of the literature, FBP streaking artifacts are due
	mostly to mismatches between FBP's simplified model of CT detector
	response and the physical process of signal acquisition. Artifacts on
	AM images are significantly mitigated as this algorithm substantially
	reduces detector-model mismatches. However, metal artifacts are reduced
	to acceptable levels only when prior knowledge of the metal object in
	the patient, including its pose, shape, and attenuation map, are used
	to constrain AM's iterations. AM image reconstruction, in combination
	with object-constrained CT to estimate the pose of metal objects in the
	patient, is a promising approach for effectively mitigating metal
	artifacts and making quantitative estimation of tissue attenuation
	coefficients a clinical possibility
['quantitative computed tomography imaging', 'foreign metal bodies', 'statisticalimage reconstruction', 'metal artifact reduction', 'brachytherapy', 'medicaldiagnostic imaging', 'signal acquisition physical process', 'object-constrained ct', 'iterative algorithm', 'alternating minimization', 'ct detector model', 'noise', 'scatter', 'polyenergetic spectra', 'clinicalpossibility', 'deterministic iterative deblurring', 'filteredbackprojection']
['image reconstruction', 'ct detector', 'severe artifact', 'detector model', 'foreign object', 'metal intracavitary', 'new iterative', 'metal foreign', 'polyenergetic spectra', 'computer simulation']

Matching PET and CT scans of the head and neck area: Development of method and
	validation
Positron emission tomography (PET) provides important information on tumor
	biology, but lacks detailed anatomical information. Our aim in the
	present study was to develop and validate an automatic registration
	method for matching PET and CT scans of the head and neck. Three
	difficulties in achieving this goal are (1) nonrigid motions of the
	neck can hamper the use of automatic ridged body transformations; (2)
	emission scans contain too little anatomical information to apply
	standard image fusion methods; and (3) no objective way exists to
	quantify the quality of the match results. These problems are solved as
	follows: accurate and reproducible positioning of the patient was
	achieved by using a radiotherapy treatment mask. The proposed method
	makes use of the transmission rather than the emission scan. To obtain
	sufficient (anatomical) information for matching, two bed positions for
	the transmission scan were included in the protocol. A mutual
	information-based algorithm was used as a registration technique. PET
	and CT data were obtained in seven patients. Each patient had two CT
	scans and one PET scan. The datasets were used to estimate the
	consistency by matching PET to CT/sub 1/, CT/sub 1/ to CT/sub 2/, and
	CT/sub 2/ to PET using the full circle consistency test. It was found
	that using our method, consistency could be obtained of 4 mm and 1.3
	degrees on average. The PET voxels used for registration were 5.15 mm,
	so the errors compared quite favorably with the voxel size. Cropping
	the images (removing the scanner bed from images) did not improve the
	consistency of the algorithm. The transmission scan, however, could
	potentially be reduced to a single position using this approach. In
	conclusion, the represented algorithm and validation technique has
	several features that are attractive from both theoretical and
	practical point of view, it is a user-independent, automatic validation
	technique for matching CT and PET scans of the head and neck, which
	gives the opportunity to compare different image enhancements
['positron emission tomography scans', 'tumor biology', 'anatomical information', 'automatic registration method', 'computerised tomography scans', 'head', 'neck', 'nonrigid motions', 'automatic ridged body transformations', 'standardimage fusion methods', 'radiotherapy treatment mask', 'bed positions', 'transmission scan', 'mutual information-based algorithm', 'registrationtechnique', 'patients', 'full circle consistency test', 'errors', 'scanner bed', 'user-independent automatic validation technique', 'image enhancements']
['ct scan', 'anatomical information', 'transmission scan', 'pet scan', 'pet', 'emission scan', '-PRON- aim', 'present study', 'automatic registration', 'neck area']

Fresh tracks [food processing]
Bar code labels and wireless terminals linked to a centralized database
	accurately track meat products from receiving to customers for Farmland
	Foods
['food processing', 'bar code labels', 'wireless terminals', 'farmland foods', 'automaticdata capture', 'intermec technologies']
['bar code label', 'food processing', 'fresh track', 'wireless terminal', 'centralized database', 'meat product', 'food', 'farmland', 'customer']

Statistical inference with partial prior information based on a Gauss-type
	inequality
Potter and Anderson (1983) have developed a Bayesian decision procedure
	requiring the specification of a class of prior distributions
	restricted to have a minimal probability content for a given subset of
	the parameter space. They do not, however, provide a method for the
	selection of that subset. We show how a generalization of Gauss'
	inequality can be used to determine the relevant parameter subset
['bayesian decision procedure', 'prior distributions', 'minimal probability content', 'parameter space', 'gauss inequality', 'prior-to-posterior sensitivity', 'partial prior information']
['minimal probability content', 'relevant parameter subset', 'bayesian decision procedure', 'partial prior information', 'statistical inference', 'gauss', 'inequality', 'prior distribution', 'anderson', 'potter']

Global stability of the attracting set of an enzyme-catalysed reaction system
The essential feature of enzymatic reactions is a nonlinear dependency of
	reaction rate on metabolite concentration taking the form of saturation
	kinetics. Recently, it has been shown that this feature is associated
	with the phenomenon of "loss of system coordination" (Liu, 1999). In
	this paper, we study a system of ordinary differential equations
	representing a branched biochemical system of enzyme-mediated
	reactions. We show that this system can become very sensitive to
	changes in certain maximum enzyme activities. In particular, we show
	that the system exhibits three distinct responses: a unique,
	globally-stable steady-state, large amplitude oscillations, and
	asymptotically unbounded solutions, with the transition between these
	states being almost instantaneous. It is shown that the appearance of
	large amplitude, stable limit cycles occurs due to a "false"
	bifurcation or canard explosion. The subsequent disappearance of limit
	cycles corresponds to the collapse of the domain of attraction of the
	attracting set for the system and occurs due to a global bifurcation in
	the flow, namely, a saddle connection. Subsequently, almost all
	nonnegative data become unbounded under the action of the dynamical
	system and correspond exactly to loss of system coordination. We
	discuss the relevance of these results to the possible consequences of
	modulating such systems
['enzymatic reactions', 'nonlinear dependency', 'metabolite concentration', 'saturationkinetics', 'biochemical system', 'ordinary differential equations', 'enzyme-mediated reactions', 'saddle connection', 'stable limit cycles', 'bifurcation']
['system coordination', 'ordinary differential equation', 'large amplitude', 'system', 'amplitude oscillation', 'biochemical system', 'unbounded solution', 'enzyme activity', 'branched biochemical', 'maximum enzyme']

A spatial rainfall simulator for crop production modeling in Southern Africa
This paper describes a methodology for simulating rainfall in dekads across a
	set of spatial units in areas where long-term meteorological records
	are available for a small number of sites only. The work forms part of
	a larger simulation model of the food system in a district of Zimbabwe,
	which includes a crop production component for yields of maize, small
	grains and groundnuts. Only a limited number of meteorological stations
	are available within or surrounding the district that have long time
	series of rainfall records. Preliminary analysis of rainfall data for
	these stations suggested that intra-seasonal temporal correlation was
	negligible, but that rainfall at any given station was correlated with
	rainfall at neighbouring stations. This spatial correlation structure
	can be modeled using a multivariate normal distribution consisting of
	30 related variables, representing dekadly rainfall in each of the 30
	wards. For each ward, log-transformed rainfall for each of the 36
	dekads in the year was characterized by a mean and standard deviation,
	which were interpolated from surrounding meteorological stations. A
	covariance matrix derived from a distance measure was then used to
	represent the spatial correlation between wards. Sets of random numbers
	were then drawn from this distribution to simulate rainfall across the
	wards in any given dekad. Cross-validation of estimated rainfall
	parameters against observed parameters for the one meteorological
	station within the district suggests that the interpolation process
	works well. The methodology developed is useful in situations where
	long-term climatic records are scarce and where rainfall shows
	pronounced spatial correlation, but negligible temporal correlation
['simulating rainfall', 'crop production modeling', 'zimbabwe', 'covariance matrix', 'rainfall records', 'rainfall data', 'spatial correlation', 'multivariatenormal distribution', 'parameter estimation', 'southern africa']
['meteorological station', 'spatial correlation', 'temporal correlation', 'crop production', 'neighbouring station', 'rainfall datum', 'rainfall', 'seasonal temporal', 'preliminary analysis', 'rainfall record']

An algorithm to generate all spanning trees with flow
Spanning tree enumeration in undirected graphs is an important issue and task
	in many problems encountered in computer network and circuit analysis.
	This paper discusses the spanning tree with flow for the case that
	there are flow requirements between each node pair. An algorithm based
	on minimal paths (MPs) is proposed to generate all spanning trees
	without flow. The proposed algorithm is a structured approach, which
	splits the system into structural MPs first, and also all steps in it
	are easy to follow
['undirected graphs', 'spanning trees', 'minimal paths', 'computer network analysis', 'circuit analysis']
['structural mp', 'minimal path', 'structured approach', 'node pair', 'circuit analysis', 'flow requirement', 'computer network', 'algorithm', 'important issue', 'tree']

Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in
	the presence of body forces and sources
In this paper, we first give a formal derivation of several systems of
	equations for injection moulding. This is done starting from the basic
	equations for nonisothermal, non-Newtonian flows in a three-dimensional
	domain. We derive systems for both (T/sup 0/, p/sup 0/) and (T/sup 1/,
	p/sup 1/) in the presence of body forces and sources. We find that body
	forces and sources have a nonlinear effect on the systems. We also
	derive a nonlinear "Darcy law". Our formulation includes not only the
	pressure gradient, but also body forces and sources, which play the
	role of a nonlinearity. Later, we prove the existence of weak solutions
	to certain boundary value problems and initial-boundary value problems
	associated with the resulting equations for (T/sup 0/, p/sup 0/) but in
	a more general mathematical setting
['injection moulding', 'body forces', 'sources', 'darcy law', 'nonlinear systems', 'boundary value problems', 'hele-shaw flows']
['body force', 'injection moulding', 'formal derivation', 'certain boundary value problem', 'system', 'nonlinear effect', 'boundary value problem', 'darcy law', '-PRON- formulation', 'nonlinear system']

Analyzing the potential of a firm: an operations research approach
An approach to analyzing the potential of a firm, which is understood as the
	firm's ability to provide goods or (and) services to be supplied to a
	marketplace under restrictions imposed by a business environment in
	which the firm functions, is proposed. The approach is based on using
	linear inequalities and, generally, mixed variables in modelling this
	ability for a broad spectrum of industrial, transportation,
	agricultural, and other types of firms and allows one to formulate
	problems of analyzing the potential of a firm as linear programming
	problems or mixed programming problems with linear constraints. This
	approach generalizes a previous one which was proposed for a more
	narrow class of models, and allows one to effectively employ a widely
	available software for solving practical problems of the considered
	kind, especially for firms described by large scale models of
	mathematical programming
['firm potential analysis', 'operations research', 'or', 'linear inequalities', 'industrial firms', 'transportation firms', 'agricultural firms', 'linearprogramming', 'mixed programming', 'large-scale models', 'mathematicalprogramming']
['mixed programming problem', 'large scale model', 'operation research approach', 'firm', 'potential', 'approach', 'broad spectrum', 'ability', 'mixed variable', 'firm function']

Discrete output feedback sliding mode control of second order systems - a
	moving switching line approach
The sliding mode control systems (SMCS) for which the switching variable is
	designed independent of the initial conditions are known to be
	sensitive to parameter variations and extraneous disturbances during
	the reaching phase. For second order systems this drawback is
	eliminated by using the moving switching line technique where the
	switching line is initially designed to pass the initial conditions and
	is subsequently moved towards a predetermined switching line. In this
	paper, we make use of the above idea of moving switching line together
	with the reaching law approach to design a discrete output feedback
	sliding mode control. The main contributions of this work are such that
	we do not require to use system states as it makes use of only the
	output samples for designing the controller. and by using the moving
	switching line a low sensitivity system is obtained through shortening
	the reaching phase. Simulation results show that the fast output
	sampling feedback guarantees sliding motion similar to that obtained
	using state feedback
['sliding mode control', 'switching variable', 'parameter variations', 'movingswitching line', 'discrete output feedback', 'fast output samplingfeedback', 'state feedback']
['discrete output feedback', 'second order system', 'switching line approach', 'mode control system', 'switching line technique', 'predetermined switching line', 'mode control', 'switching line', 'sampling feedback guarantee', 'initial condition']

When a better interface and easy navigation aren't enough: examining the
	information architecture in a law enforcement agency
An information architecture that allows users to easily navigate through a
	system and quickly recover from mistakes is often defined as a highly
	usable system. But usability in systems design goes beyond a good
	interface and efficient navigation. In this article we describe two
	database systems in a law enforcement agency. One system is a legacy,
	text-based system with cumbersome navigation (RMS); the newer system is
	a graphical user interface with simplified navigation (CopNet). It is
	hypothesized that law enforcement users will evaluate CopNet higher
	than RMS, but experts of the older system will evaluate it higher than
	others will. We conducted two user studies. One study examined what
	users thought of RMS and CopNet, and compared RMS experts' evaluations
	with nonexperts. We found that all users evaluated CopNet as more
	effective, easier to use, and easier to navigate than RMS, and this was
	especially noticeable for users who were not experts with the older
	system. The second, follow-up study examined use behavior after CopNet
	was deployed some time later. The findings revealed that evaluations of
	CopNet were not associated with its use. If the newer system had a
	better interface and was easier to navigate than the older, legacy
	system, why were law enforcement personnel reluctant to switch? We
	discuss reasons why switching to a new system is difficult, especially
	for those who are most adept at using the older system. Implications
	for system design and usability are also discussed
['information architecture', 'law enforcement agency', 'legacy text-based system', 'rms', 'graphical user interface', 'simplified navigation', 'copnet', 'lawenforcement users']
['law enforcement agency', 'law enforcement user', 'good interface', 'law enforcement personnel reluctant', 'graphical user interface', 'information architecture', 'system design', 'new system', 'easy navigation', 'system']

Optimization of planning an advertising campaign of goods and services
A generalization of the mathematical model and operations research problems
	formulated on its basis, which were presented by Belenky (2001) in the
	framework of an approach to planning an advertising campaign of goods
	and services, is considered, and corresponding nonlinear programming
	problems with linear constraints are formulated
['optimization', 'advertising campaign planning', 'operations research', 'or', 'nonlinearprogramming']
['advertising campaign', 'operation research problem', 'mathematical model', 'good', 'service', 'generalization', 'basis', 'belenky', 'nonlinear programming', 'linear constraint']

All-optical XOR gate using semiconductor optical amplifiers without additional
	input beam
The novel design of an all-optical XOR gate by using cross-gain modulation of
	semiconductor optical amplifiers has been suggested and demonstrated
	successfully at 10 Gb/s. Boolean AB and AB of the two input signals A
	and B have been obtained and combined to achieve the all-optical XOR
	gate. No additional input beam such as a clock signal or continuous
	wave light is used in this new design, which is required in other
	all-optical XOR gates
['semiconductor optical amplifiers', 'all-optical-xor gate', 'design', 'cross-gainmodulation', 'boolean logic', '10 gbit/s']
['semiconductor optical amplifier', 'optical xor gate', 'optical xor', 'additional input beam', 'input beam', 'novel design', 'boolean ab', 'input signal', 'clock signal', 'ab']

Trust in online advice
Many people are now influenced by the information and advice they find on the
	Internet, much of it of dubious quality. This article describes two
	studies concerned with those factors capable of influencing people's
	response to online advice. The first study is a qualitative account of
	a group of house-hunters attempting to find worthwhile information
	online. The second study describes a survey of more than 2,500 people
	who had actively sought advice over the Internet. A framework for
	understanding trust in online advice is proposed in which first
	impressions are distinguished from more detailed evaluations. Good Web
	design can influence the first process, but three key factors-source
	credibility, personalization, and predictability-are shown to predict
	whether people actually follow the advice given
['online advice trust', 'internet', 'survey', 'online mortgage advice', 'web design', 'source credibility', 'personalization', 'predictability', 'e-commerce', 'housebuying advice']
['online advice', 'many people', 'dubious quality', 'factor capable', 'qualitative account', 'advice', 'worthwhile information', 'key factor', 'people', 'second study']

The social impact of Internet gambling
Technology has always played a role in the development of gambling practices
	and continues to provide new market opportunities. One of the fastest
	growing areas is that of Internet gambling. The effect of such
	technologies should not be accepted uncritically, particularly as there
	may be areas of potential concern based on what is known about problem
	gambling offline. This article has three aims. First, it overviews some
	of the main social concerns about the rise of Internet gambling.
	Second, it looks at the limited research that has been carried out in
	this area. Third, it examines whether Internet gambling is doubly
	addictive, given research that suggests that the Internet can be
	addictive itself. It is concluded that technological developments in
	Internet gambling will increase the potential for problem gambling
	globally, but that many of the ideas and speculations outlined in this
	article need to be addressed further by large-scale empirical studies
['social impact', 'internet gambling', 'market opportunities', 'technologicaldevelopments', 'addiction', 'electronic cash', 'psychology']
['internet gambling', 'main social concern', 'scale empirical study', 'gambling practice', 'new market opportunity', 'potential concern', 'gambling offline', 'social impact', 'limited research', 'area']

Computer-mediated communication and remote management: integration or
	isolation?
The use of intranets and e-mails to communicate with remote staff is increasing
	rapidly within organizations. For many companies this is viewed as a
	speedy and cost-effective way of keeping in contact with staff and
	ensuring their continuing commitment to company goals. This article
	highlights the problems experienced by staff when managers use
	intranets and e-mails in an inappropriate fashion for these purposes.
	Issues of remoteness and isolation are discussed, along with the
	reports of frustration and disidentification experienced. However, it
	will be shown that when used appropriately, communication using these
	technologies can facilitate shared understanding and help remote staff
	to view their company as alive and exciting. Theoretical aspects are
	highlighted and the implications of these findings are discussed
['computer-mediated communication', 'remote management', 'intranets', 'e-mails', 'remotestaff', 'organizations', 'companies', 'cost-effective', 'managers', 'remoteness']
['remote staff', 'effective way', 'company goal', 'inappropriate fashion', 'communication', 'remote management', 'isolation', 'intranet', 'staff', 'mail']

Collective action in the age of the Internet: mass communication and online
	mobilization
This article examines how the Internet transforms collective action. Current
	practices on the Web bear witness to thriving collective action ranging
	from persuasive to confrontational, individual to collective,
	undertakings. Even more influential than direct calls for action is the
	indirect mobilizing influence of the Internet's powers of mass
	communication, which is boosted by an antiauthoritarian ideology on the
	Web. Theoretically, collective action through the otherwise socially
	isolating computer is possible because people rely on internalized
	group memberships and social identities to achieve social involvement.
	Empirical evidence from an online survey among environmental activists
	and nonactivists confirms that online action is considered an
	equivalent alternative to offline action by activists and nonactivists
	alike. However, the Internet may slightly alter the motives underlying
	collective action and thereby alter the nature of collective action and
	social movements. Perhaps more fundamental is the reverse influence
	that successful collective action will have on the nature and function
	of the Internet
['internet', 'mass communication', 'online mobilization', 'collective action', 'worldwide web', 'antiauthoritarian ideology', 'group memberships', 'socialidentities', 'online survey', 'anonymity', 'politics']
['collective action', 'mass communication', 'web bear witness', 'successful collective action', 'direct call', 'internet', 'antiauthoritarian ideology', 'group membership', 'social identity', 'social involvement']

Explanations for the perpetration of and reactions to deception in a virtual
	community
Cases of identity deception on the Internet are not uncommon. Several cases of
	a revealed identity deception have been reported in the media. The
	authors examine a case of deception in an online community composed
	primarily of information technology professionals. In this case, an
	established community member (DF) invented a character (Nowheremom)
	whom he fell in love with and who was eventually killed in a tragic
	accident. When other members of the community eventually began to
	question Nowheremom's actual identity, DF admitted that he invented
	her. The discussion board was flooded with reactions to DF's
	revelation. The authors propose several explanations for the
	perpetration of identity deception, including psychiatric illness,
	identity play, and expressions of true self. They also analyze the
	reactions of community members and propose three related explanations
	(social identity, deviance, and norm violation) to account for their
	reactions. It is argued that virtual communities' reactions to such
	threatening events provide invaluable clues for the study of group
	processes on the Internet
['virtual community', 'identity deception', 'internet', 'online community', 'informationtechnology professionals', 'psychiatric illness', 'group processes', 'socialprocesses', 'web sites', 'psychology', 'bulletin boards']
['identity deception', 'information technology professional', 'community member', 'several case', 'reaction', 'online community', 'case', 'explanation', 'deception', 'perpetration']

The effects of asynchronous computer-mediated group interaction on group
	processes
This article reports a study undertaken to investigate some of the social
	psychological processes underlying computer-supported group discussion
	in natural computer-mediated contexts. Based on the concept of
	deindividuation, it was hypothesized that personal identifiability and
	group identity would be important factors that affect the perceptions
	and behavior of members of computer-mediated groups. The degree of
	personal identifiability and the strength of group identity were
	manipulated across groups of geographically dispersed computer users
	who took part in e-mail discussions during a 2-week period. The results
	do not support the association between deindividuation and uninhibited
	behavior cited in much previous research. Instead, the data provide
	some support for a social identity perspective of computer-mediated
	communication, which explains the higher levels uninhibited in
	identifiable computer-mediated groups. However, predictions based on
	social identity theory regarding group polarization and group cohesion
	were not supported. Possible explanations for this are discussed and
	further research is suggested to resolve these discrepancies
['asynchronous computer-mediated group interaction', 'group processes', 'socialissues', 'psychology', 'deindividuation', 'internet', 'personalidentifiability', 'group identity', 'geographically dispersed computerusers', 'e-mail discussions', 'social identity theory', 'group polarization', 'group cohesion']
['social identity theory', 'personal identifiability', 'group identity', 'social identity perspective', 'important factor', 'group', '2-week period', 'computer', 'previous research', 'mail discussion']

Online longitudinal survey research: viability and participation
This article explores the viability of conducting longitudinal survey research
	using the Internet in samples exposed to trauma. A questionnaire
	battery assessing psychological adjustment following adverse life
	experiences was posted online. Participants who signed up to take part
	in the longitudinal aspect of the study were contacted 3 and 6 months
	after initial participation to complete the second and third waves of
	the research. Issues of data screening and sample attrition rates are
	considered and the demographic profiles and questionnaire scores of
	those who did and did not take part in the study during successive time
	points are compared. The results demonstrate that it is possible to
	conduct repeated measures survey research online and that the
	similarity in characteristics between those who do and do not take part
	during successive time points mirrors that found in traditional
	pencil-and-paper trauma surveys
['online longitudinal survey research', 'internet', 'trauma', 'questionnaire', 'psychological adjustment', 'data screening', 'sample attrition rates', 'demographic profiles', 'world wide web', 'psychology research']
['online longitudinal survey research', 'successive time point mirror', 'longitudinal survey research', 'sample attrition rate', 'adverse life', 'psychological adjustment', 'longitudinal aspect', 'initial participation', 'questionnaire score', 'datum screening']

Internet-based psychological experimenting: five dos and five don'ts
Internet-based psychological experimenting is presented as a method that needs
	careful consideration of a number of issues-from potential data
	corruption to revealing confidential information about participants.
	Ten issues are grouped into five areas of actions to be taken when
	developing an Internet experiment (dos) and five errors to be avoided
	(don'ts). Dos include: (a) utilizing dropout as a dependent variable,
	(b) the use of dropout to detect motivational confounding, (c)
	placement of questions for personal information, (d) using a collection
	of techniques, and (e) using Internet-based tools. Don'ts are about:
	(a) unprotected directories, (b) public access to confidential data,
	(c) revealing the experiment's structure, (d) ignoring the Internet's
	technical variance, and (e) improper use of form elements
['internet-based psychological experimenting', 'data corruption', 'dataconfidentiality', 'dropout', 'motivational confounding', 'personalinformation', 'unprotected directories', 'web experiment', 'online researchtechniques', 'psychology']
['psychological experimenting', 'form element', 'confidential datum', 'internet', 'improper use', 'technical variance', 'personal information', 'public access', 'motivational confounding', 'confidential information']

Pervasive computing goes to work: interfacing to the enterprise
The paperless office is an idea whose time has come, and come, and come again.
	To see how pervasive computing applications might bring some substance
	to this dream, the author spoke recently with key managers and
	technologists at McKesson Corporation (San Francisco), a healthcare
	supplier, service, and technology company with US$50 billion in sales
	last year, and also at AvantGo (Hayward, Calif.), a provider of mobile
	infrastructure software and services. For the past several years,
	McKesson has used mobility middleware developed by AvantGo to deploy
	major supply chain applications with thousands of pervasive clients and
	multiple servers that replace existing paper-based tracking systems.
	According to McKesson's managers, their system greatly reduced errors
	and associated costs caused by redelivery or loss of valuable products,
	giving McKesson a solid return on its investment
['paperless office', 'pervasive clients', 'multiple servers', 'mobile workers', 'enterprise resource planning', 'data warehousing']
['major supply chain application', 'pervasive computing application', 'past several year', 'idea whose time', 'pervasive computing', 'paperless office', 'mckesson', 'san francisco', 'technology company', 'mckesson corporation']

Psychology and the Internet
This article presents an overview of the way that the Internet is being used to
	assist psychological research and mediate psychological practice. It
	shows how psychologists are using the Internet to examine the
	interactions between people and computers, and highlights some of the
	ways that this research is important to the design and development of
	useable and acceptable computer systems. In particular, this
	introduction reviews the research presented at the International
	Conference on Psychology and the Internet held in the United Kingdom.
	The final part introduces the eight articles in this special edition.
	The articles are representative of the breadth of research being
	conducted on psychology and the Internet: there are two on
	methodological issues, three on group processes, one on organizational
	implications, and two on social implications of Internet use
['internet', 'psychological research', 'human-computer interactions', 'usability', 'social implications', 'psychology', 'organizational implications', 'groupprocesses', 'methodological issues', 'online research']
['internet', 'acceptable computer system', 'psychology', 'assist psychological research', 'article', 'way', 'research', 'overview', 'psychological practice', 'psychologist']

Extended depth-of-focus imaging of chlorophyll fluorescence from intact leaves
Imaging dynamic changes in chlorophyll a fluorescence provides a valuable means
	with which to examine localised changes in photosynthetic function.
	Microscope-based systems provide excellent spatial resolution which
	allows the response of individual cells to be measured. However, such
	systems have a restricted depth of focus and, as leaves are inherently
	uneven, only a small proportion of each image at any given focal plane
	is in focus. In this report we describe the development of algorithms,
	specifically adapted for imaging chlorophyll fluorescence and
	photosynthetic function in living plant cells, which allow
	extended-focus images to be reconstructed from images taken in
	different focal planes. We describe how these procedures can be used to
	reconstruct images of chlorophyll fluorescence and calculated
	photosynthetic parameters, as well as producing a map of leaf topology.
	The robustness of this procedure is demonstrated using leaves from a
	number of different plant species
['chlorophyll fluorescence', 'intact leaves', 'extended depth-of-focus imaging', 'leaftopology map', 'plant species', 'calculated photosynthetic parameters', 'individual cells response', 'microscope-based systems', 'charge-coupleddevice', 'maximum fluorescence yield', 'minimum fluorescence yield', 'variable fluorescence', 'numerical aperture', 'primary quinone acceptor', 'spatial resolution', 'algorithms development', 'extended-focus imagesreconstruction', 'biophysical research technique']
['chlorophyll fluorescence', 'photosynthetic function', 'different focal plane', 'different plant specie', 'excellent spatial resolution', 'dynamic change', 'intact leaf', 'valuable mean', 'individual cell', 'restricted depth']

Allan variance and fractal Brownian motion
Noise filtering is the subject of a voluminous literature in radio engineering.
	The methods of filtering require knowledge of the frequency response,
	which is usually unknown. D.W. Allan (see Proc. IEEE, vol.54, no.2,
	p.221-30, 1966; IEEE Trans. Instr. Measur., vol.IM-36, p.646-54, 1987)
	proposed a simple method of determining the interval between equally
	accurate observations which does without this information. In this
	method, the variances of the increments of noise and signal are equal,
	so that, in observations with a greater step, the variations caused by
	noise are smaller than those caused by the signal. This method is the
	standard accepted by the USA metrology community. The present paper is
	devoted to a statistical analysis of the Allan method and acquisition
	of additional information
['allan variance', 'fractal brownian motion', 'noise filtering', 'radio engineering', 'frequency response', 'usa metrology community', 'statistical analysis', 'white noise']
['fractal brownian motion', 'usa metrology community', 'allan variance', 'radio engineering', 'd.w. allan', 'method', 'frequency response', 'noise', 'noise filtering', 'voluminous literature']

Ideal sliding mode in the problems of convex optimization
The characteristics of the sliding mode that appears with using continuous
	convex-programming algorithms based on the exact penalty functions were
	discussed. For the case under study, the ideal sliding mode was shown
	to occur in the absence of infinite number of switchings
['ideal sliding mode', 'convex optimization', 'continuous convex-programmingalgorithms', 'exact penalty functions']
['exact penalty function', 'mode', 'ideal', 'programming algorithm', 'convex optimization', 'convex', 'problem', 'characteristic', 'continuous', 'infinite number']

Automation of the recovery of efficiency of complex structure systems
Basic features are set forth of the method for automation of the serviceability
	recovery of systems of complex structures in real time without the
	interruption of operation. Specific features of the method are revealed
	in an important example of the system of control of hardware components
	of ships
['efficiency recovery', 'serviceability recovery', 'complex structure systems', 'ships', 'hardware components']
['complex structure system', 'automation', 'complex structure', 'basic feature', 'recovery', 'real time', 'specific feature', 'method', 'efficiency', 'system']

Control of combustion processes in an internal combustion engine by
	low-temperature plasma
A new method of operation of internal combustion engines enhances power and
	reduces fuel consumption and exhaust toxicity. Low-temperature plasma
	control combines working processes of thermal engines and steam
	machines into a single process
['combustion processes', 'internal combustion engine', 'low-temperature plasma', 'fuelconsumption', 'exhaust toxicity', 'working processes', 'thermal engines', 'steam machines']
['internal combustion engine', 'reduce fuel consumption', 'temperature plasma', 'control', 'new method', 'combustion process', 'low', 'thermal engine', 'working process', 'operation']

Optimization of the characteristics of computational processes in scalable
	resources
The scalableness of resources is taken to mean the possibility of the prior
	change in the obtained dynamic characteristics of computational
	processes for a certain basic set of processors and the communication
	medium in an effort to optimize the dynamics of software applications.
	A method is put forward for the generation of optimal strategies-a set
	of the versions of the fulfillment of programs on the basis of a vector
	criterion. The method is urgent for the effective use of resources of
	computational clusters and metacomputational media and also for dynamic
	control of processes in real time on the basis of the static scaling
['computational processes', 'scalable resources', 'dynamic characteristics', 'communication medium', 'software applications', 'optimal strategies', 'vectorcriterion', 'computational clusters', 'metacomputational media', 'dynamiccontrol', 'static scaling']
['certain basic set', 'resource', 'computational process', 'dynamic characteristic', 'characteristic', 'process', 'software application', 'optimal strategy', 'dynamic', 'metacomputational medium']

The p-p rearrangement and failure-tolerance of double p-ary multirings and
	generalized hypercubes
It is shown that an arbitrary grouped p-element permutation can be implemented
	in a conflict-free way through the commutation of channels on the
	double p-ary multiring or the double p-ary hypercube. It is revealed
	that in arbitrary single-element permutations, these commutators
	display the property of the (p-1)-nodal failure-tolerance and the
	generalized hypercube displays in addition the property of the
	(p-1)-channel failure-tolerance
['p-p rearrangement', 'failure-tolerance', 'double p-ary multirings', 'generalizedhypercubes', 'p-element permutation', 'conflict-free implementation', 'single-element permutations', 'commutators']
['double p', 'ary multiring', 'element permutation', 'generalized hypercube', 'free way', 'p-1)-channel failure', 'ary hypercube', 'tolerance', 'arbitrary single', 'hypercube display']

Solutions for cooperative games
A new concept of the characteristic function is defined. It matches cooperative
	games far better than the classical characteristic function and is
	useful in reducing the number of decisions that can be used as the
	unique solution of a game
['cooperative games', 'characteristic function', 'decisions', 'unique solution', 'transferrable utility']
['characteristic function', 'classical characteristic function', 'new concept', 'game', 'cooperative game', 'solution', 'cooperative', 'unique solution', 'useful', 'decision']

Location of transport nets on a heterogeneous territory
The location of transport routes on a heterogeneous territory is studied. The
	network joins a given set of terminal points and a certain number of
	additional (branch) points. The problem is formulated, properties of
	the optimal solution for a. tree-like network, and the number of branch
	points are studied. A stepwise optimization algorithm for a. network
	with given adjacency matrix based on an algorithm for constructing
	minimal-cost routes is designed
['transport nets', 'heterogeneous territory', 'transport routes', 'terminal points', 'branch points', 'tree-like network', 'stepwise optimization algorithm', 'adjacency matrix']
['heterogeneous territory', 'stepwise optimization algorithm', 'like network', 'transport route', 'certain number', 'a. tree', 'a. network', 'adjacency matrix', 'optimal solution', 'terminal point']

Knowledge management-capturing the skills of key performers in the power
	industry
The growing pressure to reduce the cost of electrical power in recent years has
	resulted in an enormous "brain-drain" within the power industry. A
	novel approach has been developed by Eskom to capture these skills
	before they are lost and to incorporate these into a computer-based
	programme called "knowledge management"
['power industry', 'key performers', 'knowledge management', 'skills capture', 'brain-drain', 'eskom', 'computer-based programme', 'south africa', 'personnelmanagement']
['knowledge management', 'key performer', 'recent year', 'electrical power', 'power industry', 'novel approach', 'skill', 'power', 'industry', 'pressure']

Control in active systems based on criteria and motivation
For active systems where the principal varies the agents' goal functions by
	adding to them appropriately weighted goal functions of other agents or
	a balanced system of inter-agent transfers, the paper formulated and
	solved the problems of control based on criteria and motivation. Linear
	active systems were considered by way of example
['goal functions', 'inter-agent transfers', 'linear active systems', 'criteria-basedcontrol', 'motivation-based control']
['active system', 'goal function', 'balanced system', 'agent transfer', 'control', 'criterion', 'motivation', 'agent', 'principal', 'inter']

Flexibility analysis of complex technical systems under uncertainty
An important problem in designing technical systems under partial uncertainty
	of the initial physical, chemical, and technological data is the
	determination of a design in which the technical system is flexible,
	i.e., its control system is capable of guaranteeing that the
	constraints hold even under changes in external and internal factors
	and application of fuzzy mathematical models in its design. Three
	flexibility problems, viz., the flexibility of a technical system of
	given structure, structural flexibility of a technical system, and the
	optimal design guaranteeing the flexibility of a technical system, are
	studied. Two approaches to these problems are elaborated. Results of a
	computation experiment are given
['flexibility analysis', 'complex technical systems', 'partial uncertainty', 'controlsystem', 'fuzzy mathematical models', 'structural flexibility', 'optimaldesign']
['technical system', 'fuzzy mathematical model', 'complex technical system', 'important problem', 'partial uncertainty', 'initial physical', 'technological datum', 'control system', 'flexibility', 'flexibility analysis']

A fuzzy logic adaptation circuit for control systems of deformable space
	vehicles: its design
A fuzzy-logic adaptation algorithm is designed for adjusting the discreteness
	period of a control system for ensuring the stability and quality of
	control process with regard to the elastic structural vibrations of a
	deformable space vehicle. Its performance is verified by digital
	modeling of a discrete control system with two objects
['fuzzy logic adaptation circuit', 'control systems', 'deformable space vehicles', 'discreteness period', 'stability', 'elastic structural vibrations', 'digitalmodeling']
['fuzzy logic adaptation circuit', 'control system', 'deformable space', 'logic adaptation algorithm', 'deformable space vehicle', 'elastic structural vibration', 'control process', 'discrete control system', '-PRON- performance', 'quality']

"Hidden convexity" of finite-dimensional stationary linear discrete-time
	systems under conical constraints
New properties of finite-dimensional linear discrete-time systems under conical
	control constraints that are similar to the "hidden convexity" of
	continuous-time systems are studied
['hidden convexity', 'finite-dimensional stationary linear discrete-time systems', 'conical constraints', 'control constraint']
['dimensional stationary linear discrete', 'dimensional linear discrete', 'time system', 'finite', 'convexity', 'control constraint', 'new property', 'conical constraint', 'time', 'system']

The set of stable polynomials of linear discrete systems: its geometry
The multidimensional stability domain of linear discrete systems is studied.
	Its configuration is determined from the parameters of its intersection
	with coordinate axes, coordinate planes, and certain auxiliary planes.
	Counterexamples for the discrete variant of the Kharitonov theorem are
	given
['stable polynomials', 'kharitonov theorem', 'characteristic polynomial', 'lineardiscrete systems', 'geometry', 'multidimensional stability domain']
['linear discrete system', 'multidimensional stability domain', 'certain auxiliary plane', 'stable polynomial', '-PRON- configuration', 'coordinate plane', 'coordinate ax', 'discrete variant', 'kharitonov theorem', 'intersection']

Stochastic systems with a random jump in phase trajectory: stability of their
	motions
The probabilistic stability of the perturbed motion of a system with parameters
	under the action of a general Markov process is studied. The phase
	vector is assumed to experience random jumps when the structure the
	system suffers random jumps. Such a situation is encountered, for
	example, in the motion of a solid with random jumps in its mass. The
	mean-square stability of random-structure linear systems and stability.
	of nonlinear systems in the first approximation are studied. The
	applied approach is helpful in studying the asymptotic probabilistic
	stability and mean-square exponential stability of stochastic systems
	through the stability of the respective deterministic systems
['stochastic systems', 'random jump', 'phase trajectory', 'general markov process', 'asymptotic probabilistic stability', 'mean-square exponential stability']
['random jump', 'stochastic system', 'phase trajectory', 'respective deterministic system', 'perturbed motion', 'probabilistic stability', 'square exponential stability', 'structure linear system', 'general markov process', 'stability']

A nonlinear time-optimal control problem
Sufficient conditions for the existence of an optimal control in a time-optimal
	control problem with fixed ends for a smooth nonlinear control system
	are formulated. The properties of this system for characterizing the
	optimal control switching points are studied
['nonlinear time-optimal control problem', 'sufficient existence conditions', 'smoothnonlinear control system', 'optimal control switching points']
['optimal control switching point', 'smooth nonlinear control system', 'optimal control problem', 'control problem', 'sufficient condition', 'nonlinear time', 'time', 'existence', 'end', 'property']

System embedding. Polynomial equations
The class of solutions of the polynomial equations including their
	generalizations in the form of the Bezout matrix identities was
	constructed analytically using the technology of constructive system
	embedding. The structure of a solution depends on the number of steps
	of the Euclidean algorithm and is obtained explicitly by appropriate
	substitutions. Illustrative and descriptive examples are presented
['determinate systems', 'polynomial equations', 'bezout matrix identities', 'constructive system embedding', 'euclidean algorithm']
['polynomial equation', 'bezout matrix identity', 'system embedding', 'constructive system', 'solution', 'euclidean algorithm', 'embedding', 'descriptive example', 'form', 'generalization']

An optimal control algorithm based on reachability set approximation and
	linearization
The terminal functional of a general control system is refined by studying an
	analogous problem for a variational system and regularization. A
	sequential refinement method is designed by combining the local
	approximation of the reachability set and reduction. The corresponding
	algorithm has relaxation properties. An illustrative example is given
['determinate systems', 'optimal control algorithm', 'reachability set approximation', 'linearization', 'terminal functional', 'variational system', 'regularization', 'sequential refinement method', 'local approximation', 'relaxationproperties']
['sequential refinement method', 'general control system', 'optimal control algorithm', 'approximation', 'reachability', 'terminal functional', 'variational system', 'analogous problem', 'linearization', 'regularization']

A universal decomposition of the integration range for exponential functions
The problem of determining the independent constants for decomposition of the
	integration range of exponential functions was solved on the basis of a
	similar approach to polynomials. The constants obtained enable one to
	decompose the integration range in two so that the integrals over them
	are equal independently of the function parameters. For the
	nontrigonometrical polynomials of even functions, an alternative
	approach was presented
['integration range universal decomposition', 'exponential functions', 'polynomials', 'integration range decomposition', 'nontrigonometrical polynomials', 'evenfunctions']
['integration range', 'exponential function', 'independent constant', 'similar approach', 'nontrigonometrical polynomial', 'universal decomposition', 'function parameter', 'decomposition', 'function', 'constant']

An application of fuzzy linear regression to the information technology in
	Turkey
Fuzzy set theory deals with the vagueness of human thought. A major
	contribution of fuzzy set theory is its capability of representing
	vague knowledge. Fuzzy set theory is very practical when sufficient and
	reliable data isn't available. Information technology (IT) is the
	acquisition, processing, storage and dissemination of information in
	all its forms (auditory, pictorial, textual and numerical) through a
	combination of computers, telecommunication, networks and electronic
	devices. IT includes matters concerned with the furtherance of computer
	science and technology, design, development, installation and
	implementation of information systems and applications. In the paper,
	assuming that there are n independent variables and the regression
	function is linear, the possible levels of information technology (the
	sale levels of computer equipment) in Turkey are forecasted by using
	fuzzy linear regression. The independent variables assumed are the
	import level and the export level of computer equipment
['fuzzy linear regression', 'information technology', 'turkey', 'vague knowledgerepresentation', 'it', 'computers', 'telecommunication', 'electronic devices', 'computer science', 'computer technology', 'information systems', 'regressionfunction', 'computer equipment export level']
['fuzzy set theory', 'fuzzy set theory deal', 'fuzzy linear regression', 'information technology', 'application', 'human thought', 'turkey', 'vague knowledge', 'reliable datum', 'information']

Synchronizing experiments with linear interval systems
Concerns generalized control problems without exact information. <P>A
	method of constructing a minimal synchronizing sequence for a linear
	interval system over the field of real numbers is developed. This
	problem is reduced to a system of linear inequalities
['synchronizing experiments', 'linear interval systems', 'minimal synchronizingsequence construction', 'real numbers', 'linear inequalities', 'generalizedcontrol problems', 'controllability']
['minimal synchronizing sequence', 'linear interval system', 'interval system', 'exact information', 'control problem', 'problem', 'real number', 'method', 'concern', 'field']

Diagnosis of the technical state of heat systems
A step-by-step approach to the diagnosis of the technical state of heat systems
	is stated. The class of physical defects is supplemented by the
	behavioral defects of objects, which are related to the disturbance of
	the modes of their operation. The implementation of the approach is
	illustrated by an example of the solution of a specific problem of the
	diagnosis of a closed heat consumption system
['heat system technical state diagnosis', 'step-by-step diagnosis', 'operational modedisturbance', 'closed heat consumption system diagnosis']
['technical state', 'heat system', 'step approach', 'closed heat consumption system', 'diagnosis', 'behavioral defect', 'specific problem', 'physical defect', 'approach', 'step']

Fault-tolerant computer-aided control systems with multiversion-threshold
	adaptation: adaptation methods, reliability estimation, and choice of
	an architecture
For multiversion majority-redundant computer-aided control systems,
	systematization of adaptation methods that are stable to hardware and
	software failures, a method for estimating their reliability from an
	event graph model, and a method for selecting a standard architecture
	with regard for reliability requirements are studied
['fault-tolerant computer-aided control systems', 'multiversion-thresholdadaptation', 'reliability estimation', 'architecture', 'multiversionmajority-redundant computer-aided control systems', 'hardware failurestability', 'software failure stability', 'event graph model']
['control system', 'adaptation method', 'reliability estimation', 'event graph model', 'multiversion majority', 'redundant computer', 'software failure', 'tolerant computer', 'method', 'standard architecture']

Nonlockability in multirings and hypercubes at serial transmission of data
	blocks
For the multiring and hypercube, a method of conflictless realization of an
	arbitrary permutation of "large" data items that can be divided into
	many "smaller" data blocks was considered, and its high efficiency was
	demonstrated
['nonlockability', 'multirings', 'hypercubes', 'data block serial transmission', 'multiprocessor computer systems']
['high efficiency', 'datum block', 'datum item', 'multiring', 'arbitrary permutation', 'hypercube', 'serial transmission', 'conflictless realization', 'datum', 'block']

Linear models of circuits based on the multivalued components
Linearization and planarization of the circuit models is pivotal to the
	submicron technologies. On the other hand, the characteristics of the
	VLSI circuits can be sometimes improved by using the multivalued
	components. It was shown that any l-level circuit based on the
	multivalued components is representable as an algebraic model based on
	l linear arithmetic polynomials mapped correspondingly into l decision
	diagrams that are linear and planar by nature. Complexity of
	representing a circuit as the linear decision diagram was estimated as
	O(G) with G for the number of multivalued components in the circuit.
	The results of testing the LinearDesignMV algorithm on circuits of more
	than 8000 LGSynth 93 multivalued components were presented
['linear circuit model', 'linearization', 'planarization', 'submicron technologies', 'vlsi circuits', 'linear arithmetic polynomials', 'linear planar decisiondiagrams', 'circuit representation complexity', 'lineardesignmv algorithm', 'lgsynth 93 multivalued components']
['l linear arithmetic polynomial', 'circuit', 'component', 'vlsi circuit', 'submicron technology', 'algebraic model', 'linear model', 'level circuit', 'l decision', 'linear decision diagram']

A new approach to the problem of structural identification. II
The subject under discussion is a new approach to the problem of structural
	identification, which relies on the recognition of a decisive role of
	the human factor in the process of structural identification. Potential
	possibilities of the suggested approach are illustrated by the
	statement of a new mathematical problem of structural identification
['structural identification', 'human factor', 'mathematical equations', 'decision-maker']
['new approach', 'structural identification', 'decisive role', 'human factor', 'approach', 'problem', 'structural', 'identification', 'ii', 'discussion']

A method of determining a sequence of the best solutions to the problems of
	optimization on finite sets and the problem of network reconstruction
A method of determining a sequence of the best solutions to the problems of
	optimization on finite sets was proposed. Its complexity was estimated
	by a polynomial of the dimension of problem input, given number of
	sequence terms, and complexity of completing the design of the original
	extremal problem. The technique developed was applied to the typical
	problem of network reconstruction with the aim of increasing its
	throughput under restricted reconstruction costs
['best solutions', 'optimization', 'finite sets', 'network reconstruction', 'complexity']
['good solution', 'network reconstruction', 'finite set', '-PRON- complexity', 'problem input', 'problem', 'sequence term', 'method', 'extremal problem', 'sequence']

Stabilization of a linear object by frequency-modulated pulsed signals
A control system consisting of an unstable continuous linear part and a
	pulse-frequency modulator in the feedback circuit is studied.
	Conditions for the boundedness of the solutions of the system under any
	initial data are determined
['discrete systems', 'stabilization', 'frequency-modulated pulsed signals', 'linearstationary object', 'control system', 'feedback circuit', 'solutionboundedness']
['unstable continuous linear', 'control system', 'feedback circuit', 'pulsed signal', 'frequency modulator', 'linear object', 'system', 'frequency', 'initial datum', 'solution']

Reachability sets of a class of multistep control processes: their design
An upper estimate and an iterative "restriction" algorithm for the reachability
	set for determining the optimal control for a class of multistep
	control processes are designed
['reachability sets', 'multistep control processes', 'discrete systems', 'upperestimate', 'iterative restriction algorithm', 'optimal control']
['multistep control process', 'control process', 'reachability set', 'upper estimate', 'reachability', 'class', 'set', 'optimal control', 'design', 'algorithm']

Generalized confidence sets for a statistically indeterminate random vector
A problem is considered for the construction of confidence sets for a random
	vector, the information on distribution parameters of which is
	incomplete. To obtain exact estimates and a detailed analysis of the
	problem, the notion is introduced of a generalized confidence set for a
	statistically indeterminate random vector. Properties of generalized
	confidence sets are studied. It is shown that the standard method of
	estimation, which relies on the unification of confidence sets, leads
	in many cases to wider confidence estimates. For a normally distributed
	random vector with an inaccurately known mean value, generalized
	confidence sets are built tip and the dependence of sizes of a
	generalized confidence set on the forms and parameters of a set of
	possible mean values is examined
['generalized confidence sets', 'statistically indeterminate random vector', 'distribution parameters', 'normally distributed random vector']
['confidence set', 'random vector', 'wide confidence estimate', 'possible mean value', 'indeterminate random vector', 'distribution parameter', 'detailed analysis', 'exact estimate', 'confidence', 'standard method']

Evolution of the high-end computing market in the USA
This paper focuses on the technological change in the high-end computing
	market. The discussion combines historical analysis with strategic
	analysis to provide a framework to analyse a key component of the
	computer industry. This analysis begins from the perspective of
	government research and development spending; then examines the
	confusion around the evolution of the high-end computing market in the
	context of standard theories of technology strategy and new product
	innovation. Rather than the high-end market being 'dead', one should
	view the market as changing due to increased capability and competition
	from the low-end personal computer market. The high-end market is also
	responding to new product innovation from the introduction of new
	parallel computing architectures. In the conclusion, key leverage
	points in the market are identified and the trends in high-end
	computing are highlighted with implications
['high-end computing market evolution', 'usa', 'historical analysis', 'strategicanalysis', 'computer industry', 'government research', 'development spending', 'technology strategy', 'new product innovation', 'competition', 'low-endpersonal computer market', 'parallel computing architectures', 'supercomputing']
['end personal computer market', 'end market', 'high', 'new product', 'increased capability', 'evolution', 'market', 'parallel computing architecture', 'new product innovation', 'development spending']

Strong active solution in non-cooperative games
For the non-cooperative games and the problems of accepting or rejecting a
	proposal, a new notion of equilibrium was proposed, its place among the
	known basic equilibria was established, and its application to the
	static and dynamic game problems was demonstrated
['strong active solution', 'noncooperative games', 'static game problems', 'dynamicgame problems']
['strong active solution', 'dynamic game problem', 'cooperative game', 'new notion', 'basic equilibria', 'non', 'place', 'equilibrium', 'static', 'application']

System embedding. Control with reduced observer
Two interrelated problems-design of the reduced observer of plant state
	separately and together with its control system-were considered from
	the standpoint of designing the multivariable linear systems from the
	desired matrix transfer functions. The matrix equations defining the
	entire constructive class of solutions of the posed problems were
	obtained using the system embedding technology. As was demonstrated,
	control based on the reduced observer is capable to provide the desired
	response to the control input, as well as the response to the nonzero
	initial conditions, only for the directly measurable part of the
	components of the state vector. An illustrative example was presented
['system embedding', 'reduced observer control', 'reduced plant state observerdesign', 'multivariable linear systems', 'matrix transfer functions', 'statevector']
['reduced observer', 'entire constructive class', 'matrix transfer function', 'multivariable linear system', 'plant state', 'control system', 'system embedding', 'matrix equation', 'control', 'problem']

Spectral characteristics of the linear systems over a bounded time interval
Consideration was given to the spectral characteristics of the linear dynamic
	systems over a bounded time interval. Singular characteristics of
	standard dynamic blocks, transcendental characteristic equations, and
	partial spectra of the singular functions were studied. Relationship
	between the spectra under study and the classical frequency
	characteristic was demonstrated
['spectral characteristics', 'bounded time interval', 'linear dynamic systems', 'singular characteristics', 'standard dynamic blocks', 'transcendentalcharacteristic equations', 'partial spectra', 'singular functions', 'frequency characteristic']
['bounded time interval', 'transcendental characteristic equation', 'spectral characteristic', 'standard dynamic block', 'linear system', 'linear dynamic', 'singular characteristic', 'singular function', 'partial spectra', 'system']

Quantum computing with solids
Science and technology could be revolutionized by quantum computers, but
	building them from solid-state devices will not be easy. The author
	outlines the challenges in scaling up the technology from lab
	experiments to practical devices
['quantum computers', 'solid-state devices']
['practical device', 'state device', 'quantum', 'technology', 'solid', 'quantum computer', 'science', 'author', 'easy', 'challenge']

The perils of privacy
The recent string of failures among dotcom companies has heightened fears of
	privacy abuse. What should happen to the names and addresses on a
	customer list if these details were obtained under a privacy policy
	which specified no disclosure to any third party? Should the personal
	data in the list be deemed to be an asset of a failing company which
	can be transferred to any future (third party) purchaser for its
	purposes? Or should the privacy policy take precedence over the
	commercial concerns of the purchaser?
['privacy abuse', 'customer list', 'privacy policy', 'disclosure']
['privacy policy', 'customer list', 'privacy abuse', 'dotcom company', 'recent string', 'privacy', 'commercial concern', 'company', 'party', 'list']

Enterprise in focus at NetSec 2002
NetSec 2002 took place in San Francisco, amid industry reflection on the
	balance to be struck between combatting cyber-terrorism and
	safeguarding civil liberties post-9.11. The author reports on the
	punditry and the pedagogy at the CSI event, focusing on security in the
	enterprise
['netsec 2002', 'csi', 'enterprise security']
['civil liberty post-9.11', 'enterprise', 'industry reflection', 'netsec', 'san francisco', 'focus', 'place', 'csi event', 'balance', 'terrorism']

Trusted...or...trustworthy: the search for a new paradigm for computer and
	network security
This paper sets out a number of major questions and challenges which include:
	(a) just what is meant by `trusted' or `trustworthy' systems after 20
	years of experience, or more likely, lack of business level experience,
	with the 'trusted computer system' criteria anyway; (b) does anyone
	really care about the adoption of international standards for computer
	system security evaluation by IT product and system manufacturers and
	suppliers (IS 15408) and, if so, how does it all relate to business
	risk management anyway (IS 17799); (c) with the explosion of adoption
	of the microcomputer and personal computer some 20 years ago, has the
	industry abandoned all that it learnt about security during the
	`mainframe era'; or - `whatever happened to MULTICS' and its lessons;
	(d) has education kept up with security requirements by industry and
	government alike in the need for safe and secure operation of large
	scale and networked information systems on national and international
	bases, particularly where Web or Internet-based information services
	are being proposed as the major `next best thing' in the IT industry;
	(e) has the `fourth generation' of computer professionals inherited the
	spirit of information systems management and control that resided by
	necessity with the last `generation', the professionals who developed
	and created the applications for shared mainframe and minicomputer
	systems?
['computer security', 'network security', 'trusted systems', 'trustworthy systems', 'international standards', 'is 15408', 'business risk management', 'is 17799', 'it manufacturers', 'microcomputer', 'personal computer', 'multics', 'education', 'large scale information systems', 'web', 'internet-based informationservices', 'fourth generation computer professionals', 'information systemsmanagement', 'information systems control']
['system security evaluation', 'information system management', 'business level experience', 'information system', 'major question', 'network security', 'system manufacturer', 'risk management', 'it product', 'new paradigm']

Much ado about nothing: Win32.Perrun
JPEG files do not contain any executable code and it is impossible to infect
	such files. The author takes a look at the details surrounding the
	Win32.Perrun virus and make clear exactly what it does. The main virus
	feature is its ability to affect JPEG image files (compressed graphic
	images) and to spread via affected JPEG files. The virus affects, or
	modifies, or alters JPEG files but does not "infect" them
['win32.perrun', 'jpeg files', 'virus', 'compressed graphic images']
['jpeg file', 'executable code', 'win32.perrun virus', 'alter jpeg file', 'jpeg image file', 'main virus', 'win32.perrun', 'virus', 'clear', 'detail']

Information security policy - what do international information security
	standards say?
One of the most important information security controls, is the information
	security policy. This vital direction-giving document is, however, not
	always easy to develop and the authors thereof battle with questions
	such as what constitutes a policy. This results in the policy authors
	turning to existing sources for guidance. One of these sources is the
	various international information security standards. These standards
	are a good starting point for determining what the information security
	policy should consist of, but should not be relied upon exclusively for
	guidance. Firstly, they are not comprehensive in their coverage and
	furthermore, tending to rather address the processes needed for
	successfully implementing the information security policy. It is far
	more important the information security policy must fit in with the
	organisation's culture and must therefore be developed with this in
	mind
['information security policy', 'international information security standards']
['information security policy', 'international information security', 'international information security standard', 'important information security control', 'good starting point', 'security policy', 'vital direction', 'existing source', 'policy author', 'guidance']

Security crisis management - the basics
Of the more pervasive problems in any kind of security event is how the
	security event is managed from the inception to the end. There's a lot
	written about how to manage a specific incident or how to deal with a
	point problem such as a firewall log, but little tends to be written
	about how to deal with the management of a security event as part of
	corporate crisis management. This article discusses the basics of
	security crisis management and of the logical steps required to ensure
	that a security crisis does not get out of hand
['security crisis management', 'security event', 'firewall log', 'corporate crisismanagement']
['security crisis management', 'security event', 'security crisis', 'corporate crisis management', 'pervasive problem', 'specific incident', 'point problem', 'firewall log', 'little tend', 'basic']

A conceptual framework for evaluation of information technology investments
The decision to acquire a new information technology poses a number of serious
	evaluation and selection problems to technology managers, because the
	new system must not only meet current information requirements of the
	organisation, but also the needs for future expansion. Tangible and
	intangible benefits factors, as well as risks factors, must be
	identified and evaluated. The paper provides a review of ten major
	evaluation categories and available models, which fall under each
	category, showing their advantages and disadvantages in handling the
	above difficulties. This paper describes strategic implications
	involved in the selection decision, and the inherent difficulties in:
	(1) choosing or developing a model, (2) obtaining realistic inputs for
	the model, and (3) making tradeoffs among the conflicting factors. It
	proposes a conceptual framework to help the decision maker in choosing
	the most appropriate methodology in the evaluation process. It also
	offers a new model, called GAHP, for the evaluation problem combining
	integer goal linear programming and analytic hierarchy process (AHP) in
	a single hybrid multiple objective multi-criteria model. A goal
	programming methodology, with zero-one integer variables and mixed
	integer constraints, is used to set goal target values against which
	information technology alternatives are evaluated and selected. AHP is
	used to structure the evaluation process providing pairwise comparison
	mechanisms to quantify subjective, nonmonetary, intangible benefits and
	risks factors, in deriving data for the model. A case illustration is
	provided showing how GAHP can be formulated and solved
['information technology investments', 'technology managers', 'informationrequirements', 'risks factors', 'evaluation categories', 'selection decision', 'tradeoffs', 'decision maker', 'analytic hierarchy process', 'hybrid multipleobjective multi-criteria model', 'goal programming methodology', 'zero-oneinteger variables', 'mixed integer constraints', 'goal target values', 'information technology alternatives', 'pairwise comparison mechanisms', 'nonmonetary benefits', 'intangible benefits', 'group decision process']
['information technology', 'conceptual framework', 'risk factor', 'intangible benefit', 'technology investment', 'new information', 'benefit factor', 'information requirement', 'technology manager', 'selection problem']

Data storage: re-format. Closely tracking a fast-moving sector
In the past few years the data center market has changed dramatically, forcing
	many companies into consolidation or bankruptcy. Gone are the days when
	companies raised millions of dollars to acquire large industrial
	buildings and transform them into glittering, high-tech palaces filled
	with the latest telecommunication and data technology. Whereas
	manufacturers of communication technology deliver the racked equipment
	in these, often mission-critical, facilities, ABB focuses mainly on the
	building infrastructure. Besides the very important redundant power
	supply, ABB also provides the redundant air conditioning and the
	security system
['building management', 'data centers', 'building infrastructure', 'mission-criticalfacilities', 'abb', 'engineering management', 'project management', 'installation', 'commissioning', 'redundant power supply', 'redundant airconditioning', 'security system']
['datum center market', 'redundant air conditioning', 'important redundant power', 'past few year', 'datum storage', 'racked equipment', 'datum technology', 'communication technology', 'large industrial', 'late telecommunication']

Industrial/sup IT/ for performance buildings
ABB has taken a close look at how buildings are used and has come up with a
	radical solution for the technical infrastructure that places the
	end-user's processes at the center and integrates all the building's
	systems around their needs. The new solution is based on the
	realization that tasks like setting up an office meeting, registering a
	hotel guest or moving a patient in a hospital, can all benefit from the
	same Industrial IT concepts employed by ABB to optimize manufacturing,
	for example in the automotive industry
['industrial/sup it/', 'abb', 'building management system', 'technical infrastructure', 'building systems integration', 'industrial it concepts']
['industrial it concept', 'hotel guest', 'technical infrastructure', 'office meeting', 'new solution', 'radical solution', 'close look', 'performance building', 'sup it/', 'abb']

Virtual engineering office: a state-of-the-art platform for engineering
	collaboration
A sales force in Latin America, the design department in Europe, and production
	in Asia? Arrangements of this kind are the new business reality for
	today's global manufacturing companies. But how are such global
	operations to be effectively coordinated? ABB's answer was to develop
	and implement a new platform for high-performance, real-time
	collaboration. Globally distributed engineering teams can now work
	together, regardless of time, location or the CAD system they use,
	making ABB easier to do business with, for customers as well as
	suppliers
['virtual engineering office', 'state-of-the-art', 'engineering collaborationplatform', 'business', 'global manufacturing companies', 'abb', 'cad system', 'globally distributed engineering teams']
['global manufacturing company', 'virtual engineering office', 'new business reality', 'design department', 'collaboration', 'latin america', 'sale force', 'art platform', 'new platform', 'engineering team']

Post-haste. 100th robotic containerization system installed in US mail sorting
	center
Spot welding, machine tending, material handling, picking, packing, painting,
	palletizing, assembly...the list of tasks being performed by ABB robots
	keeps on growing. Adding to this portfolio is a new robot
	containerization system (RCS) that ABB developed specifically for the
	United States Postal Service (USPS). The RCS has brought new levels of
	speed, accuracy, efficiency and productivity to the process of sorting
	and containerizing mail and packages. Recently, the 100th ABB RCS was
	installed at the USPS processing and distribution center in Columbus,
	Ohio
['mail sorting center', 'robotic containerization system', 'usa', 'abb robots', 'unitedstates postal service', 'mail sorting', 'packages sorting']
['united states postal service', '100th robotic containerization system', 'material handling', 'machine tending', 'abb robot', 'spot welding', 'new robot', 'us mail', 'mail', 'center']

Optimize/sup IT/ robot condition monitoring tool
As robots have gained more and more 'humanlike' capability, users have looked
	increasingly to their builders for ways to measure the critical
	variables-the robotic equivalent of a physical check-up-in order to
	monitor their condition and schedule maintenance more effectively. This
	is all the more essential considering the tremendous pressure there is
	to improve productivity in today's global markets. Developed for ABB
	robots with an S4-family controller and based on the company's broad
	process know-how, Optimize/sup IT/ robot condition monitoring offers
	maintenance routines with embedded checklists that give a clear
	indication of a robot's operating condition. It performs semi-automatic
	measurements that support engineers during trouble-shooting and enable
	action to be taken to prevent unplanned stops. By comparing these
	measurements with reference data, negative trends can be detected early
	and potential breakdowns predicted. Armed with all these features,
	Optimize/sup IT/ robot condition monitoring provides the ideal basis
	for reliability-centered maintenance (RCM) for robots
['optimize/sup it/ robot condition monitoring tool', 'maintenance scheduling', 'condition monitoring', 'abb robots', 's4-family controller', 'semi-automaticmeasurements', 'reliability-centered maintenance']
['sup it/ robot condition monitoring', 'sup it/ robot condition', 'optimize', 'global market', 'physical check', 'schedule maintenance', 'tremendous pressure', 'process know', 'robotic equivalent', 'maintenance routine']

Pane relief. Robotic solutions for car windshield assembly
Just looking through a car's windshield doesn't give us much reason to wonder
	about how it's made. The idea that special manufacturing expertise
	might be required can hardly occur to anyone, but that's exactly what
	is needed to ensure crystal-clear visibility, not to mention a perfect
	fit every time one is pressed into place on a car production line.
	Comprising two thin glass sheets joined by a vinyl interlayer,
	windshields are assembled-usually manually-to very precise product and
	environmental specifications. To make sure this is done as perfectly as
	possible, the industry invests heavily in the equipment used for their
	fabrication. ABB has now developed a robot-based Compact Assembling
	System for the automatic assembly of laminated windshields that speeds
	up production and increases cost efficiency
['car windshield assembly robots', 'manufacturing expertise', 'car production line', 'compact assembling system', 'laminated windshields assembly automation', 'production', 'cost efficiency', 'abb']
['thin glass sheet', 'special manufacturing expertise', 'car production line', 'car windshield assembly', 'pane relief', 'robotic solution', 'clear visibility', 'environmental specification', 'vinyl interlayer', 'precise product']

Shaping the future. BendWizard: a tool for off-line programming of robotic
	tending systems
Setting up a robot to make metal cabinets or cases for desktop computers can be
	a complex operation. For instance, one expert might be required to
	carry out a feasibility study, and then another to actually program the
	robot. Understandably, the need for so much expertise, and the time
	that's required, generally limits the usefulness of automation to
	high-volume production. Workshops producing parts in batches smaller
	than 50 or so, or which rely heavily on semiskilled operators, are
	therefore often discouraged from investing in automation, and so miss
	out on its many advantages. What is needed is a software tool that
	operators without special knowledge of robotics, or with no more than
	rudimentary CAD skills, can use. One which allows easy offline
	programming and simulation of the work cell on a PC
['robotic tending systems', 'bendwizard offline programming tool', 'metal cabinets', 'desktop computer cases', 'feasibility study', 'high-volume production', 'workshops', 'cad skills', 'work cell simulation']
['rudimentary cad skill', 'semiskilled operator', 'special knowledge', 'feasibility study', 'batch small', 'complex operation', 'software tool', 'volume production', 'desktop computer', 'easy offline']

Press shop. Industrial IT solutions for the press shop
Globalization of the world's markets is challenging the traditional limits of
	manufacturing efficiency. The competitive advantage belongs to those
	who understand the new requirements and opportunities, and who commit
	to integrated solutions that span the value chain all the way from
	demand to production. ABB's automation and IT expertise and the process
	know-how gained from its long involvement with the automotive industry,
	have been brought together in new, state-of-the-art software solutions
	for press shops. Integrated into Industrial IT architecture, they allow
	the full potential of the shops to be realized, with advantages at
	every step in the supply chain
['press shops', 'industrial it solutions', 'market globalisation', 'manufacturingefficiency', 'automation', 'state-of-the-art', 'software solutions', 'carmanufacturing business', 'supply chain']
['press shop', 'industrial it', 'traditional limit', 'competitive advantage', 'manufacturing efficiency', 'new requirement', 'integrated solution', 'value chain', 'industrial it architecture', 'it expertise']

Real-time enterprise solutions for discrete manufacturing and consumer goods
Customer satisfaction and a focus on core competencies have dominated the
	thinking of a whole host of industries in recent years. However, one
	outcome, the outsourcing of noncore activities, has made the production
	of goods-from order entry to final delivery-more and more complex.
	Suppliers, subsuppliers, producers and customers are therefore busy
	adopting a new, more collaborative approach. This is mainly taking the
	form of order-driven planning and scheduling of production, but it is
	also being steered by a need to reduce inventories and working capital
	as well as a desire to increase throughput and optimize production
['real-time enterprise solutions', 'discrete manufacturing', 'consumer goods', 'customer satisfaction', 'core competencies', 'order-driven planning', 'production scheduling', 'working capital reduction', 'inventories reduction']
['time enterprise solution', 'core competency', 'final delivery', 'customer satisfaction', 'recent year', 'noncore activity', 'consumer good', 'order entry', 'discrete manufacturing', 'collaborative approach']

Extinction cross sections of realistic raindrops: data-bank established using
	T-matrix method and nonlinear fitting technique
A new computer program is developed based on the T-matrix method to generate a
	large number of total (extinction) cross sections (TCS) values of the
	realistic raindrops that are deformed due to a balance of the forces
	that act on a drop failing under gravity, and were described in shape
	by Pruppacher and Pitter (1971). These data for various dimensions of
	the raindrops (mean effective radius from 0 to 3.25 mm), frequencies
	(10 to 80 GHz), (horizontal and vertical) polarizations, and
	temperatures (0, 10 and 20 degrees C) are stored to establish a data
	bank. Furthermore, a curve fitting technique, i.e., interpolation of
	order 3, is implemented for the TCS values in the data bank. Therefore,
	the interpolated TCS results can be obtained readily from the
	interpolation process with negligible or even null computational time
	and efforts. Error analysis is carried out to show the high accuracy of
	the present analysis and applicability of the interpolation. At three
	operating frequencies of 15, 21.225, and 38 GHz locally used in
	Singapore, some new TCS values are obtained from the new fast and
	efficient interpolation with a good accuracy
['extinction cross sections', 'realistic raindrops', 'data-bank', 't-matrix method', 'total cross sections', 'temperature', 'error analysis', 'mean effectiveradius', 'gravity', 'horizontal polarization', 'vertical polarization', 'interpolation', 'nonlinear curve fitting technique', 'operatingfrequencies', 'singapore', 'shf', 'ehf', 'electromagnetic wave scattering', 'emwave scattering', 'computer program', '15 ghz', '21.225 ghz', '38 ghz', '10 to 80ghz', '0 to 3.25 mm', '0 c', '10 c', '20 c']
['new computer program', 'curve fitting technique', 'new tcs value', 'extinction cross section', 'realistic raindrop', 'matrix method', 'cross section', 'fitting technique', 'bank', 'large number']

Challenges and trends in discrete manufacturing
Over 50 years ago, the 100,000 workers at Ford's Rouge automobile factory
	turned out 1200 cars per day. Nowadays, Ford's plant on that same site
	still produces 800 cars each day but with just 3000 workers. Similar
	stories abound in the manufacturing industries; technology revolution
	and evolution; a shift from vertical integration, better business and
	production practices and improved industrial relations-all have changed
	manufacturing beyond recognition. So what are the current challenges
	and trends in manufacturing? Certainly, the relentless advance of
	technology will continue, as will user pressure for more customized
	design or improved environmental friendliness. Some trends are already
	with us and more, as yet indiscernible, will come. But one major,
	fundamental shift now resounding throughout industry is the way in
	which information involving every single aspect of the manufacturing
	process is being integrated into one seamless system
['discrete manufacturing', 'challenges', 'automobile factory', 'technology revolution', 'technology evolution', 'business practices', 'production practices', 'industrial relations', 'trends', 'seamless manufacturing process']
['rouge automobile factory', 'manufacturing', 'trend', 'discrete manufacturing', 'industrial relation', 'relentless advance', 'current challenge', 'production practice', 'ford', 'worker']

On the Beth properties of some intuitionistic modal logics
Let L be one of the intuitionistic modal logics. As in the classical modal
	case, we define two different forms of the Beth property for L, which
	are denoted by B1 and B2; in this paper we study the relation among B1,
	B2 and the interpolation properties C1 and C2. It turns out that C1
	implies B1, but contrary to the boolean case, is not equivalent to B1.
	It is shown that B2 and C2 are independent, and moreover it comes out
	that, in contrast to classical case, there exists an extension of the
	intuitionistic modal logic of S/sub 4/-type, that has not the property
	B2. Finally we give two algebraic properties, that characterize
	respectively B1 and B2
['beth properties', 'interpolation properties', 'intuitionistic modal logics']
['intuitionistic modal logic', 'interpolation property c1', 'beth property', 'b1', 'b2', 'classical modal', 'different form', 'case', 'boolean case', 'c2']

More constructions for Boolean algebras
We construct Boolean algebras with prescribed behaviour concerning depth for
	the free product of two Boolean algebras over a third, in ZFC using
	pcf; assuming squares we get results on ultraproducts. We also deal
	with the family of cardinalities and topological density of homomorphic
	images of Boolean algebras (you can translate it to topology-on the
	cardinalities of closed subspaces); and lastly we deal with
	inequalities between cardinal invariants, mainly d(B)/sup kappa
	/<|B| implies ind(B)>/sup kappa /V Depth(B)>or=log(|B|)
['boolean algebras', 'prescribed behaviour', 'free product', 'zfc', 'ultraproducts', 'homomorphic images', 'cardinal invariants']
['boolean algebra', 'prescribed behaviour', 'free product', '/v depth(b)>or', 'more construction', 'topological density', 'cardinal invariant', 'closed subspace', 'cardinality', 'image']

IT as a key enabler to law firm competitiveness
Professional services firms have traditionally been able to thrive in virtually
	any market conditions. They have been consistently successful for
	several decades without ever needing to reexamine or change their basic
	operating model. However, gradual but inexorable change in client
	expectations and the business environment over recent years now means
	that more of the same is no longer enough. In future, law firms will
	increasingly need to exploit IT more effectively in order to remain
	competitive. To do this, they will need to ensure that all their
	information systems function as an integrated whole and are available
	to their staff, clients and business partners. The authors set out the
	lessons to be learned for law firms in the light of the recent PA
	Consulting survey
['professional services firms', 'client expectations', 'business environment', 'information systems', 'law firms']
['professional service firm', 'law firm', 'law firm competitiveness', 'market condition', 'recent year', 'inexorable change', 'operating model', 'business environment', 'key enabler', 'client']

Electronic signatures - much ado?
Whilst the market may be having a crisis of confidence regarding the prospects
	for e-commerce, the EU and the Government continue apace to develop the
	legal framework. Most recently, this has resulted in the Electronic
	Signatures Regulations 2002. These Regulations were made on 13 February
	2002 and came into force on 8 March 2002. The Regulations implement the
	European Electronic Signatures Directive (1999/93/EC). Critics may say
	that the Regulations were implemented too late (they were due to have
	been implemented by 19 July 2001), with too short a consultation period
	(25 January 2002 to 12 February 2002) and with an unconvincing case as
	to what they add to English law (as to which, read on). The author
	explains the latest development on e-signatures and the significance of
	Certification Service Providers (CSPs)
['e-commerce', 'legal framework', 'electronic signatures regulations 2002', 'europeanelectronic signatures directive']
['european electronic signatures directive', 'signatures regulations', 'regulations', 'legal framework', 'february', 'consultation period', 'english law', 'unconvincing case', 'late development', 'march']

Naomi Campbell: drugs, distress and the Data Protection Act
In the first case of its kind, Naomi Campbell successfully sued Mirror Group
	Newspapers for damage and distress caused by breach of the Data
	Protection Act 1998. Partner N. Wildish and assistant M. Turle of City
	law firm Field Fisher Waterhouse discuss the case and the legal
	implications of which online publishers should be aware
['drugs', 'distress', 'data protection act', 'naomi campbell', 'online publishers']
['naomi campbell', 'law firm field fisher waterhouse', 'data protection act', 'assistant m. turle', 'partner n. wildish', 'protection act', 'mirror group', 'distress', 'case', 'kind']

Don't always believe what you Reed [optimisation techniques for Web sites and
	trade mark infringement]
On 20 May 2002, Mr Justice Pumfrey gave judgment in the case of (1) Reed
	Executive Plc (2) Reed Solutions Plc versus (1) Reed Business
	Information Limited (2) Reed Elsevier (UK) Limited (3) totaljobs.com
	Limited. The case explored for the first time in any detail the extent
	to which the use of various optimisation techniques for Web sites could
	give rise to new forms of trade mark infringement and passing off. The
	author reports on the case and offers his comments
['reed executive plc', 'reed solutions plc', 'reed business information limited', 'reedelsevier (uk) limited', 'totaljobs.com limited', 'optimisation techniques', 'web sites', 'trade mark infringement', 'passing off']
['trade mark infringement', 'mr justice pumfrey', 'reed solutions plc', 'web site', 'optimisation technique', 'executive plc', 'case', 'information limited', 'reed elsevier', 'reed business']

Finally! some sensible European legislation on software
The European Commission has formally tabled a draft Directive on the Protection
	by Patents of Computer-Implemented Inventions. The aim of this very
	important Directive is to harmonise national patent laws relating to
	inventions using software. It follows an extensive consultation
	launched by the Commission in October 2000. The impetus behind the
	Directive was the recognition at EU level of a total lack of unity
	between the European Patent Office and European national courts in
	deciding what was or was not deemed patentable when it came to the
	subject of computer programs
['european commission', 'directive on the protection by patents ofcomputer-implemented inventions', 'national patent laws', 'lawharmonisation', 'eu', 'european patent office', 'national courts', 'computerprograms']
['european national court', 'national patent law', 'sensible european legislation', 'european patent office', 'draft directive', 'european commission', 'software', 'important directive', 'extensive consultation', 'computer']

Cyberobscenity and the ambit of English criminal law
The author looks at a recent case and questions the Court of Appeal's approach.
	In the author's submission, the Court of Appeal's decision in Perrin
	was wrong. P published no material in England and Wales, and should not
	have been convicted of any offence under English law, even if it were
	proved that he sought to attract English subscribers to his site. That
	may be an unpalatable conclusion but, if the content of foreign-hosted
	Internet sites is to be controlled, the only sensible way forward is
	through international agreement and cooperation. The Council of
	Europe's Cybercrime Convention provides some indication of the limited
	areas over which widespread international agreement might be achieved
['cyberobscenity', 'criminal law', 'court of appeal', 'internet sites', 'cybercrimeconvention', 'council of europe', 'international agreement', 'england']
['english criminal law', 'widespread international agreement', 'recent case', 'english law', 'international agreement', 'english subscriber', 'unpalatable conclusion', 'internet site', 'sensible way', 'author']

E-government
The author provides an introduction to the main issues surrounding E-government
	modernisation and electronic delivery of all public services by 2005.
	The author makes it clear that E-government is about transformation,
	not computers and hints at the special legal issues which may arise
['e-government', 'modernisation', 'electronic delivery', 'public services', 'legal issues']
['special legal issue', 'public service', 'government', 'electronic delivery', 'main issue', 'author', 'introduction', 'modernisation', 'hint', 'clear']

Vendor qualifications for IT staff and networking
In some cases, vendor-run accreditation schemes can offer an objective measure
	of a job applicant's skills, but they do not always indicate the true
	extent of practical abilities
['vendor-run accreditation schemes', 'job applicant', 'it staff', 'networkadministrators', 'practical abilities']
['run accreditation scheme', 'it staff', 'vendor qualification', 'job applicant', 'objective measure', 'vendor', 'case', 'networking', 'practical ability', 'skill']

Evolution of litigation support systems
For original paper see ibid., vol. 12, no. 6: "The E-mail of the Species". The
	author responds to that paper and argues that printing, scanning and
	imaging E-mails or other electronic (rather than paper) documents prior
	to listing and disclosure seems to be unnecessary, not 'proportionate'
	(from a costs point of view) and not particularly helpful, to either
	side. He asks how litigation support systems might evolve to help and
	support the legal team in their task
['litigation support systems', 'e-mail', 'legal team']
['litigation support system', 'original paper', 'paper', 'mail', 'cost point', 'proportionate', 'unnecessary', 'disclosure', 'species', 'author']

Evicting orang utans from the office [electronic storage of legal files]
Having espoused the principle of the paperless office some time ago, we decided
	to apply it to our stored files. First we consulted the Law Society
	rules governing storage of files on electronic media. The next step was
	for us to draw up a protocol for scanning the files. The benefits of
	the exercise have been significant. The area previously used for
	storage has been freed for other use. Files are now available online,
	instantaneously. When we have needed to send out files to the client or
	following a change of solicitor, we have been able to do so almost
	immediately, by E-mail, retaining a copy for our future reference. The
	files are protected from loss or deterioration, back-up copies having
	been taken which are stored off site. The complete stored file archive
	can be put in your pocket (in CD-ROM format) or on a laptop,
	facilitating remote working
['paperless office', 'legal files', 'electronic storage', 'law society rules', 'filescanning', 'cd-rom', 'file archive']
['electronic medium', 'remote working', 'rom format', 'future reference', 'file archive', 'file', 'law society', 'stored file', 'paperless office', 'legal file']

Electronic data exchange for real estate
With HM Land Registry's consultation now underway, no one denies that the
	property industry is facing a period of unprecedented change. PISCES
	(Property Information Systems Common Exchange) is a property-focused
	electronic data exchange standard. The standard is a set of definitions
	and rules to facilitate electronic transfer of data between key
	business areas and between different types of software packages that
	are used regularly by the property industry. It is not itself a piece
	of software but an enabling technology that allows software providers
	to prepare solutions within their own packages to transfer data between
	databases. This provides the attractive prospect of seamless transfer
	of data within and between systems and organisations
['hm land registry', 'property industry', 'pisces', 'property information systemscommon exchange', 'electronic data exchange', 'standard', 'software packages', 'databases', 'seamless transfer']
['electronic datum exchange', 'property information systems common exchange', 'hm land registry', 'electronic datum exchange standard', 'property industry', 'unprecedented change', 'real estate', 'software package', 'electronic transfer', 'different type']

E-mail and the legal profession
The widespread use of E-mail can be found in all areas of commerce, and the
	legal profession is one that has embraced this new medium of
	communication. E-mail is not without its drawbacks, however. Due to the
	nature of the technologies behind the medium, it is a less secure form
	of communication than many of those traditionally used by the legal
	profession, including DX, facsimile, and standard and registered post.
	There are a number of ways in which E-mails originating from the
	practice may be protected, including software encryption, hardware
	encryption and various methods of controlling and administering access
	to the E-mails
['e-mail', 'legal profession', 'secure communication', 'software encryption', 'hardwareencryption', 'access control']
['legal profession', 'mail', 'widespread use', 'software encryption', 'registered post', 'secure form', 'new medium', 'legal', 'profession', 'communication']

Spam solution?
The author describes a solution to spam E-mails: disposable E-mail addresses
	(DEA). Mailshell's free trial Web-based E-mail service allows you, if
	you start getting spammed on that DEA, just to delete the DEA in
	Mailshell, and all E-mail thereafter sent to that address will
	automatically be junked (though you can later restore that address if
	you want). Mailshell allows any number of DEA
['spam e-mails', 'disposable e-mail addresses', 'mailshell', 'web-based e-mail']
['free trial web', 'mail address', 'mail service', 'dea', 'spam solution', 'disposable e', 'mailshell', 'address', 'solution', 'author']

7 key tests in choosing your Web site firm
Most legal firms now have a Web site and are starting to evaluate the return on
	their investment. The paper looks at factors involved when choosing a
	firm to help set up or improve a Web site. (1) Look for a company that
	combines technical skills and business experience. (2) Look for a
	company that offers excellent customer service. (3) Check that the Web
	site firm is committed to developing and proactively updating the Web
	site. (4) Make sure the firm has a proven track record and a good
	portfolio. (5) Look for a company with both a breadth as well as depth
	of skills. (6) Make sure the firm can deliver work on target, in budget
	and to specification. (7) Ensure that you will enjoy working and feel
	comfortable with the Web site firm staff
['web site', 'customer service', 'proactive updating', 'legal firms', 'return oninvestment', 'technical skills', 'business experience']
['web site firm', 'most legal firm', 'excellent customer service', 'web site', 'key test', 'business experience', 'technical skill', 'company', 'track record', 'web site firm staff']

Why your Web strategy is, err, wrong
An awkward look at a few standard views from the author, who thinks that most
	people have got it, err, wrong. Like every other investment, when the
	time comes to sign the contract, the question that should be asked is
	not whether it is a good investment, but whether it is the best
	investment the firm can make with the money. the author argues that he
	would be surprised if any law firm Web site he has seen yet would jump
	that particular hurdle
['web strategy', 'law firm web site']
['law firm web site', 'standard view', 'good investment', 'awkward look', 'web strategy', 'author', 'err', 'wrong', 'investment', 'particular hurdle']

A humanist's legacy in medical informatics: visions and accomplishments of
	Professor Jean-Raoul Scherrer
The objective is to report on the work of Prof. Jean-Raoul Scherrer, and show
	how his humanist vision, medical skills and scientific background have
	enabled and shaped the development of medical informatics over the last
	30 years. Starting with the mainframe-based patient-centred hospital
	information system DIOGENE in the 70s, Prof. Scherrer developed,
	implemented and evolved innovative concepts of man-machine interfaces,
	distributed and federated environments, leading the way with
	information systems that obstinately focused on the support of care
	providers and patients. Through a rigorous design of terminologies and
	ontologies, the DIOGENE data would then serve as a basis for the
	development of clinical research, data mining, and lead to innovative
	natural language processing techniques. In parallel, Prof. Scherrer
	supported the development of medical image management, ranging from a
	distributed picture archiving and communication systems (PACS) to
	molecular imaging of protein electrophoreses. Recognizing the need for
	improving the quality and trustworthiness of medical information of the
	Web, Prof. Scherrer created the Health-On-the Net (HON) foundation.
	These achievements, made possible thanks to his visionary mind, deep
	humanism, creativity, generosity and determination, have made of Prof.
	Scherrer a true pioneer and leader of the human-centered,
	patient-oriented application of information technology for improving
	healthcare
['professor jean-raoul scherrer', 'medical informatics', 'mainframe based patientcentered hospital information system', 'medical image management', 'pacs', 'internet', 'diogene system', 'man-machine interfaces', 'distributed systems', 'federated systems', 'data mining', 'natural language processing']
['medical informatic', 'information system', 'natural language', 'raoul scherrer', 'datum mining', 'scherrer', 'system diogene', 'clinical research', 'federated environment', 'machine interface']

Medicine in the 21 st century: global problems, global solutions
The objectives are to discuss application areas of information, technology in
	medicine and health care on the occasion of the opening of the Private
	Universitat fur Medizinische Informatik and Technik Tirol/University
	for Health Informatics and Technology Tyrol (LIMIT) at Innsbruck,
	Tyrol, Austria. Important application areas of information technology
	in medicine and health are appropriate individual access to medical
	knowledge, new engineering developments such as new radiant imaging
	methods and the implantable pacemaker/defibrillator devices,
	mathematical modeling for understanding the workings of the human body,
	the computer-based patient record, as well as new knowledge in
	molecular biology, human genetics, and biotechnology. Challenges and
	responsibilities for medical informatics research include medical data
	privacy and intellectual property rights inherent in the content of the
	information systems
['health care', 'medicine', 'information technology', 'individual medical knowledgeaccess', 'engineering developments', 'radiant imaging methods', 'implantablepacemaker devices', 'implantable defibrillator devices', 'mathematicalmodeling', 'human body', 'computer-based patient record', 'molecular biology', 'human genetics', 'biotechnology', 'medical informatics research', 'medicaldata privacy', 'intellectual property rights', 'information systems']
['information system', 'right inherent', 'application area', 'property right', 'medical datum', 'intellectual property', 'human genetic', 'informatic research', 'molecular biology', 'mathematical modeling']

Guidelines, the Internet, and personal health: insights from the Canadian
	HEALNet experience
The objectives are to summarize the insights gained in collaborative research
	in a Canadian Network of Centres of Excellence, devoted to the
	promotion of evidence-based practice, and to relate this experience to
	Internet support of health promotion and consumer health informatics. A
	subjective review of insights is undertaken. Work directed the
	development of systems incorporating guidelines, care maps, etc., for
	use by professionals met with limited acceptance. Evidence-based tools
	for health care consumers are a desirable complement but require
	radically different content and delivery modes. In addition to
	evidence-based material offered by professionals, a wide array of
	Internet-based products and services provided by consumers for
	consumers emerged and proved a beneficial complement. The
	consumer-driven products and services provided via the Internet are a
	potentially important and beneficial complement of traditional health
	services. They affect the health consumer-provider roles and require
	changes in healthcare practices
['collaborative research', 'canadian network of centres of excellence', 'evidence-based practice', 'internet support', 'health promotion', 'consumerhealth informatics', 'personal health', 'health consumer-provider roles']
['health care consumer', 'consumer health informatic', 'internet', 'guideline', 'insight', 'beneficial complement', 'canadian network', 'subjective review', 'health promotion', 'delivery mode']

ISCSI poised to lower SAN costs
IT managers building storage area networks or expanding their capacity may be
	able to save money by using iSCSI and IP systems rather than Fibre
	Channel technologies
['san costs', 'storage area networks', 'iscsi', 'ip systems']
['storage area network', 'iscsi', 'it manager', 'san cost', 'ip system', 'capacity', 'able', 'channel technology', 'money', 'fibre']

Standard protocol for exchange of health-checkup data based on SGML: the
	Health-checkup Data Markup Language (HDML)
The objectives are to develop a health/medical data interchange model for
	efficient electronic exchange of data among health-checkup facilities.
	A Health-checkup Data Markup Language (HDML) was developed on the basis
	of the Standard Generalized Markup Language (SGML), and a feasibility
	study carried out, involving data exchange between two health checkup
	facilities. The structure of HDML is described. The transfer of
	numerical lab data, summary findings and health status assessment was
	successful. HDML is an improvement to laboratory data exchange. Further
	work has to address the exchange of qualitative and textual data
['health checkup data exchange', 'sgml', 'health-checkup data markup language', 'datainterchange model', 'numerical lab data', 'summary findings', 'health statusassessment']
['checkup data markup language', 'medical datum interchange model', 'standard generalized markup language', 'health', 'standard protocol', 'efficient electronic exchange', 'hdml', 'checkup datum', 'sgml', 'checkup facility']

Development of a health guidance support system for lifestyle improvement
The objective is to provide automated advice for lifestyle adjustment based on
	an assessment of the results of a questionnaire and medical examination
	or health checkup data. A system was developed that gathers data based
	on questions regarding weight gain, exercise, smoking, sleep, eating
	habits, salt intake, animal fat intake, snacks, alcohol, and oral
	hygiene, body mass index, resting blood pressure, fasting blood sugar,
	total cholesterol, triglycerides, uric acid and liver function tests.
	Based on the relationships between the lifestyle data and the health
	checkup data, a health assessment sheet was generated for persons being
	allocated to a multiple-risk factor syndrome group. Health assessment
	and useful advice for lifestyle improvement were automatically
	extracted with the system, toward the high risk group for life style
	related diseases. The system is operational. In comparison with
	conventional, limited advice methods, we developed a practical system
	that defined the necessity for lifestyle improvement more clearly, and
	made giving advice easier
['health guidance support system', 'lifestyle improvement', 'questionnaire', 'medicalexamination', 'health checkup data', 'weight gain', 'smoking', 'exercise', 'sleep', 'eating habits', 'salt intake', 'animal fat intake', 'snacks', 'alcohol', 'oral hygiene', 'body mass index', 'resting blood pressure', 'fasting bloodsugar', 'total cholesterol', 'triglycerides', 'uric acid', 'liver functiontests']
['lifestyle improvement', 'checkup datum', 'lifestyle adjustment', 'health assessment', 'weight gain', 'health checkup', 'medical examination', 'fat intake', 'animal fat', 'salt intake']

Organization design: The continuing influence of information technology
Drawing from an information processing perspective, this paper examines how
	information technology (IT) has been a catalyst in the development of
	new forms of organizational structures. The article draws a historical
	linkage between the relative stability of an organization's task
	environment starting after the Second World War to the present
	environmental instability that now characterizes many industries.
	Specifically, the authors suggest that advances in IT have enabled
	managers to adapt existing forms and create new models for
	organizational design that better fit requirements of an unstable
	environment. Time has seemingly borne out this hypothesis as the
	bureaucratic structure evolved to the matrix to the network and now to
	the emerging shadow structure. IT has gone from a support mechanism to
	a substitute for organizational structures in the form of the shadow
	structure. The article suggests that the evolving and expanding role of
	IT will continue for organizations that face unstable environments
['organization design', 'information processing perspective', 'organizationalstructures', 'organization task environment', 'environmental instability', 'information technology']
['organizational design that good fit requirement', 'second world war', 'information processing perspective', 'organizational structure', 'information technology', 'relative stability', 'organization design', 'environmental instability', 'new form', 'new model']

Knowledge-based structures and organisational commitment
Organisational commitment, the emotional attachment of an employee to the
	employing organisation, has attracted a substantial body of literature,
	relating the concept to various antecedents, including organisational
	structure, and to a range of consequences, including financially
	important performance factors such as productivity and staff turnover.
	The new areas of knowledge management and learning organisations offer
	substantial promise as imperatives for the organisation of business
	enterprises. As organisations in the contemporary environment adopt
	knowledge-based structures to improve their competitive position, there
	is value in examining these structures against other performance
	related factors. Theoretical knowledge-based structures put forward by
	R. Miles et al. (1997) and J. Quinn et al. (1996) and an existing
	implementation are examined to determine common features inherent in
	these approaches. These features are posited as a typical form and
	their impact on organisational commitment and hence on individual and
	organisational performance is examined
['knowledge-based structures', 'emotional attachment', 'performance factors', 'productivity', 'staff turnover', 'earning organisations', 'organisationalcommitment']
['organisational commitment', 'j. quinn et al', 'important performance factor', 'emotional attachment', 'r. miles et al', 'substantial body', 'structure', 'substantial promise', 'learning organisation', 'knowledge management']

The evolution of information systems: Their impact on organizations and
	structures
Information systems and organization structures have been highly interconnected
	with each other. Over the years, information systems architectures as
	well as organization structures have evolved from centralized to more
	decentralized forms. This research looks at the evolution of both
	information systems and organization structures. In the process, it
	looks into the impact of computers on organizations, and examines the
	ways organization structures have changed, in association with changes
	in information system architectures. It also suggests logical linkages
	between information system architectures and their "fit" with certain
	organization structures and strategies. It concludes with some
	implications for emerging and future organizational forms, and provides
	a quick review of the effect of the Internet on small businesses
	traditionally using stand-alone computers
['information systems evolution', 'information system architectures']
['information system architecture', 'information system', 'organization structure', 'way organization structure', '-PRON- impact', 'future organizational form', 'decentralized form', 'evolution', 'logical linkage', 'computer']

In search of a general enterprise model
Many organisations, particularly SMEs, are reluctant to invest time and money
	in models to support decision making. Such reluctance could be overcome
	if a model could be used for several purposes rather than using a
	traditional "single perspective" model. This requires the development
	of a "general enterprise model" (GEM), which can be applied to a wide
	range of problem domains with unlimited scope. Current enterprise
	modelling frameworks only deal effectively with nondynamic modelling
	issues whilst dynamic modelling issues have traditionally only been
	addressed at the operational level. Although the majority of research
	in this area relates to manufacturing companies, the framework for a
	GEM must be equally applicable to service and public sector
	organisations. The paper identifies five key design issues that need to
	be considered when constructing a GEM. A framework for such a GEM is
	presented based on a "plug and play" methodology and demonstrated by a
	simple case study
['general enterprise model', 'business process re-engineering', 'smes', 'decisionmaking', 'single perspective model', 'gem', 'problem domains', 'enterprisemodelling frameworks', 'operational level', 'dynamic modelling issues', 'public sector organisations', 'service sector organisations', 'plug andplay methodology', 'case study']
['general enterprise model', 'key design issue', 'simple case study', 'dynamic modelling issue', 'such reluctance', 'many organisation', 'gem', 'decision making', 'single perspective', 'modelling framework']

Strategies for high throughput, templated zeolite synthesis
The design and redesign of high throughput experiments for zeolite synthesis
	are addressed. A model that relates materials function to the chemical
	composition of the zeolite and the structure directing agent is
	introduced. Using this model, several Monte Carlo-like design protocols
	are evaluated. Multi-round protocols are bound to be effective, and
	strategies that use a priori information about the structure-directing
	libraries are found to be the best
['templated zeolite synthesis', 'high throughput strategies', 'materials function', 'chemical composition', 'structure directing agent', 'monte carlo-likedesign protocols', 'multi-round protocols', 'a priori information', 'catalytic activity', 'catalytic selectivity', 'organo-cation templatemolecules', 'combinatorial methods', 'random energy model', 'figure of merit', 'material discovery', 'small molecule design', 'voronoi diagram', 'phase-dependent random gaussian variables', 'metropolis-type method', 'ligand libraries', 'reflecting boundary conditions']
['like design protocol', 'zeolite synthesis', 'high throughput experiment', 'high throughput', 'strategy', 'material function', 'monte carlo', 'model', 'round protocol', 'zeolite']

Variable structure intelligent control for PM synchronous servo motor drive
The variable structure control (VSC) of discrete time systems based on
	intelligent control is presented in this paper. A novel approach is
	proposed for the state estimation. A linear observer is firstly
	designed. Then a neural network is used for compensating uncertainty.
	The parameter of the VSC scheme is adjusted online by a neural network.
	Practical operating results from a PM synchronous motor (PMSM)
	illustrate the effectiveness and practicability of the proposed
	approach
['pm synchronous servo motor drive', 'variable structure intelligent control', 'control design', 'discrete time systems', 'state estimation', 'linearobserver', 'neural network', 'uncertainty compensation', 'control performance']
['pm synchronous servo motor drive', 'variable structure intelligent control', 'discrete time system', 'variable structure control', 'neural network', 'linear observer', 'novel approach', 'state estimation', 'vsc scheme', 'approach']

A nonlinear modulation strategy for hybrid AC/DC power systems
A nonlinear control strategy to improve transient stability of a multi-machine
	AC power system with several DC links terminated in the presence of
	large disturbances is presented. The approach proposed in this paper is
	based on differential geometric theory, and the HVDC systems are taken
	as a variable admittance connected at the inverter or rectifier AC bus.
	After deriving the analytical description of the relationship between
	the variable admittance and active power flows of each generator, the
	traditional generator dynamic equations can thus be expressed with the
	variable admittance of HVDC systems as an additional state variable and
	changed to an affine form, which is suitable for global linearization
	method being used to determine its control variable. An important
	feature of the proposed method is that, the modulated DC power is an
	adaptive and non-linear function of AC system states, and it can be
	realized by local feedback and less transmitted data from, adjacent
	generators. The design procedure is tested on a dual-infeed hybrid
	AC/DC system
['nonlinear control strategy', 'transient stability', 'multi-machine ac power system', 'dc links', 'nonlinear modulation strategy', 'hybrid ac/dc power systems', 'differential geometric theory', 'hvdc systems', 'variable admittance', 'inverter', 'rectifier ac bus', 'active power flows', 'generator dynamicequations', 'affine form', 'global linearization method', 'local feedback', 'adjacent generators', 'dual-infeed hybrid ac/dc system']
['ac power system', 'nonlinear control strategy', 'dc power system', 'nonlinear modulation strategy', 'variable admittance', 'dc power', 'hvdc system', 'ac bus', 'rectifi ac', 'analytical description']

Mobile computing "Killer app" competition
Design competitions offer students an excellent way to gain hands-on experience
	in engineering and computer science courses. The University of Florida,
	in partnership with Motorola, has held two mobile computing design
	competitions. In Spring and Fall 2001, students in Abdelsalam Helal's
	Mobile Computing class designed killer apps for a Motorola smart phone
['mobile computing', 'smart phone', 'motorola', 'design competitions']
['killer app', 'mobile computing', 'motorola smart phone', 'design competition', 'excellent way', 'mobile computing class', 'mobile computing design', 'computer science course', 'competition', 'abdelsalam helal']

Firewall card shields data
The SlotShield 3000 firewall on a PCI card saves power and space, but might not
	offer enough security for large networks
['slotshield 3000 firewall', 'pci card', 'security', 'large networks']
['firewall card shield datum', 'pci card', 'offer enough security', 'space', 'power', 'large network', 'slotshield']

Standards for service discovery and delivery
For the past five years, competing industries and standards developers have
	been hotly pursuing automatic configuration, now coined the broader
	term service discovery. Jini, Universal Plug and Play (UPnP),
	Salutation, and Service Location Protocol are among the front-runners
	in this new race. However, choosing service discovery as the topic of
	the hour goes beyond the need for plug-and-play solutions or support
	for the SOHO (small office/home office) user. Service discovery's
	potential in mobile and pervasive computing environments motivated my
	choice
['service discovery', 'jini', 'universal plug and play', 'salutation', 'service locationprotocol', 'mobile computing', 'pervasive computing']
['service discovery', 'service location protocol', 'pervasive computing environment', 'term service discovery', 'standard developer', 'automatic configuration', 'universal plug', 'new race', 'play solution', 'standard']

The role of speech input in wearable computing
Speech recognition seems like an attractive input mechanism for wearable
	computers, and as we saw in this magazine's first issue, several
	companies are promoting products that use limited speech interfaces for
	specific tasks. However, we must overcome several challenges to using
	speech recognition in more general contexts, and interface designers
	must be wary of applying the technology to situations where speech is
	inappropriate
['wearable computing', 'speech input', 'speech recognition', 'wearable computer', 'speechrecognizers', 'mobile speech recognition', 'background noise', 'speechinterfaces']
['limited speech interface', 'speech recognition', 'attractive input mechanism', 'wearable computing', 'speech input', 'specific task', 'interface designer', 'general context', 'product', 'company']

The ubiquitous provisioning of internet services to portable devices
Advances in mobile telecommunications and device miniaturization call for
	providing both standard and novel location- and context-dependent
	Internet services to mobile clients. Mobile agents are dynamic,
	asynchronous, and autonomous, making the MA programming paradigm
	suitable for developing novel middleware for mobility-enabled services
['mobile telecommunications', 'device miniaturization', 'internet services', 'mobileclients', 'mobile agents', 'mobility-enabled services', 'middleware']
['internet service', 'ma programming paradigm', 'portable device', 'device miniaturization', 'mobile telecommunication', 'mobile agent', 'novel location-', 'mobile client', 'novel middleware', 'ubiquitous provisioning']

Integrating virtual and physical context to support knowledge workers
The Kimura system augments and integrates independent tools into a pervasive
	computing system that monitors a user's interactions with the computer,
	an electronic whiteboard, and a variety of networked peripheral devices
	and data sources
['pervasive computing', 'knowledge workers', 'networked peripheral devices', 'electronic whiteboard', 'kimura system', 'data sources']
['data source', 'peripheral device', 'computing system', 'electronic whiteboard', 'independent tool', 'kimura system', 'knowledge worker', 'physical context', 'computer', 'interaction']

Data management in location-dependent information services
Location-dependent information services have great promise for mobile and
	pervasive computing environments. They can provide local and nonlocal
	news, weather, and traffic reports as well as directory services.
	Before they can be implemented on a large scale, however, several
	research issues must be addressed
['location-dependent information services', 'wireless networks', 'pervasivecomputing', 'mobile computing', 'news', 'weather', 'traffic reports', 'datamanagement', 'directory services']
['dependent information service', 'pervasive computing environment', 'data management', 'great promise', 'directory service', 'traffic report', 'location', 'large scale', 'weather', 'research issue']

Modeling privacy control in context-aware systems
Significant complexity issues challenge designers of context-aware systems with
	privacy control. Information spaces provide a way to organize
	information, resources, and services around important privacy-relevant
	contextual factors. In this article, we describe a theoretical model
	for privacy control in context-aware systems based on a core
	abstraction of information spaces. We have previously focused on
	deriving socially based privacy objectives in pervasive computing
	environments. Building on Ravi Sandhu's four-layer OM-AM (objectives,
	models, architectures, and mechanisms) idea, we aim to use information
	spaces to construct a model for privacy control that supports our
	socially based privacy objectives. We also discuss how we can introduce
	decentralization, a desirable property for many pervasive computing
	systems, into our information space model, using unified privacy
	tagging
['privacy', 'pervasive computing', 'context-aware systems', 'privacy control', 'smartoffice']
['privacy control', 'aware system', 'significant complexity issue', 'information space model', 'information space', 'important privacy', 'contextual factor', 'privacy objective', 'theoretical model', 'pervasive computing']

ConChat: a context-aware chat program
ConChat is a context-aware chat program that enriches electronic communication
	by providing contextual information and resolving potential semantic
	conflicts between users.ConChat uses contextual information to improve
	electronic communication. Using contextual cues, users can infer during
	a conversation what the other person is doing and what is happening in
	his or her immediate surroundings. For example, if a user learns that
	the other person is talking with somebody else or is involved in some
	urgent activity, he or she knows to expect a slower response.
	Conversely, if the user learns that the other person is sitting in a
	meeting directly related to the conversation, he or she then knows to
	respond more quickly. Also, by informing users about the other person's
	context and tagging potentially ambiguous chat messages, ConChat
	explores how context can improve electronic communication by reducing
	semantic conflicts
['context-aware chat program', 'conchat', 'contextual information', 'semanticconflicts', 'contextual cues']
['aware chat program', 'electronic communication', 'contextual information', 'user', 'potential semantic', 'contextual cue', 'conchat', 'person', 'immediate surrounding', 'urgent activity']

A context-aware decision engine for content adaptation
Building a good content adaptation service for mobile devices poses many
	challenges. To meet these challenges, this quality-of-service-aware
	decision engine automatically negotiates for the appropriate adaptation
	decision for synthesizing an optimal content version
['content adaptation', 'mobile devices', 'quality-of-service-aware', 'decision engine', 'optimal content version', 'adaptation decision']
['good content adaptation service', 'aware decision engine', 'decision engine', 'mobile device', 'challenge', 'appropriate adaptation', 'quality', 'optimal content version', 'building', 'context']

Reconfigurable context-sensitive middleware for pervasive computing
Context-sensitive applications need data from sensors, devices, and user
	actions, and might need ad hoc communication support to dynamically
	discover new devices and engage in spontaneous information exchange.
	Reconfigurable Context-Sensitive Middleware facilitates the development
	and runtime operations of context-sensitive pervasive computing
	software
['pervasive computing', 'reconfigurable context-sensitive middleware', 'context-sensitive pervasive computing', 'middleware', 'context-sensitiveapplications']
['reconfigurable context', 'sensitive middleware', 'sensitive application', 'pervasive computing', 'sensitive pervasive computing', 'spontaneous information exchange', 'discover new device', 'communication support', 'context', 'runtime operation']

Activity and location recognition using wearable sensors
Using measured acceleration and angular velocity data gathered through
	inexpensive, wearable sensors, this dead-reckoning method can determine
	a user's location, detect transitions between preselected locations,
	and recognize and classify sitting, standing, and walking behaviors.
	Experiments demonstrate the proposed method's effectiveness
['measured acceleration', 'angular velocity', 'wearable sensors', 'dead-reckoningmethod', "user's location", 'preselected locations', 'transitions', 'sitting', 'standing', 'walking']
['wearable sensor', 'angular velocity datum', 'location', 'location recognition', 'method', 'acceleration', 'inexpensive', 'transition', 'user', 'proposed method']

Analyzing the benefits of 300 mm conveyor-based AMHS
While the need for automation in 300 mm fabs is not debated, the form and
	performance of such automation is still in question. Software
	simulation that compares conveyor-based continuous flow transport
	technology to conventional car-based wafer-lot delivery has detailed
	delivery time and throughput advantages to the former
['software simulation', 'car-based wafer-lot delivery', 'conveyor-based continuousflow transport technology', 'automated material handling system', 'semiconductor fab', 'throughput', 'wafer processing', 'delivery time', '300 mm']
['continuous flow transport', 'mm fab', 'mm conveyor', 'conveyor', 'conventional car', 'automation', 'lot delivery', 'delivery time', 'throughput advantage', 'amhs']

How to avoid merger pitfalls
Paul Diamond of consultancy KPMG explains why careful IT asset management is
	crucial to the success of mergers
['consultancy', 'kpmg', 'it asset management', 'mergers']
['careful it asset management', 'consultancy kpmg', 'paul diamond', 'merger pitfall', 'merger', 'success', 'crucial']

Labscape: a smart environment for the cell biology laboratory
Labscape is a smart environment that we designed to improve the experience of
	people who work in a cell biology laboratory. Our goal in creating it
	was to simplify, laboratory work by making information available where
	it is needed and by collecting and organizing data where and when it is
	created into a formal representation that others can understand and
	process. By helping biologists produce a more complete record of their
	work with less effort, Labscape is designed to foster improved
	collaboration in conjunction with increased individual efficiency and
	satisfaction. A user-driven system, although technologically
	conservative, embraces a central goal of ubiquitous computing: to
	enhance the ability to perform domain tasks through fluid interaction
	with computational resources. Smart environments could soon replace the
	pen and paper commonly used in the laboratory setting
['cell biology', 'labscape', 'laboratory work', 'ubiquitous computing', 'smartenvironment', 'experimental technologies', 'biochemical procedure']
['cell biology laboratory', 'smart environment', '-PRON- goal', 'labscape', 'information available', 'laboratory work', 'organizing datum', 'formal representation', 'complete record', 'individual efficiency']

Broadcasts keep staff in picture [intranets]
Mark Hawkins, chief operating officer at UK-based streaming media specialist
	Twofourtv, explains how firms can benefit by linking their corporate
	intranets to broadcasting technology
['corporate intranets', 'twofourtv', 'streaming media', 'broadcasting technology']
['chief operating officer', 'intranet', 'mark hawkins', 'medium specialist', 'uk', 'picture', 'twofourtv', 'firm', 'staff', 'corporate']

Java portability put to the test
Sun Microsystems' recently launched Java Verification Program aims to enable
	companies to assess the cross-platform portability of applications
	written in Java, and to help software vendors ensure that their
	solutions can run in heterogenous J2EE application server environments
['sun microsystems', 'java verification program', 'cross-platform portability']
['heterogenous j2ee application server environment', 'java verification program', 'sun microsystems', 'java portability', 'platform portability', 'software vendor', 'cross', 'company', 'solution', 'test']

The eyes have it [hotel security]
CCTV systems can help lodging establishments accomplish a range of objectives,
	from deterring criminals to observing staff interactions with
	clientele. But pitfalls can arise if the CCTV system has not been
	properly integrated into the overall hotel security plan. CCTV system
	designs at new hotel properties are often too sophisticated, too
	complicated, and too costly, and do not take into consideration the
	security realities of site management. These problems arise when the
	professionals designing or installing the system, including architects,
	construction engineers, integrators, and consultants, are not familiar
	with a hotel's operating strategies or security standards
['hotel security', 'cctv system', 'site management', 'operating strategies']
['cctv system', 'overall hotel security plan', 'new hotel property', 'staff interaction', 'site management', 'system', 'security reality', 'construction engineer', 'objective', 'pitfall']

Online masquerade: whose e-mail is it?
E-mails carrying viruses like the recent Klez worm use deceptively simple
	techniques and known vulnerabilities to spread from one computer to
	another with ease
['e-mail', 'klez worm', 'viruses', 'vulnerabilities']
['recent klez worm', 'online masquerade', 'mail', 'vulnerability', 'technique', 'computer', 'virus', 'simple', 'ease']

Relativistic constraints on the distinguishability of orthogonal quantum states
The constraints imposed by special relativity on the distinguishability of
	quantum states are discussed. An explicit expression relating the
	probability of an error in distinguishing two orthogonal single-photon
	states to their structure, the time t at which a measurement starts,
	and the interval of time T elapsed from the start of the measurement
	until the time at which the outcome is obtained by an observer is given
	as an example
['relativistic constraints', 'orthogonal quantum states', 'special relativity', 'orthogonal single-photon states', 'time interval', 'observer', 'nonrelativistic quantum information theory', 'quantum communicationchannels', 'quantum-state distinguishability']
['orthogonal quantum state', 'time t', 'quantum state', 'relativistic constraint', 'special relativity', 'explicit expression', 'orthogonal single', 'measurement', 'distinguishability', 'time']

Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel
	electrophoresis of DNA
Large-scale genotyping, mapping and expression profiling require affordable,
	fully automated high-throughput devices enabling rapid,
	high-performance analysis using minute quantities of reagents. In this
	paper, we describe a new combination of microwell polymerase chain
	reaction (PCR) based DNA amplification technique with automated
	ultrathin-layer gel electrophoresis analysis of the resulting products.
	This technique decreases the reagent consumption (total reaction volume
	0.75-1 mu L), the time requirement of the PCR (15-20 min) and
	subsequent ultrathin-layer gel electrophoresis based fragment analysis
	(5 min) by automating the current manual procedure and reducing the
	human intervention using sample loading robots and computerized real
	time data analysis. Small aliquots (0.2 mu L) of the submicroliter size
	PCR reaction were transferred onto loading membranes and analyzed by
	ultrathin-layer gel electrophoresis which is a novel, high-performance
	and automated microseparation technique. This system employs integrated
	scanning laser-induced fluorescence-avalanche photodiode detection and
	combines the advantages of conventional slab and capillary gel
	electrophoresis. Visualization of the DNA fragments was accomplished by
	"in migratio" complexation with ethidium bromide during the
	electrophoresis process also enabling real time imaging and data
	analysis
['rapid microwell polymerase chain reaction', 'ultrathin-layer gel electrophoresis', 'dna amplification', 'large-scale genotyping', 'expression profiling', 'rapidhigh-performance analysis', 'automated electrophoresis analysis', 'reagentconsumption', 'sample loading robots', 'computerized real time dataanalysis', 'automated microseparation', 'integrated scanning lif apddetection', 'complexation with ethidium bromide', 'real time imaging']
['layer gel electrophoresis', 'microwell polymerase chain', 'subsequent ultrathin', 'layer gel', 'scale genotyping', 'expression profiling', 'mu l', 'analysis', 'rapid microwell', 'throughput device']

Simple minds [health care IT]
A few things done properly, and soon, is the short-term strategy for the UK NHS
	IT programme. Can it deliver this time?
['uk nhs it programme', 'health care', 'strategy']
['health care it', 'simple mind', 'it programme', 'uk nhs', 'term strategy', 'short', 'time', 'thing']

Absorption of long waves by nonresonant parametric microstructures
Using simple acoustical and mechanical models, we consider the conceptual
	possibility of designing an active absorbing (nonreflecting) coating in
	the form of a thin layer with small-scale stratification and fast time
	modulation of parameters. Algorithms for space-time modulation of the
	controlled-layer structure are studied in detail for a one-dimensional
	boundary-value problem. These algorithms do not require wave-field
	measurements, which eliminates the self-excitation problem that is
	characteristic of active systems. The majority of the considered
	algorithms of parametric control transform the low-frequency incident
	wave to high-frequency waves of the technological band for which the
	waveguiding medium inside the layer is assumed to be opaque
	(absorbing). The efficient use conditions are found for all the
	algorithms. It is shown that the absorbing layer can be as thin as
	desired with respect to the minimum spatial scale of the incident wave
	and ensures efficient absorption in a wide frequency interval (starting
	from zero frequency) that is bounded from above only by a finite
	space-time resolution of the parameter-control operations. The
	structure of a three-dimensional parametric "'black" coating whose
	efficiency is independent of the angle of incidence of an incoming wave
	is developed on the basis of the studied one-dimensional problems. The
	general solution of the problem of diffraction of incident waves from
	such a coating is obtained. This solution is analyzed in detail for the
	case of a disk-shaped element
['acoustical models', 'mechanical models', 'active absorbing coating', 'nonreflectingcoating', 'thin layer', 'small-scale stratification', 'fast time modulation', 'space-time modulation', 'controlled-layer structure', 'one-dimensionalboundary-value problem', 'parametric control', 'low-frequency incidentwave', 'high-frequency waves', 'waveguiding medium', 'absorbing layer', 'angleof incidence', 'one-dimensional problems', 'diffraction', 'disk-shapedelement']
['incident wave', 'spatial scale', 'use condition', 'minimum spatial', 'waveguiding medium', 'efficient use', 'technological band', 'frequency wave', 'frequency incident', 'layer structure']

2002 in-house fulfillment systems report [publishing]
CM's 13th annual survey of in-house fulfillment system suppliers brings you up
	to date on the current capabilities of the leading publication software
	packages
['survey', 'in-house fulfillment system', 'suppliers', 'publication software packages']
['house fulfillment system', 'house fulfillment system supplier', '13th annual survey', 'current capability', 'cm', 'publication software', 'date', 'package', 'publishing']

Writing the fulfillment RFP [publishing]
For the uninitiated, writing a request for proposal can seem both mysterious
	and daunting. Here's a format that will make you look like a pro the
	first time out
['request for proposal', 'fulfillment', 'publisher']
['fulfillment rfp', 'daunting', 'format', 'mysterious', 'proposal', 'request', 'uninitiated', 'time', 'publishing']

Library services today and tomorrow: lessons from iLumina, a digital library
	for creating and sharing teaching resources
This article is based on the emerging experience associated with a digital
	library of instructional resources, iLumina, in which the contributors
	of resources and the users of those resources are the same-an open
	community of instructors in science, mathematics, engineering, and
	technology. Moreover, it is not the resources, most of which will be
	distributed across the Internet, but metadata about the resources that
	is the focus of the central iLumina repository and its support services
	for resource contributors and users. The distributed iLumina library is
	a community-sharing library for repurposing and adding value to
	potentially useful, mostly non-commercial instructional resources that
	are typically more granular in nature than commercially developed
	course materials. The experience of developing iLumina is raising a
	range of issues that have nothing to do with the place and time
	characteristics of the instructional context in which iLumina
	instructional resources are created or used. The issues instead have
	their locus in the democratization of both the professional roles of
	librarians and the quality assurance mechanisms associated with
	traditional peer review
['ilumina', 'digital library', 'teaching resource sharing', 'internet', 'metadata', 'information resources', 'community-sharing library', 'professional roles', 'academic library', 'librarians', 'quality assurance', 'peer review', 'libraryautomation', 'standards', 'interoperability', 'reusable software', 'distributedsystems', 'user issues']
['commercial instructional resource', 'library service today', 'traditional peer review', 'quality assurance mechanism', 'central ilumina repository', 'instructional resource', 'resource', 'ilumina', 'teaching resource', 'library']

The Canadian National Site Licensing Project
In January 2000, a consortium of 64 universities in Canada signed a historic
	inter-institutional agreement that launched the Canadian National Site
	Licensing Project (CNSLP), a three-year pilot project aimed at
	bolstering the research and innovation capacity of the country's
	universities. CNSLP tests the feasibility of licensing, on a national
	scale, electronic versions of scholarly publications; in its initial
	phases the project is focused on full-text electronic journals and
	research databases in science, engineering, health and environmental
	disciplines. This article provides an overview of the CNSLP initiative,
	summarizes organizational and licensing accomplishments to date, and
	offers preliminary observations on challenges and opportunities for
	subsequent phases of the project
['canadian national site licensing project', 'inter-institutional agreement', 'research and innovation', 'cnslp', 'academic libraries', 'informationresources', 'electronic scholarly publications', 'full-text electronicjournals', 'research databases']
['canadian national site licensing project', 'year pilot project', 'cnslp', 'text electronic journal', 'institutional agreement', 'university', 'innovation capacity', 'scholarly publication', 'electronic version', 'research database']

The UK's National Electronic Site Licensing Initiative (NESLI)
In 1998 the UK created the National Electronic Site Licensing Initiative
	(NESLI) to increase and improve access to electronic journals and to
	negotiate license agreements on behalf of academic libraries. The use
	of a model license agreement and the success of site licensing is
	discussed. Highlights from an interim evaluation by the Joint
	Information Systems Committee (JISC) are noted and key issues and
	questions arising from the evaluation are identified
['national electronic site licensing initiative', 'nesli', 'electronic journals', 'license agreements', 'academic libraries', 'joint information systemscommittee', 'usage statistics', 'jisc', 'icolc']
['national electronic site licensing initiative', 'negotiate license agreement', 'model license agreement', 'electronic journal', 'academic library', 'information systems committee', 'uk', 'interim evaluation', 'nesli', 'evaluation']

The role of CAUL (Council of Australian Libraries) in consortial purchasing
The Council of Australian University Librarians, constituted in 1965 for the
	purposes of cooperative action and the sharing of information, assumed
	the role of consortial purchasing agent in 1996 on behalf of its
	members and associate organisations in Australia and New Zealand. This
	role continues to grow in tandem with the burgeoning of electronic
	publication and the acceptance of publishers of the advantages of
	dealing with consortia. The needs of the Australian university
	community overlap significantly with consortia in North America and
	Europe, but important differences are highlighted
['council of australian university librarians', 'cooperative action', 'informationsharing', 'consortial purchasing', 'australia', 'new zealand', 'electronicpublication', 'north america', 'europe']
['consortial purchasing agent', 'australian university librarians', 'consortial purchasing', 'australian university', 'cooperative action', 'role', 'new zealand', 'australian libraries', 'associate organisation', 'council']

Licensing experiences in the Netherlands
The licensing strategy of university libraries in the Netherlands is closely
	connected with university policies to develop document servers and to
	make research publications available on the Web. National agreements
	have been made with major publishers, such as Elsevier Science and
	Kluwer Academic, to provide access to a wide range of scientific
	information and to experiment with new ways of providing information
	and new business models
['licensing strategy', 'university libraries', 'netherlands', 'university policies', 'document servers', 'research publications', 'web', 'elsevier science', 'kluweracademic', 'scientific information', 'business models']
['research publication available', 'new business model', 'document server', 'kluwer academic', 'wide range', 'elsevier science', 'university library', 'national agreement', 'university policy', 'major publisher']

International library consortia: positive starts, promising futures
Library consortia have grown substantially over the past ten years, both within
	North America and globally. As this resurgent consortial movement has
	begun to mature, and as publishers and vendors have begun to adapt to
	consortial purchasing models, consortia have expanded their agendas for
	action. The movement to globalize consortia is traced (including the
	development and current work of the International Coalition of Library
	Consortia-ICOLC). A methodology is explored to classify library
	consortia by articulating the key factors that affect and distinguish
	consortia as organizations within three major areas: strategic,
	tactical, and practical (or managerial) concerns. Common consortial
	values are examined, and a list of known international library
	consortia is presented
['consortial purchasing models', 'international library consortia']
['consortial purchasing model', 'international library consortia', 'resurgent consortial movement', 'international library', 'consortium', 'positive start', 'library consortium', 'north america', 'international coalition', 'current work']

The Open Archives Initiative: realizing simple and effective digital library
	interoperability
The Open Archives Initiative (OAI) is dedicated to solving problems of digital
	library interoperability. Its focus has been on defining simple
	protocols, most recently for the exchange of metadata from archives.
	The OAI evolved out of a need to increase access to scholarly
	publications by supporting the creation of interoperable digital
	libraries. As a first step towards such interoperability, a metadata
	harvesting protocol was developed to support the streaming of metadata
	from one repository to another, ultimately to a provider of user
	services such as browsing, searching, or annotation. This article
	provides an overview of the mission, philosophy, and technical
	framework of the OAI
['open archives initiative', 'digital library interoperability', 'protocols', 'exchangemetadata', 'scholarly publications', 'metadata harvesting protocol', 'streaming metadata', 'annotation', 'searching', 'browsing', 'user services']
['open archives initiative', 'effective digital library', 'oai', 'interoperability', '-PRON- focus', 'simple', 'library interoperability', 'metadata', 'interoperable digital', 'harvesting protocol']

Content standards for electronic books: the OEBF publication structure and the
	role of public interest participation
In the emerging world of electronic publishing how we create, distribute, and
	read books will be in a large part determined by an underlying
	framework of content standards that establishes the range of
	technological opportunities and constraints for publishing and reading
	systems. But efforts to develop content standards based on sound
	engineering models must skillfully negotiate competing and sometimes
	apparently irreconcilable objectives if they are to produce results
	relevant to the rapidly changing course of technology. The Open eBook
	Forum's Publication Structure, an XML-based specification for
	electronic books, is an example of the sort of timely and innovative
	problem solving required for successful real-world standards
	development. As a result of this effort, the electronic book industry
	will not only happen sooner and on a larger scale than it would have
	otherwise, but the electronic books it produces will be more
	functional, more interoperable, and more accessible to all readers.
	Public interest participants have a critical role in this process
['electronic publishing', 'electronic books', 'content standards', 'oebf publicationstructure', 'public interest participation', 'open ebook forum publicationstructure', 'xml-based specification']
['content standard', 'electronic book', 'public interest participation', 'public interest participant', 'electronic book industry', 'oebf publication structure', 'publication structure', 'electronic publishing', 'read book', 'technological opportunity']

Fuzzy modeling based on generalized conjunction operations
An approach to fuzzy modeling based on the tuning of parametric conjunction
	operations is proposed. First, some methods for the construction of
	parametric generalized conjunction operations simpler than the known
	parametric classes of conjunctions are considered and discussed.
	Second, several examples of function approximation by fuzzy models,
	based on the tuning of the parameters of the new conjunction
	operations, are given and their approximation performances are compared
	with the approaches based on a tuning of membership functions and other
	approaches proposed in the literature. It is seen that the tuning of
	the conjunction operations can be used for obtaining fuzzy models with
	a sufficiently good performance when the tuning of membership functions
	is not possible or not desirable
['fuzzy modeling', 'generalized conjunction operations', 'function approximation', 'tuning', 'approximation performances', 'membership functions', 't-norm', 'fuzzyinference systems']
['fuzzy modeling', 'conjunction operation', 'conjunction operation simple', 'parametric conjunction', 'parametric class', 'membership function', 'function approximation', 'tuning', 'new conjunction', 'approximation performance']

Project Euclid and the role of research libraries in scholarly publishing
Project Euclid, a joint electronic journal publishing initiative of Cornell
	University Library and Duke University Press is discussed in the
	broader contexts of the changing patterns of scholarly communication
	and the publishing scene of mathematics. Specific aspects of the
	project such as partnerships and the creation of an economic model are
	presented as well as what it takes to be a publisher. Libraries have
	gained important and relevant experience through the creation and
	management of digital libraries, but they need to develop further
	skills if they want to adopt a new role in the life cycle of scholarly
	communication
['project euclid', 'joint electronic journal publishing initiative', 'cornelluniversity library', 'duke university press', 'scholarly communication', 'mathematics', 'partnerships', 'economic model', 'scholarly publishing', 'research libraries']
['joint electronic journal publishing initiative', 'project euclid', 'duke university press', 'scholarly publishing', 'research library', 'specific aspect', 'publishing scene', 'university library', 'scholarly communication', 'broad context']

Perspectives on scholarly online books: the Columbia University Online Books
	Evaluation Project
The Online Books Evaluation Project at Columbia University studied the
	potential for scholarly online books from 1995 to 1999. Issues included
	scholars' interest in using online books, the role they might play in
	scholarly life, features that scholars and librarians sought in online
	books, the costs of producing and owning print and online books, and
	potential marketplace arrangements. Scholars see potential for online
	books to make their research, learning, and teaching more efficient and
	effective. Librarians see potential to serve their scholars better.
	Librarians may face lower costs if they can serve their scholars with
	online books instead of print books. Publishers may be able to offer
	scholars greater opportunities to use their books while enhancing their
	own profitability
['columbia university online books evaluation project', 'scholarly online books', 'print books', 'costs', 'marketplace arrangements', 'research', 'learning']
['online books evaluation project', 'scholarly online book', 'columbia university online books', 'potential', 'potential marketplace arrangement', 'scholarly life', 'cost', 'librarians', 'low cost', 'print']

The California Digital Library and the eScholarship program
The eScholarship program was launched in 2000 to foster faculty-led innovation
	in scholarly publishing. An initiative of the University of California
	(UC) and a program of the California Digital Library, the eScholarship
	program has stimulated significant interest in its short life. Its
	modest but visible accomplishments garner praise from many quarters,
	within and beyond the University of California. In perhaps the best
	indication of its timeliness and momentum, there are more proposals
	submitted to eScholarship today than the CDL can manage. This early
	success is due in part to the sheer power of an idea whose time has
	come, but also to the unique approach on which CDL was founded and the
	eScholarship initiative was first launched
['escholarship program', 'faculty-led innovation', 'california digital library', 'scholarly publishing', 'university of california']
['california digital library', 'escholarship program', 'scholarly publishing', 'escholarship', 'idea whose time', 'visible accomplishment', 'program', 'short life', 'significant interest', 'university']

BioOne: a new model for scholarly publishing
This article describes a unique electronic journal publishing project involving
	the University of Kansas, the Big 12 Plus Libraries Consortium, the
	American Institute of Biological Sciences, Allen Press, and SPARC, the
	Scholarly Publishing and Academic Resources Coalition. This partnership
	has created BioOne, a database of 40 full-text society journals in the
	biological and environmental sciences, which was launched in April,
	2001. The genesis and development of the project is described and
	financial, technical, and intellectual property models for the project
	are discussed. Collaborative strategies for the project are described
['bioone full-text society journal database', 'electronic journal publishingproject', 'scholarly publishing model', 'university of kansas', 'big 12 pluslibraries consortium', 'american institute of biological sciences', 'allenpress', 'sparc', 'scholarly publishing and academic resources coalition', 'biological sciences', 'environmental sciences', 'intellectual propertymodels', 'technical models', 'financial models', 'collaborative strategies']
['unique electronic journal publishing project', 'scholarly publishing', 'plus libraries consortium', 'academic resources coalition', 'text society journal', 'bioone', 'allen press', 'biological sciences', 'american institute', 'new model']

Symbiosis or alienation: advancing the university press/research library
	relationship through electronic scholarly communication
University presses and research libraries have a long tradition of
	collaboration. The rapidly expanding electronic scholarly communication
	environment offers important new opportunities for cooperation and for
	innovative new models of publishing. The economics of libraries and
	scholarly publishers have strained the working relationship and
	promoted debates on important information policy issues. This article
	explores the context for advancing the partnership, cites examples of
	joint efforts in electronic publishing, and presents an action plan for
	working together
['university press/research library relationship', 'electronic scholarlycommunication', 'economics', 'information policy', 'electronic publishing']
['electronic scholarly communication', 'important information policy issue', 'innovative new model', 'university press', 'important new opportunity', 'research library', 'long tradition', 'relationship', 'library', 'scholarly publisher']

Support vector machines model for classification of thermal error in machine
	tools
This paper addresses a change in the concept of machine tool thermal error
	prediction which has been hitherto carried out by directly mapping them
	with the temperature of critical elements on the machine. The model
	developed herein using support vector machines, a powerful
	data-training algorithm, seeks to account for the impact of specific
	operating conditions, in addition to temperature variation, on the
	effective prediction of thermal errors. Several experiments were
	conducted to study the error pattern, which was found to change
	significantly with variation in operating conditions. This model
	attempts to classify the error based on operating conditions. Once
	classified, the error is then predicted based on the temperature
	states. This paper also briefly describes the concept of the
	implementation of such a comprehensive model along with an on-line
	error assessment and calibration system in a PC-based open-architecture
	controller environment, so that it could be employed in regular
	production for the purpose of periodic calibration of machine tools
['svm', 'support vector machines model', 'thermal error classification', 'machine toolthermal error prediction', 'critical element temperature', 'data-trainingalgorithm', 'error pattern', 'online error assessment', 'online calibrationsystem', 'pc-based open-architecture controller environment']
['machine tool thermal error', 'support vector machine model', 'support vector machine', 'thermal error', 'operating condition', 'concept', 'temperature variation', 'critical element', 'effective prediction', 'several experiment']

Adaptive and efficient mutual exclusion
The paper presents adaptive algorithms for mutual exclusion using only read and
	write operations; the performance of the algorithms depends only on the
	point contention, i.e., the number of processes that are concurrently
	active during algorithm execution (and not on n, the total number of
	processes). Our algorithm has O(k) remote step complexity and O(log k)
	system response time, where k is the point contention. The remote step
	complexity is the maximal number of steps performed by a process where
	a wait is counted as one step. The system response time is the time
	interval between subsequent entries to the critical section, where one
	time unit is the minimal interval in which every active process
	performs at least one step. The space complexity of this algorithm is
	O(N log n), where N is the range of process names. We show how to make
	the space complexity of our algorithm depend solely on n, while
	preserving the other performance measures of the algorithm
['adaptive mutual exclusion', 'adaptive algorithms', 'read operations', 'writeoperations', 'point contention', 'algorithm execution', 'remote stepcomplexity', 'system response time', 'critical section', 'minimal interval', 'active process', 'space complexity', 'performance measures']
['system response time', 'efficient mutual exclusion', 'remote step complexity', 'point contention', 'mutual exclusion', 'space complexity', 'remote step', 'algorithm execution', 'adaptive algorithm', '-PRON- algorithm']

The congenial talking philosophers problem in computer networks
Group mutual exclusion occurs naturally in situations where a resource can be
	shared by processes of the same group, but not by processes of
	different groups. For example, suppose data is stored in a CD-jukebox.
	Then, when a disc is loaded for access, users that need data on the
	disc can concurrently access the disc, while users that need data on a
	different disc have to wait until the current disc is unloaded. The
	design issues for group mutual exclusion have been modeled as the
	Congenial Talking Philosophers problem, and solutions for shared memory
	models have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir,
	1999). As in ordinary mutual exclusion and many other problems in
	distributed systems, however, techniques developed for shared memory do
	not necessarily apply to message passing (and vice versa). We
	investigate solutions for Congenial Talking Philosophers in computer
	networks where processes communicate by asynchronous message passing.
	We first present a solution that is a straightforward adaptation from
	G. Ricart and A.K. Agrawala's (1981) algorithm for ordinary mutual
	exclusion. Then we show that the simple modification suffers a severe
	performance degradation that could cause the system to behave as though
	only one process of a group can be in the critical section at a time.
	We then present a more efficient and highly concurrent distributed
	algorithm for the problem, the first such solution in computer networks
['congenial talking philosophers problem', 'computer networks', 'group mutualexclusion', 'resource sharing', 'shared-memory models', 'distributed systems', 'process communication', 'asynchronous message passing', 'critical section', 'concurrent distributed algorithm']
['group mutual exclusion', 'computer network', 'philosopher problem', 'ordinary mutual exclusion', 'congenial', 'different group', 'process', 'problem', 'datum', 'different disc']

Universal dynamic synchronous self-stabilization
We prove the existence of a "universal" synchronous self-stabilizing protocol,
	that is, a protocol that allows a distributed system to stabilize to a
	desired nonreactive behaviour (as long as a protocol stabilizing to
	that behaviour exists). Previous proposals required drastic increases
	in asymmetry and knowledge to work, whereas our protocol does not use
	any additional knowledge, and does not require more symmetry-breaking
	conditions than available; thus, it is also stabilizing with respect to
	dynamic changes in the topology. We prove an optimal quiescence time n
	+ D for a synchronous network of n processors and diameter D; the
	protocol can be made finite state with a negligible loss in quiescence
	time. Moreover, an optimal D + 1 protocol is given for the case of
	unique identifiers. As a consequence, we provide an effective proof
	technique that allows one to show whether self-stabilization to a
	certain behaviour is possible under a wide range of models
['universal dynamic synchronous self-stabilization', 'synchronous self-stabilizingprotocol', 'distributed system', 'nonreactive behaviour', 'topology', 'dynamicchanges', 'optimal quiescence time', 'synchronous network', 'finite state', 'quiescence time', 'optimal protocol', 'unique identifiers', 'proof technique', 'self-stabilization', 'anonymous networks', 'graph fibrations']
['universal dynamic synchronous self', 'protocol', 'optimal quiescence time', 'stabilization', 'drastic increase', 'additional knowledge', 'nonreactive behaviour', 'previous proposal', 'dynamic change', 'diameter d']

Randomized two-process wait-free test-and-set
We present the first explicit, and currently simplest, randomized algorithm for
	two-process wait-free test-and-set. It is implemented with two 4-valued
	single writer single reader atomic variables. A test-and-set takes at
	most 11 expected elementary steps, while a reset takes exactly 1
	elementary step. Based on a finite-state analysis, the proofs of
	correctness and expected length are compressed into one table
['randomized two-process wait-free test-and-set', 'randomized algorithm', '4-valuedsingle writer single reader atomic variables', 'expected elementarysteps', 'finite-state analysis', 'correctness proofs', 'symmetry breaking', 'asynchronous distributed protocols', 'fault-tolerance', 'shared memory', 'wait-free read/write registers']
['free test', 'single writer single reader atomic variable', 'randomized algorithm', 'elementary step', 'process', 'randomized', 'state analysis', 'test', 'simple', 'explicit']

Identification of evolving fuzzy rule-based models
An approach to identification of evolving fuzzy rule-based (eR) models is
	proposed. eR models implement a method for the noniterative update of
	both the rule-base structure and parameters by incremental unsupervised
	learning. The rule-base evolves by adding more informative rules than
	those that previously formed the model. In addition, existing rules can
	be replaced with new rules based on ranking using the informative
	potential of the data. In this way, the rule-base structure is
	inherited and updated when new informative data become available,
	rather than being completely retrained. The adaptive nature of these
	evolving rule-based models, in combination with the highly transparent
	and compact form of fuzzy rules, makes them a promising candidate for
	modeling and control of complex processes, competitive to neural
	networks. The approach has been tested on a benchmark problem and on an
	air-conditioning component modeling application using data from an
	installation serving a real building. The results illustrate the
	viability and efficiency of the approach
['evolving fuzzy rule-based models', 'identification', 'noniterative update', 'rule-base structure', 'incremental unsupervised learning', 'ranking', 'informative potential', 'fuzzy rules', 'complex processes', 'air-conditioningcomponent modeling', 'adaptive nonlinear control', 'fault detection', 'faultdiagnostics', 'performance analysis', 'forecasting', 'knowledge extraction', 'robotics', 'behavior modeling']
['fuzzy rule', 'base structure', 'conditioning component modeling application', 'new informative datum', 'incremental unsupervised', 'informative rule', 'base evolf', 'er model', 'noniterative update', 'rule']

Aim for the enterprise: Microsoft Project 2002
A long-time favorite of project managers, Microsoft Project 2002 is making its
	enterprise debut. Its new Web-based collaboration tools and improved
	scalability with OLAP support make it much easier to manage multiple
	Web projects with disparate workgroups and budgets
['microsoft project 2002', 'web-based collaboration tools', 'scalability', 'olapsupport', 'multiple web project management', 'workgroups', 'budgets']
['microsoft project', '-PRON- new web', 'project manager', 'time favorite', 'enterprise debut', 'collaboration tool', 'olap support', 'web project', 'disparate workgroup', 'enterprise']

Central hub for design assets: Adobe GoLive 6.0
Adobe GoLive is a strong contender for Web authoring and publishing. Version
	6.0 features a flexible GUI environment combined with a comprehensive
	workgroup and collaboration server, plus tight integration with leading
	design tools
['adobe golive 6.0', 'flash', 'real', 'java', 'application servers', 'web authoring', 'gui', 'workgroup server', 'collaboration server', 'livemotion 2.0', 'animation andscripting tool', 'macromedia swf format', 'workgroup environment', 'webpublishing environment', 'design-centric dynamic content']
['adobe golive', 'flexible gui environment', 'strong contender', 'design asset', 'central hub', 'collaboration server', 'tight integration', 'design tool', 'version', 'workgroup']

Reaching for five nines: ActiveWatch and SiteSeer
Every Web admin's dream is achieving the fabled five nines-99.999 percent
	uptime. To attain such availability, your Web site must be down no more
	than about five minutes per year. Technologies like RAID, clustering,
	and load balancing make this easier, but to actually track uptime,
	maintain auditable records, and discover patterns in failures to
	prevent downtime in the future, you'll need to set up external
	monitoring. Because your Internet connection is a key factor in
	measuring uptime, you must monitor your site from the Internet itself,
	beyond your firewall. You could monitor with custom software on remote
	hosts, or you could use one of the two reasonably priced services
	available: Mercury Interactive's ActiveWatch and Freshwater Software's
	SiteSeer. (Freshwater Software has been a subsidiary of Mercury
	Interactive for about a year now.) The two services offer a slightly
	different mix of features and target different markets. Both services
	offer availability and performance monitoring from several remote
	locations, alerts to email or pager, and periodic reports. They differ
	in what's most easily monitored, and in the way you interact with the
	services
['web site', 'uptime tracking', 'auditable records', 'failure pattern discovery', 'downtime', 'external monitoring', 'internet connection', 'mercury interactiveactivewatch', 'freshwater software siteseer', 'performance monitoring', 'availability monitoring', 'remote locations', 'email alerts', 'pager alerts', 'periodic reports']
['maintain auditable record', 'freshwater software', 'activewatch', 'siteseer', 'web site', 'uptime', 'key factor', 'load balancing', 'nines-99.999 percent', 'prevent downtime']

Accessible streaming content
Make sure your Web site is offering quality service to all your users. The
	article provides some tips and tactics for making your streaming media
	accessible. Accessibility of streaming content for people with
	disabilities is often not part of the spec for multimedia projects, but
	it certainly affects your quality of service. Most of the resources
	available on Web accessibility deal with HTML. Fortunately, rich media
	and streaming content developers have a growing number of experts to
	turn to for information and assistance. The essentials of providing
	accessible streaming content are simple: blind and visually impaired
	people need audio to discern important visual detail and interface
	elements, while deaf and hard-of-hearing people need text to access
	sound effects and dialog. Actually implementing these principles is
	quite a challenge, though. Now due to a relatively new law in the US,
	known as Section 508, dealing with accessibility issues is becoming an
	essential part of publishing on the Web
['web site', 'quality service', 'streaming media', 'content providers', 'united states', 'accessible streaming content', 'disabled users', 'multimedia projects', 'webaccessibility', 'html', 'streaming content developers', 'visually impairedpeople', 'blind people', 'visual detail', 'interface elements', 'deaf people', 'hard-of-hearing people', 'sound effects', 'section 508', 'accessibilityissues', 'web publishing']
['accessible streaming content', 'sure -PRON- web site', 'web accessibility deal', 'streaming content developer', 'people', 'important visual detail', 'quality service', 'multimedia project', 'rich medium', 'essential']

What you get is what you see [Web performance monitoring]
To get the best possible performance from your Web infrastructure, you'll need
	a complete view. Don't neglect the big picture because you're too busy
	concentrating on details. The increasing complexity of Web sites and
	the content they provide has consequently increased the complexity of
	the infrastructure that supports them. But with some knowledge of
	networking, a handful of useful tools, and the insight that those tools
	provide, designing and operating for optimal performance and
	reliability is within your grasp
['web performance', 'web sites', 'web infrastructure', 'networking', 'reliability']
['good possible performance', 'web performance monitoring', 'big picture', 'web site', 'complete view', 'web infrastructure', 'useful tool', 'complexity', 'infrastructure', 'optimal performance']

The culture of usability
Now that most of us agree that usability testing is an integral investment in
	site development, it's time to recognize that the standard approach
	falls short. It is possible to do less work and get better results
	while spending less money. By bringing usability testing in-house and
	breaking tests into more manageable sessions, you can vastly improve
	your online offering without affecting your profit margin
['usability testing program', 'web site']
['usability testing', 'site development', 'integral investment', 'standard approach', 'profit margin', 'online offering', 'good result', 'manageable session', 'breaking test', 'usability']

Debugging Web applications
The author considers how one can save time tracking down bugs in Web-based
	applications by arming yourself with the right tools and programming
	practices. A wide variety of debugging tools have been written with Web
	developers in mind
['web application debugging tools', 'programming']
['wide variety', 'web application', 'right tool', 'web', 'application', 'author', 'time', 'bug', 'tool', 'practice']

Unsafe at any speed?
While Sun prides itself on Java's secure sandbox programming model, Microsoft
	takes a looser approach. Its C# language incorporates C-like concepts,
	including pointers and memory management. But is unsafe code really a
	boon to programmers, or is it a step backward?
['microsoft c# language', 'c-like concepts', 'pointers', 'memory management', 'sun javasecure sandbox programming model']
['secure sandbox programming model', 'unsafe', '-PRON- c', 'like concept', 'unsafe code', 'memory management', 'loose approach', 'java', 'sun', 'speed']

Building digital collections at the OAC: current strategies with a view to
	future uses
Providing a context for the exploration of user defined virtual collections,
	the article describes the history and recent development of the Online
	Archive of California (OAC). Stating that usability and user needs are
	primary factors in digital resource development, issues explored
	include collaborations to build digital collections, reliance upon
	professional standards for description and encoding, system
	architecture, interface design, the need for user tools, and the role
	of archivists as interpreters in the digital environment
['digital collections', 'oac', 'future uses', 'user defined virtual collections', 'history', 'online archive of california', 'user needs', 'digital resource', 'professional standards', 'system architecture', 'interface design', 'usertools', 'digital environment', 'encoded archival description', 'archivaldescriptive standards', 'metadata standards', 'best practices', 'user studies']
['digital collection', 'current strategy', 'digital resource development', 'future us', 'virtual collection', 'recent development', 'user need', 'primary factor', 'professional standard', 'oac']

Nuts and bolts: implementing descriptive standards to enable virtual
	collections
To date, online archival information systems have relied heavily on legacy
	finding aids for data to encode and provide to end users, despite
	fairly strong indications in the archival literature that such legacy
	data is problematic even as a mediated access tool. Archivists have
	only just begun to study the utility of archival descriptive data for
	end users in unmediated settings such as via the Web. The ability of
	future archival information systems to respond to the expectations and
	needs of end users is inextricably linked to archivists getting their
	collective data house in order. The General International Standard
	Archival Description (ISAD(G)) offers the profession a place from which
	to start extricating ourselves from the idiosyncracies of our legacy
	data and description practices
['descriptive standards', 'virtual collections', 'online archival informationsystems', 'end users', 'archival literature', 'legacy data', 'mediated accesstool', 'archivists', 'archival descriptive data', 'archival informationsystems', 'collective data house', 'general international standard archivaldescription', 'isad', 'online archive of california', 'oac']
['archival literature that such legacy', 'archival information system', 'future archival information system', 'archival descriptive datum', 'end user', 'descriptive standard', 'finding aid', 'strong indication', 'general international standard', 'collective datum house']

Learning weights for the quasi-weighted means
We study the determination of weights for quasi-weighted means (also called
	quasi-linear means) when a set of examples is given. We consider first
	a simple case, the learning of weights for weighted means, and then we
	extend the approach to the more general case of a quasi-weighted mean.
	We consider the case of a known arbitrary generator f. The paper
	finishes considering the use of parametric functions that are suitable
	when the values to aggregate are measure values or ratio
['quasi-weighted means', 'quasi-linear means', 'learning', 'parametric functions', 'measure values', 'ratio values']
['known arbitrary generator', 'general case', 'mean', 'weighted mean', 'simple case', 'weight', 'linear mean', 'parametric function', 'determination', 'measure value']

Prospecting virtual collections
Virtual collections are a distinct sub-species of digital collections and
	digital archives. Archivists and curators as archivists and curators do
	not construct virtual collections; rather they enable virtual
	collections through the application of descriptive and other standards.
	Virtual collections are constructed by end users
['virtual collections', 'digital collections', 'digital archives', 'archivists', 'curators', 'descriptive standards', 'end users', 'digitization']
['virtual collection', 'distinct sub', 'digital collection', 'digital archive', 'virtual', 'collection', 'archivist', 'curator', 'specie', 'end user']

Union outreach - a pilgrim's progress
As the American labor movement continues on its path toward reorganization and
	rejuvenation, archivists are challenged to ensure that the
	organizational, political, and cultural changes labor unions are
	experiencing are fully documented. The article examines the need for
	labor archivists to reach out actively to unions and the problems they
	face in getting their message across, not only to union leadership but
	also to union members. Outreach by labor archivists is vital on three
	critical fronts: the need to secure union funding in support of labor
	archival programs; obtaining union cooperation in reviewing and
	amending obsolete deposit agreements; and coordinating efforts with
	unions to save the records of closing district and local union offices.
	Attempting to resolve these outstanding issues, labor archivists are
	pulled between two distinct institutional cultures (one academic in
	nature, the other enmeshed in a union bureaucracy) and often have their
	own labor archival programs compromised by the internal dynamics and
	politics inherent in administering large academic libraries and unions.
	If labor archivists are to be successful, they must find their
	collective voice within the labor movement and establish their
	relevancy to unions during a period of momentous change and
	restructuring. Moreover, archivists need to give greater thought to
	designing and implementing outreach programs that bridge the
	fundamental "disconnect" between union bureaucracies and the rank and
	file, and unions and the public
['american labor movement', 'archivists', 'political changes', 'cultural changes', 'laborunions', 'labor archivists', 'union leadership', 'union members', 'unionfunding', 'labor archival programs', 'union cooperation', 'obsolete depositagreements', 'union offices', 'institutional cultures', 'union bureaucracy', 'internal dynamics', 'large academic libraries', 'collective voice']
['labor archivist', 'cultural change labor', 'american labor movement', 'labor movement', 'union', 'archival program', 'union bureaucracy', 'union outreach', 'union leadership', 'union member']

The impact of EAD adoption on archival programs: a pilot survey of early
	implementers
The article reports the results of a survey conducted to assess the impact that
	the implementation of Encoded Archival Description (EAD) has on
	archival programs. By gathering data related to the funding, staffing,
	and evaluation of EAD programs and about institutional goals for EAD
	implementation, the study explored how EAD has affected the operations
	of the institutions which are utilizing it and the extent to which EAD
	has become a part of regular repository functions
['ead adoption', 'archival programs', 'encoded archival description', 'funding', 'staffing', 'ead programs', 'institutional goals', 'ead implementation', 'regular repository functions', 'archival descriptive standards', 'diffusionof innovation']
['archival program', 'pilot survey', 'ead', 'archival description', 'regular repository function', 'institutional goal', 'impact', 'ead program', 'ead adoption', 'implementation']

K-12 instruction and digital access to archival materials
Providing K-12 schools with digital access to archival materials can strengthen
	both student learning and archival practice, although it cannot replace
	direct physical access to records. The article compares a variety of
	electronic and nonelectronic projects to promote teaching with primary
	source materials. The article also examines some of the different
	historiographical and pedagogical approaches used in archival Web sites
	geared for K-12 instruction, focusing on differences between the
	educational sites sponsored by the Library of Congress and the National
	Archives and Records Administration
['k-12 instruction', 'digital access', 'archival materials', 'student learning', 'archival practice', 'electronic projects', 'nonelectronic projects', 'primarysource materials', 'direct physical access', 'historiographical approaches', 'pedagogical approaches', 'archival web', 'educational sites', 'library ofcongress', 'national archives and records administration']
['k-12 instruction', 'archival web site', 'digital access', 'direct physical access', 'archival material', 'providing k-12 school', 'archival practice', 'student learning', 'nonelectronic project', 'source material']

The archival imagination of David Bearman, revisited
Many archivists regard the archival imagination evidenced in the writings of
	David Bearman as avant-garde. Archivist L. Henry (1998) has sharply
	criticized Bearman for being irreverent toward the archival theory and
	practice outlined by classical American archivist T. R. Schellenberg.
	Although Bearman is sometimes credited (and sometimes berated) for
	establishing "a new paradigm" centered on the archival management of
	electronic records, his methods and strategies are intended to
	encompass all forms of record keeping. The article provides general
	observations on Bearman's archival imagination, lists some of its
	components, and addresses elements of Henry's critique. Although the
	long lasting impact of Bearman's imagination upon the archival
	profession might be questioned, it nonetheless deserves continued
	consideration by archivists and inclusion as a component of graduate
	archival education
['archival imagination', 'david bearman', 'archival theory', 'classical americanarchivist', 'schellenberg', 'archival management', 'electronic records', 'record keeping', 'archival profession', 'graduate archival education']
['archival imagination', 'david bearman', 'classical american archivist t. r. schellenberg', 'many archivist', 'archivist l. henry', 'bearman', 'archival theory', 'archival', 'new paradigm', 'electronic record']

Pattern recognition strategies for molecular surfaces. II. Surface
	complementarity
For pt.I see ibid., vol.23, p.1176-87 (2002). Fuzzy logic based algorithms for
	the quantitative treatment of complementarity of molecular surfaces are
	presented. Therein, the overlapping surface patches defined in part I
	of this series are used. The identification of complementary surface
	patches can be considered as a first step for the solution of molecular
	docking problems. Standard technologies can then be used for further
	optimization of the resulting complex structures. The algorithms are
	applied to 33 biomolecular complexes. After the optimization with a
	downhill simplex method, for all these complexes one structure was
	found, which is in very good agreement with the experimental results
['pattern recognition strategies', 'surface complementarity', 'fuzzy logic basedalgorithms', 'quantitative treatment', 'molecular surfaces', 'overlappingsurface', 'biomolecular complexes', 'optimization', 'downhill simplex method']
['pattern recognition strategy', 'molecular surface', 'algorithm', 'quantitative treatment', 'fuzzy logic', 'surface patch', 'complementarity', 'complementary surface', 'standard technology', 'surface']

Pattern recognition strategies for molecular surfaces. I. Pattern generation
	using fuzzy set theory
A new method for the characterization of molecules based on the model approach
	of molecular surfaces is presented. We use the topographical properties
	of the surface as well as the electrostatic potential, the local
	lipophilicity/hydrophilicity, and the hydrogen bond density on the
	surface for characterization. The definition and the calculation method
	for these properties are reviewed. The surface is segmented into
	overlapping patches with similar molecular properties. These patches
	can be used to represent the characteristic local features of the
	molecule in a way that is beyond the atomistic resolution but can
	nevertheless be applied for the analysis of partial similarities of
	different molecules as well as for the identification of molecular
	complementarity in a very general sense. The patch representation can
	be used for different applications, which will be demonstrated in
	subsequent articles
['pattern recognition strategies', 'molecular surfaces', 'pattern generation', 'fuzzyset theory', 'model approach', 'topographical properties', 'electrostaticpotential', 'local lipophilicity/hydrophilicity', 'hydrogen bond density', 'segmented surface', 'overlapping patches', 'molecular properties', 'localfeatures', 'atomistic resolution', 'partial similarities', 'molecularcomplementarity', 'patch representation', 'lipophilicity', 'hydrophilicity']
['characteristic local feature', 'fuzzy set theory', 'similar molecular property', 'hydrogen bond density', 'pattern recognition strategy', 'i. pattern generation', 'molecular surface', 'surface', 'new method', 'model approach']

An efficient parallel algorithm for the calculation of canonical MP2 energies
We present the parallel version of a previous serial algorithm for the
	efficient calculation of canonical MP2 energies. It is based on the
	Saebo-Almlof direct-integral transformation, coupled with an efficient
	prescreening of the AO integrals. The parallel algorithm avoids
	synchronization delays by spawning a second set of slaves during the
	bin-sort prior to the second half-transformation. Results are presented
	for systems with up to 2000 basis functions. MP2 energies for molecules
	with 400-500 basis functions can be routinely calculated to
	microhartree accuracy on a small number of processors (6-8) in a matter
	of minutes with modern PC-based parallel computers
['parallel algorithm', 'canonical mp2 energies', 'saebo-almlof direct-integraltransformation', 'ao integrals', 'synchronization delays', 'secondhalf-transformation', 'basis functions', 'mp2 energies', 'microhartreeaccuracy', 'pc-based parallel computers']
['canonical mp2 energy', 'previous serial algorithm', 'parallel algorithm avoid', 'efficient parallel algorithm', 'mp2 energy', 'parallel version', 'efficient calculation', 'integral transformation', 'basis function', 'almlof direct']

A method for correlations analysis of coordinates: applications for molecular
	conformations
We describe a new method to analyze multiple correlations between subsets of
	coordinates that represent a sample. The correlation is established
	only between specific regions of interest at the coordinates. First,
	the region(s) of interest are selected at each molecular coordinate.
	Next, a correlation matrix is constructed for the selected regions. The
	matrix is subject to further analysis, illuminating the
	multidimensional structural characteristics that exist in the
	conformational space. The method's abilities are demonstrated in
	several examples: it is used to analyze the conformational space of
	complex molecules, it is successfully applied to compare related
	conformational spaces, and it is used to analyze a diverse set of
	protein folding trajectories
['multiple correlation analysis', 'regions of interest', 'correlation matrix', 'molecular coordinate', 'multidimensional structural characteristics', 'complex molecules', 'conformational spaces', 'protein folding trajectories', 'molecular conformations']
['conformational space', 'multidimensional structural characteristic', 'correlation matrix', 'multiple correlation', 'specific region', 'molecular coordinate', 'coordinate', 'complex molecule', 'new method', 'correlation analysis']

Genetic algorithm guided selection: variable selection and subset selection
A novel genetic algorithm guided selection method, GAS, has been described. The
	method utilizes a simple encoding scheme which can represent both
	compounds and variables used to construct a QSAR/QSPR model. A genetic
	algorithm is then utilized to simultaneously optimize the encoded
	variables that include both descriptors and compound subsets. The GAS
	method generates multiple models each applying to a subset of the
	compounds. Typically the subsets represent clusters with different
	chemotypes. Also a procedure based on molecular similarity is presented
	to determine which model should be applied to a given test set
	compound. The variable selection method implemented in GAS has been
	tested and compared using the Selwood data set (n = 31 compounds; nu =
	53 descriptors). The results showed that the method is comparable to
	other published methods. The subset selection method implemented in GAS
	has been first tested using an artificial data set (n = 100 points; nu
	= 1 descriptor) to examine its ability to subset data points and second
	applied to analyze the XLOGP data set (n = 1831 compounds; nu = 126
	descriptors). The method is able to correctly identify artificial data
	points belonging to various subsets. The analysis of the XLOGP data set
	shows that the subset selection method can be useful in improving a
	QSAR/QSPR model when the variable selection method fails
['genetic algorithm guided selection method', 'encoding scheme', 'compounds', 'variables', 'variable selection', 'subset selection', 'qsar/qspr model', 'optimization', 'descriptors', 'compound subsets', 'multiple models', 'clusters', 'chemotypes', 'molecular similarity', 'selwood data set', 'xlogp data set', 'artificial data points']
['variable selection method', 'subset selection method', 'selwood datum set', 'simple encoding scheme', 'genetic algorithm', 'novel genetic algorithm', 'variable selection', 'qspr model', 'subset selection', 'selection method']

A formal model of computing with words
Classical automata are formal models of computing with values. Fuzzy automata
	are generalizations of classical automata where the knowledge about the
	system's next state is vague or uncertain. It is worth noting that like
	classical automata, fuzzy automata can only process strings of input
	symbols. Therefore, such fuzzy automata are still (abstract) devices
	for computing with values, although a certain vagueness or uncertainty
	are involved in the process of computation. We introduce a new kind of
	fuzzy automata whose inputs are instead strings of fuzzy subsets of the
	input alphabet. These new fuzzy automata may serve as formal models of
	computing with words. We establish an extension principle from
	computing with values to computing with words. This principle indicates
	that computing with words can be implemented with computing with values
	with the price of a big amount of extra computations
['formal model', 'computing with words', 'fuzzy automata', 'fuzzy subsets', 'inputalphabet', 'extension principle', 'pushdown automata']
['formal model', 'classical automaton', 'fuzzy automaton', 'fuzzy automaton whose input', 'word', 'new fuzzy automaton', 'value', 'certain vagueness', 'new kind', 'fuzzy subset']

Using molecular equivalence numbers to visually explore structural features
	that distinguish chemical libraries
A molecular equivalence number (meqnum) classifies a molecule with respect to a
	class of structural features or topological shapes such as its cyclic
	system or its set of functional groups. Meqnums can be used to organize
	molecular structures into nonoverlapping, yet highly relatable classes.
	We illustrate the construction of some different types of meqnums and
	present via examples some methods of comparing diverse chemical
	libraries based on meqnums. In the examples we compare a library which
	is a random sample from the MDL Drug Data Report (MDDR) with a library
	which is a random sample from the Available Chemical Directory (ACD).
	In our analyses, we discover some interesting features of the
	topological shape of a molecule and its set of functional groups that
	are strongly linked with compounds occurring in the MDDR but not in the
	ACD. We also illustrate the utility of molecular equivalence indices in
	delineating the structural domain over which an SAR conclusion is valid
['molecular equivalence number', 'molecule classification', 'structural features', 'topological shapes', 'cyclic system', 'functional groups', 'nonoverlappingrelatable classes', 'chemical libraries', 'mdl drug data report', 'availablechemical directory', 'molecular equivalence indices']
['molecular equivalence number', 'distinguish chemical library', 'mdl drug data report', 'topological shape', 'functional group', 'structural feature', 'meqnum', 'available chemical directory', 'random sample', 'molecular structure']

On the use of neural network ensembles in QSAR and QSPR
Despite their growing popularity among neural network practitioners, ensemble
	methods have not been widely adopted in structure-activity and
	structure-property correlation. Neural networks are inherently
	unstable, in that small changes in the training set and/or training
	parameters can lead to large changes in their generalization
	performance. Recent research has shown that by capitalizing on the
	diversity of the individual models, ensemble techniques can minimize
	uncertainty and produce more stable and accurate predictors. In this
	work, we present a critical assessment of the most common ensemble
	technique known as bootstrap aggregation, or bagging, as applied to
	QSAR and QSPR. Although aggregation does offer definitive advantages,
	we demonstrate that bagging may not be the best possible choice and
	that simpler techniques such as retraining with the full sample can
	often produce superior results. These findings are rationalized using
	Krogh and Vedelsby's (1995) decomposition of the generalization error
	into a term that measures the average generalization performance of the
	individual networks and a term that measures the diversity among them.
	For networks that are designed to resist over-fitting, the benefits of
	aggregation are clear but not overwhelming
['neural network ensembles', 'qsar', 'qspr', 'training set', 'training parameters', 'generalization performance', 'uncertainty', 'bootstrap aggregation', 'bagging', 'retraining', 'generalization error decomposition', 'structure-activity correlation', 'structure-property correlation']
['average generalization performance', 'neural network practitioner', 'good possible choice', 'neural network ensemble', 'neural network', 'bootstrap aggregation', 'qspr', 'common ensemble', 'ensemble technique', 'qsar']

Median partitioning: a novel method for the selection of representative subsets
	from large compound pools
A method termed median partitioning (MP) has been developed to select diverse
	sets of molecules from large compound pools. Unlike many other methods
	for subset selection, the MP approach does not depend on pairwise
	comparison of molecules and can therefore be applied to very large
	compound collections. The only time limiting step is the calculation of
	molecular descriptors for database compounds. MP employs arrays of
	property descriptors with little correlation to divide large compound
	pools into partitions from which representative molecules can be
	selected. In each of n subsequent steps, a population of molecules is
	divided into subpopulations above and below the median value of a
	property descriptor until a desired number of 2/sup n/ partitions are
	obtained. For descriptor evaluation and selection, an entropy
	formulation was embedded in a genetic algorithm. MP has been applied to
	generate a subset of the Available Chemicals Directory, and the results
	have been compared with cell-based partitioning
['median partitioning', 'large compound pools', 'representative subset selection', 'molecules', 'time limiting step', 'molecular descriptors', 'databasecompounds', 'property descriptor array', 'entropy formulation', 'geneticalgorithm', 'available chemicals directory', 'cell-based partitioning']
['large compound pool', 'median partitioning', 'large compound', 'property descriptor', 'novel method', 'representative subset', 'mp approach', 'little correlation', 'subset selection', 'partitioning']

Chemical information based scaling of molecular descriptors: a universal
	chemical scale for library design and analysis
Scaling is a difficult issue for any analysis of chemical properties or
	molecular topology when disparate descriptors are involved. To compare
	properties across different data sets, a common scale must be defined.
	Using several publicly available databases (ACD, CMC, MDDR, and NCI) as
	a basis, we propose to define chemically meaningful scales for a number
	of molecular properties and topology descriptors. These chemically
	derived scaling functions have several advantages. First, it is
	possible to define chemically relevant scales, greatly simplifying
	similarity and diversity analyses across data sets. Second, this
	approach provides a convenient method for setting descriptor boundaries
	that define chemically reasonable topology spaces. For example,
	descriptors can be scaled so that compounds with little potential for
	biological activity, bioavailability, or other drug-like
	characteristics are easily identified as outliers. We have compiled
	scaling values for 314 molecular descriptors. In addition the 10th and
	90th percentile values for each descriptor have been calculated for use
	in outlier filtering
['universal chemical scale', 'library design', 'library analysis', 'chemicalinformation based scaling', 'molecular descriptors', 'molecular topology', 'chemical properties', 'databases', 'diversity analyses', 'similarityanalyses', 'data sets', 'descriptor boundaries', 'drug-like characteristics', 'biological activity', 'bioavailability', 'outliers']
['molecular descriptor', '90th percentile value', 'reasonable topology space', 'derived scaling function', 'different data set', 'data set', 'disparate descriptor', 'common scale', 'library design', 'chemical scale']

MTD-PLS: a PLS-based variant of the MTD method. II. Mapping ligand-receptor
	interactions. Enzymatic acetic acid esters hydrolysis
The PLS variant of the MTD method (T.I. Oprea et al., SAR QSAR Environ. Res.
	2001, 12, 75-92) was applied to a series of 25 acetylcholinesterase
	hydrolysis substrates. Statistically significant MTD-PLS models (q/sup
	2/ between 0.7 and 0.8) are in agreement with previous MTD models, with
	the advantage that local contributions are understood beyond the
	occupancy/nonoccupancy interpretation in MTD. A "chemically intuitive"
	approach further forces MTD-PLS coefficients to assume only negative
	(or zero) values for fragmental volume descriptors and positive (or
	zero) values for fragmental hydrophobicity descriptors. This further
	separates the various kinds of local interactions at each vertex of the
	MTD hypermolecule, making this method suitable for medicinal chemistry
	synthesis planning
['minimum topological difference method', 'mtd-pls models', 'pls-based variant', 'ligand-receptor interactions mapping', 'enzymatic acetic acid estershydrolysis', 'acetylcholinesterase hydrolysis substrates', 'chemicallyintuitive approach', 'fragmental volume descriptors', 'fragmentalhydrophobicity descriptors', 'hypermolecule', 'medicinal chemistrysynthesis planning', 'steric misfit', 'additive approach', 'intermolecularforce categories', 'regression coefficients', 'ligand binding affinity', 'hydrogen bonding', 'polarizabilities', 'statistical model stability']
['t.i. oprea et al', 'sar qsar environ', 'mtd method', 'enzymatic acetic acid', 'previous mtd model', 'pls', 'mtd', 'mapping ligand', 'fragmental volume descriptor', 'fragmental hydrophobicity descriptor']

Prediction of ultraviolet spectral absorbance using quantitative
	structure-property relationships
High performance liquid chromatography (HPLC) with ultraviolet (UV)
	spectrophotometric detection is a common method for analyzing reaction
	products in organic chemistry. This procedure would benefit from a
	computational model for predicting the relative response of organic
	molecules. Models are now reported for the prediction of the integrated
	UV absorbance for a diverse set of organic compounds using a
	quantitative structure-property relationship (QSPR) approach. A
	seven-descriptor linear correlation with a squared correlation
	coefficient (R/sup 2/) of 0.815 is reported for a data set of
	521.compounds. Using the sum of ZINDO oscillator strengths in the
	integration range as an additional descriptor allowed reduction in the
	number of descriptors producing a robust model for 460 compounds with
	five descriptors and a squared correlation coefficient 0.857. The
	descriptors used in the models are discussed with respect to the
	physical nature of the UV absorption process
['ultraviolet spectral absorbance prediction', 'quantitative structure-propertyrelationship', 'high performance liquid chromatography', 'ultravioletspectrophotometric detection', 'reaction products', 'organic chemistry', 'computational model', 'relative response', 'seven-descriptor linearcorrelation', 'squared correlation coefficient', 'zindo oscillatorstrengths', 'combinatorial chemistry', 'generic quantitation', 'configurationinteraction calculation', 'codessa program', 'mos-f package']
['high performance liquid chromatography', 'property relationship', 'ultraviolet spectral absorbance', 'descriptor linear correlation', 'zindo oscillator strength', 'common method', 'prediction', 'organic chemistry', 'spectrophotometric detection', 'relative response']

Assessment of the macrocyclic effect for the complexation of crown-ethers with
	alkali cations using the substructural molecular fragments method
The Substructural Molecular Fragments method (Solov'ev, V. P.; Varnek, A. A.;
	Wipff, G. J. Chem. Inf. Comput. Sci. 2000, 40, 847-858) was applied to
	assess stability constants (logK) of the complexes of crown-ethers,
	polyethers, and glymes with Na/sup +/, K/sup +/, and Cs/sup +/ in
	methanol. One hundred forty-seven computational models including
	different fragment sets coupled with linear or nonlinear fitting
	equations were applied for the data sets containing 69 (Na/sup +/), 123
	(K/sup +/), and 31 (Cs/sup +/) compounds. To account for the
	"macrocyclic effect" for crown-ethers, an additional "cyclicity"
	descriptor was used. "Predicted" stability constants both for
	macrocyclic compounds and for their open-chain analogues are in good
	agreement with the experimental data reported earlier and with those
	studied experimentally in this work. The macrocyclic effect as a
	function of cation and ligand is quantitatively estimated for all
	studied crown-ethers
['substructural molecular fragments method', 'stability constants', 'complexation', 'crown-ethers', 'alkali cations', 'macrocyclic effect', 'computational models', 'different fragment sets', 'nonlinear fitting equations', 'linear fittingequations', 'cyclicity descriptor', 'open-chain analogues', 'data mining', 'structure-property tool', 'molecular graph decomposition', 'quantitativestructure-properties relationship', 'augmented atom', 'trail program', 'statistical parameters', 'thermodynamic parameters']
['substructural molecular fragments method', 'macrocyclic effect', 'substructural molecular fragment method', 'alkali cation', 'g. j. chem', 'assess stability constant', 'ether', 'crown', 'a. a.', 'v. p.']

Improving the predicting power of partial order based QSARs through linear
	extensions
Partial order theory (POT) is an attractive and operationally simple method
	that allows ordering of compounds, based on selected structural and/or
	electronic descriptors (modeled order), or based on their end points,
	e.g., solubility (experimental order). If the modeled order resembles
	the experimental order, compounds that are not experimentally
	investigated can be assigned a position in the model that eventually
	might lead to a prediction of an end-point value. However, in the
	application of POT in quantitative structure-activity relationship
	modeling, only the compounds directly comparable to the noninvestigated
	compounds are applied. To explore the possibilities of improving the
	methodology, the theory is extended by application of the so-called
	linear extensions of the model order. The study show that partial
	ordering combined with linear extensions appears as a promising tool
	providing probability distribution curves in the range of possible
	end-point values for compounds not being experimentally investigated
['quantitative structure-activity relationships', 'partial order theory', 'predictingpower improvement', 'linear extensions', 'structural descriptors', 'electronic descriptors', 'modeled order', 'end points', 'graphicalrepresentation', 'combinatorial rule', 'most probable linear order', 'partially ordered set', 'hasse diagram', 'solubilities', 'organic compounds']
['partial order theory', 'probability distribution curve', 'point value', 'experimental order', 'compound', 'activity relationship', 'partial order', 'linear extension', 'quantitative structure', 'pot']

Novel ZE-isomerism descriptors derived from molecular topology and their
	application to QSAR analysis
We introduce several series of novel ZE-isomerism descriptors derived directly
	from two-dimensional molecular topology. These descriptors make use of
	a quantity named ZE-isomerism correction, which is added to the vertex
	degrees of atoms connected by double bonds in Z and E configurations.
	This approach is similar to the one described previously for
	topological chirality descriptors (Golbraikh, A., et al. J. Chem. Inf.
	Comput. Sci. 2001, 41, 147-158). The ZE-isomerism descriptors include
	modified molecular connectivity indices, overall Zagreb indices,
	extended connectivity, overall connectivity, and topological charge
	indices. They can be either real or complex numbers. Mathematical
	properties of different subgroups of ZE-isomerism descriptors are
	discussed. These descriptors circumvent the inability of conventional
	topological indices to distinguish between Z and E isomers. The
	applicability of ZE-isomerism descriptors to QSAR analysis is
	demonstrated in the studies of a series of 131 anticancer agents
	inhibiting tubulin polymerization
['ze-isomerism descriptors', 'two-dimensional molecular topology', 'qsar analysis', 'quantitative structure-activity relationship', 'ze-isomerism correction', 'vertex degrees', 'double bond connected atoms', 'modified molecularconnectivity indices', 'overall zagreb indices', 'extended connectivity', 'overall connectivity', 'topological charge indices', 'complex numbers', 'anticancer agents', 'tubulin polymerization', 'descriptor pharmacophore', 'chemical databases', 'molecular graphs', 'computer-assisted drug design', 'toxicities', 'combinatorial chemical libraries']
['isomerism descriptor', 'novel ze', 'molecular connectivity index', 'qsar analysis', 'molecular topology', 'topological chirality descriptor', 'dimensional molecular topology', 'ze', 'isomerism correction', 'double bond']

Computer mediated communication and university international students
The design for the preliminary study presented was based on the experiences of
	the international students and faculty members of a small southwest
	university being surveyed and interviewed. The data collection
	procedure blends qualitative and quantitative data. A strong consensus
	was found that supports the study's premise that there is an
	association between the use of computer mediated communication (CMC)
	and teaching and learning performance of international students. Both
	groups believe CMC to be an effective teaching and learning tool by:
	increasing the frequency and quality of communication between students
	and instructors; improving language skills through increased writing
	and communication opportunities; allowing students and instructors to
	stay current and to compete effectively; providing alternative teaching
	and learning methods to increase students' confidence in their ability
	to communicate effectively with peers and instructors; and improving
	the instructors' pedagogical focus and questioning techniques
['computer mediated communication', 'university international students', 'facultymembers', 'small southwest university', 'data collection procedure', 'quantitative data', 'qualitative data', 'cmc', 'teaching', 'learningperformance', 'language skills', 'communication opportunities', 'instructors', 'student confidence', 'peers', 'pedagogical focus', 'questioning techniques']
['university international student', 'international student', 'communication', 'preliminary study', 'small southwest', 'faculty member', 'strong consensus', 'datum collection', 'computer', 'quantitative datum']

Uncertainty bounds and their use in the design of interval type-2 fuzzy logic
	systems
We derive inner- and outer-bound sets for the type-reduced set of an interval
	type-2 fuzzy logic system (FLS), based on a new mathematical
	interpretation of the Karnik-Mendel iterative procedure for computing
	the type-reduced set. The bound sets can not only provide estimates
	about the uncertainty contained in the output of an interval type-2
	FLS, but can also be used to design an interval type-2 FLS. We
	demonstrate, by means of a simulation experiment, that the resulting
	system can operate without type-reduction and can achieve similar
	performance to one that uses type-reduction. Therefore, our new design
	method, based on the bound sets, can relieve the computation burden of
	an interval type-2 FLS during its operation, which makes an interval
	type-2 FLS useful for real-time applications
['uncertainty bounds', 'interval type-2 fuzzy logic systems', 'inner-bound sets', 'outer-bound sets', 'type-reduced set', 'karnik-mendel iterative procedure', 'real-time applications', 'time-series forecasting']
['interval type-2 fls', 'mendel iterative procedure', 'fuzzy logic system', 'bound set', 'uncertainty', 'type-2 fls useful', 'set', 'fuzzy logic', 'interval type-2', 'new mathematical']

Entrepreneurs in Action: a Web-case model
Much of the traditional schooling in America is built around systems of
	compliance and control, characteristics which stifle the creative and
	entrepreneurial instincts of the children who are subjected to these
	tactics. The article explores a different approach to education, one
	that involves capturing the interest of the student through the use of
	problem and project-based instruction delivered via the Internet.
	Called Entrepreneurs in Action, this program seeks to involve students
	in a problem at the outset and to promote the learning of traditional
	subject areas as a process of the problem-solving activities that are
	undertaken. The program's details are explained, from elementary school
	through university level courses, and the authors outline their plans
	to test the efficacy of the program at each level
['entrepreneurs in action', 'web-case model', 'traditional schooling', 'america', 'entrepreneurial instincts', 'project-based instruction', 'internet', 'traditional subject areas', 'problem-solving activities', 'elementaryschool', 'university level courses']
['university level course', 'different approach', 'subject area', 'entrepreneurial instinct', 'action', 'traditional schooling', 'elementary school', 'case model', 'traditional', 'problem']

Factors contributing to preservice teachers' discomfort in a Web-based course
	structured as an inquiry
A report is given of a qualitative emergent design study of a Science,
	Technology, Society Interaction (STS) Web-enhanced course. Students'
	discomfort during the pilot test provided insight into the intellectual
	scaffolding that preservice secondary science teachers needed to
	optimize their performance when required to develop understanding
	through open-ended inquiry in a Web environment. Eight factors
	identified contributed to student discomfort: computer skills, paradigm
	shifts, trust, time management, thinking about their own thinking,
	systematic inquiry, self-assessment, and scientific discourse. These
	factors suggested developing understanding through inquiry by
	conducting a self-designed, open-ended, systematic inquiry required
	autonomous learning involving metacognitive skills and time management
	skills. To the extent in which students either came into the course
	with this scaffolding, or developed it during the course, they were
	successful in learning about STS and its relationship to science
	teaching. Changes in the Web site made to accommodate learners' needs
	as they surfaced are described
['preservice teacher discomfort', 'web-based course', 'qualitative emergent designstudy', 'science technology society interaction course', 'web-enhancedcourse', 'student discomfort', 'intellectual scaffolding', 'preservicesecondary science teachers', 'open-ended inquiry', 'web environment', 'computer skills', 'paradigm shifts', 'trust', 'time management', 'thinking', 'systematic inquiry', 'self-assessment', 'scientific discourse', 'autonomouslearning', 'metacognitive skills', 'time management skills', 'sts', 'scienceteaching']
['qualitative emergent design study', 'optimize -PRON- performance', 'factor', 'time management', 'secondary science teacher', 'systematic inquiry', 'web', 'discomfort', 'preservice teacher', 'scientific discourse']

Recommendations for implementing Internet inquiry projects
The purpose of the study presented was to provide recommendations to teachers
	who are interested in implementing Internet inquiry projects. Four
	classes of ninth- and tenth-grade honors students (N = 100)
	participated in an Internet inquiry project in which they were
	presented with an ecology question that required them to make a
	decision based on information that they gathered, analyzed, and
	synthesized from the Internet and their textbook. Students then
	composed papers with a rationale for their decision. Students in one
	group had access to pre-selected relevant Web sites, access to the
	entire Internet, and were provided with less online support. Students
	in the other group had access to only pre-selected relevant Web sites,
	but were provided with more online support. Two of the most important
	recommendations were: 1) to provide students with more online support;
	and 2) to provide students with pre-selected relevant Web sites and
	allow them to search the Internet for information
['internet inquiry projects', 'teachers', 'honors students', 'ecology question', 'pre-selected relevant web sites', 'online support']
['internet inquiry project', 'relevant web site', 'grade honor student', 'recommendation', 'student', 'online support', 'decision', 'access', 'ecology question', 'pre']

Alien Rescue: a problem-based hypermedia learning environment for middle school
	science
The article describes an innovative hypermedia product for sixth graders in
	space science: Alien Rescue. Using a problem-based learning approach
	that is highly interactive, Alien Rescue engages students in scientific
	investigations aimed at finding solutions to complex and meaningful
	problems. Problem-based learning (PBL) is an instructional strategy
	proven to be effective in medical and business fields, and it is
	increasingly popular in education. However, using PBL in K-12
	classrooms is challenging and requires access to rich knowledge bases
	and cognitive tools. Alien Rescue is designed to provide such cognitive
	support for successful use of PBL in sixth-grade classrooms. The design
	and development of Alien Rescue is guided by current educational
	research. Research is an integral part of this project. Results of
	formative evaluation and research studies are being integrated into the
	development and improvement of the program. Alien Rescue is designed in
	accordance with the National Science Standards and the Texas Essential
	Knowledge and Skills (TEKS) for science. So far Alien Rescue has been
	field-tested by approximately 1400 sixth graders. More use in middle
	schools is in progress and more research on its use is planned
['alien rescue', 'problem-based hypermedia learning environment', 'middle schoolscience', 'space science', 'sixth graders', 'scientific investigations', 'pbl', 'instructional strategy', 'medical fields', 'business fields', 'k-12classrooms', 'rich knowledge bases', 'cognitive tools', 'cognitive support', 'educational research', 'formative evaluation', 'middle schools']
['alien rescue', 'national science standards', 'rich knowledge base', 'innovative hypermedia product', 'hypermedia learning environment', 'middle school', 'sixth grader', 'space science', 'instructional strategy', 'problem']

Project-based learning: teachers learning and using high-tech to preserve Cajun
	culture
Using project-based learning pedagogy in EdTc 658 Advances in Educational
	Technology, the author has trained inservice teachers in Southwestern
	Louisiana with an advanced computer multimedia program called
	Director(R) (Macromedia, Inc.). The content of this course focused on
	modeling the project-based learning pedagogy and researching Acadian's
	traditions and legacy. With the multi-functions of microcomputers, new
	technologies were used to preserve and celebrate the local culture with
	superiority of text, graphics, animation, sound, and video. The article
	describes how several groups of school teachers in the surrounding
	areas of a regional state university of Louisiana learned computer
	multimedia using project-based learning and integrated their learning
	into local cultural heritage
['project-based learning', 'teachers', 'cajun culture', 'project-based learningpedagogy', 'edtc 658 advances in educational technology', 'inserviceteachers', 'advanced computer multimedia program', 'director', 'acadiantraditions', 'macromedia', 'new technologies', 'local culture', 'schoolteachers', 'regional state university', 'computer multimedia', 'localcultural heritage']
['project', 'advanced computer multimedia program', 'learning', 'pedagogy', 'teacher', 'technology', 'inservice teacher', 'louisiana', 'high', 'culture']

Presentation media, information complexity, and learning outcomes
Multimedia computing provides a variety of information presentation modality
	combinations. Educators have observed that visuals enhance learning
	which suggests that multimedia presentations should be superior to
	text-only and text with static pictures in facilitating optimal human
	information processing and, therefore, comprehension. The article
	reports the findings from a 3 (text-only, overhead slides, and
	multimedia presentation)*2 (high and low information complexity)
	factorial experiment. Subjects read a text script, viewed an acetate
	overhead slide presentation, or viewed a multimedia presentation
	depicting the greenhouse effect (low complexity) or photocopier
	operation (high complexity). Multimedia was superior to text-only and
	overhead slides for comprehension. Information complexity diminished
	comprehension and perceived presentation quality. Multimedia was able
	to reduce the negative impact of information complexity on
	comprehension and increase the extent of sustained attention to the
	presentation. These findings suggest that multimedia presentations
	invoke the use of both the verbal and visual working memory channels
	resulting in a reduction of the cognitive load imposed by increased
	information complexity. Moreover, multimedia superiority in
	facilitating comprehension goes beyond its ability to increase
	sustained attention; the quality and effectiveness of information
	processing attained (i.e., use of verbal and visual working memory) is
	also significant
['presentation media', 'information complexity', 'learning outcomes', 'cognitiveprocessing limitations', 'human working memory', 'verbal working memorychannel', 'visual working memory channel', 'multimedia computing', 'information presentation modality combinations', 'educators', 'multimediapresentations', 'static pictures', 'optimal human information processing', 'overhead slides', 'text script', 'acetate overhead slide presentation', 'multimedia presentation', 'greenhouse effect', 'photocopier operation', 'cognitive load', 'multimedia superiority', 'sustained attention']
['information complexity', 'visual working memory channel', 'overhead slide presentation', 'multimedia presentation', 'low information complexity', 'information presentation modality', 'visual working memory', 'multimedia computing', 'overhead slide', 'information processing']

Real-time tissue characterization on the basis of in vivo Raman spectra
The application of in vivo Raman spectroscopy for clinical diagnosis demands
	dedicated software that can perform the necessary signal processing and
	subsequent (multivariate) data analysis, enabling clinically relevant
	parameters to be extracted and made available in real time. Here we
	describe the design and implementation of a software package that
	allows for real-time signal processing and data analysis of Raman
	spectra. The design is based on automatic data exchange between Grams,
	a spectroscopic data acquisition and analysis program, and Matlab, a
	program designed for array-based calculations. The data analysis
	software has a modular design providing great flexibility in developing
	custom data analysis routines for different applications. The
	implementation is illustrated by a computationally demanding
	application for the classification of skin spectra using principal
	component analysis and linear discriminant analysis
['real-time tissue characterization', 'clinically relevant parameters extraction', 'array-based calculations', 'computationally demanding application', 'modular design', 'data analysis software', 'clinical diagnosis', 'dedicatedsoftware', 'multivariate data analysis', 'automatic data exchange', 'grams', 'matlab', 'linear discriminant analysis', 'skin spectra classification']
['custom datum analysis routine', 'spectroscopic datum acquisition', 'necessary signal processing', 'automatic datum exchange', 'vivo raman spectroscopy', 'time signal processing', 'vivo raman spectra', 'time tissue characterization', 'real', 'datum analysis']

Loudspeaker voice-coil inductance losses: circuit models, parameter estimation,
	and effect on frequency response
When the series resistance is separated and treated as a separate element, it
	is shown that losses in an inductor require the ratio of the flux to
	MMF in the core to be frequency dependent. For small-signal operation,
	this dependence leads to a circuit model composed of a lossless
	inductor and a resistor in parallel, both of which are frequency
	dependent. Mathematical expressions for these elements are derived
	under the assumption that the ratio of core flux to MMF varies as omega
	/sup n-1/, where n is a constant. A linear regression technique is
	described for extracting the model parameters from measured data.
	Experimental data are presented to justify the model for the lossy
	inductance of a loudspeaker voice-coil. A SPICE example is presented to
	illustrate the effects of voice-coil inductor losses on the frequency
	response of a typical driver
['loudspeaker voice-coil inductance losses', 'circuit models', 'parameter estimation', 'frequency response', 'series resistance', 'small-signal operation', 'linearregression', 'lossy inductance', 'spice', 'loudspeaker driver', 'losslessinductor', 'core flux to mmf ratio']
['loudspeaker voice', 'circuit model', 'coil inductance loss', 'coil inductor loss', 'linear regression technique', 'parameter estimation', 'frequency response', 'series resistance', 'separate element', 'frequency dependent']

Complexity transitions in global algorithms for sparse linear systems over
	finite fields
We study the computational complexity of a very basic problem, namely that of
	finding solutions to a very large set of random linear equations in a
	finite Galois field modulo q. Using tools from statistical mechanics we
	are able to identify phase transitions in the structure of the solution
	space and to connect them to the changes in the performance of a global
	algorithm, namely Gaussian elimination. Crossing phase boundaries
	produces a dramatic increase in memory and CPU requirements necessary
	for the algorithms. In turn, this causes the saturation of the upper
	bounds for the running time. We illustrate the results on the specific
	problem of integer factorization, which is of central interest for
	deciphering messages encrypted with the RSA cryptosystem
['complexity transitions', 'global algorithms', 'sparse linear systems', 'finitefields', 'random linear equations', 'finite galois field', 'statisticalmechanics', 'gaussian elimination', 'phase boundaries', 'integerfactorization', 'message deciphering', 'encryption', 'rsa cryptosystem', 'disordered systems']
['finite galois field modulo q.', 'sparse linear system', 'random linear equation', 'cpu requirement necessary', 'crossing phase boundary', 'complexity transition', 'global algorithm', 'large set', 'basic problem', 'finding solution']

Noise effect on memory recall in dynamical neural network model of hippocampus
We investigate some noise effect on a neural network model proposed by Araki
	and Aihara (1998) for the memory recall of dynamical patterns in the
	hippocampus and the entorhinal cortex; the noise effect is important
	since the release of transmitters at synaptic clefts, the operation of
	gate of ion channels and so on are known as stochastic phenomena. We
	consider two kinds of noise effect due to a deterministic noise and a
	stochastic noise. By numerical simulations, we find that reasonable
	values of noise give better performance on the memory recall of
	dynamical patterns. Furthermore we investigate the effect of the
	strength of external inputs on the memory recall
['hippocampus', 'dynamical neural network model', 'noise effect', 'memory recall', 'dynamical patterns', 'entorhinal cortex', 'synaptic clefts', 'gate of ionchannels', 'stochastic phenomena', 'deterministic noise', 'stochastic noise', 'brain functions', 'numerical simulations', 'synaptic strength', 'inhibitoryconnection']
['noise effect', 'memory recall', 'dynamical neural network model', 'neural network model', 'dynamical pattern', 'entorhinal cortex', 'synaptic cleft', 'ion channel', 'noise', 'stochastic phenomenon']

Fuzzy polynomial neural networks: hybrid architectures of fuzzy modeling
We introduce a concept of fuzzy polynomial neural networks (FPNNs), a hybrid
	modeling architecture combining polynomial neural networks (PNNs) and
	fuzzy neural networks (FNNs). The development of the FPNNs dwells on
	the technologies of computational intelligence (CI), namely fuzzy sets,
	neural networks, and genetic algorithms. The structure of the FPNN
	results from a synergistic usage of FNN and PNN. FNNs contribute to the
	formation of the premise part of the rule-based structure of the FPNN.
	The consequence part of the FPNN is designed using PNNs. The structure
	of the PNN is not fixed in advance as it usually takes place in the
	case of conventional neural networks, but becomes organized dynamically
	to meet the required approximation error. We exploit a group method of
	data handling (GMDH) to produce this dynamic topology of the network.
	The performance of the FPNN is quantified through experimentation that
	exploits standard data already used in fuzzy modeling. The obtained
	experimental results reveal that the proposed networks exhibit high
	accuracy and generalization capabilities in comparison to other similar
	fuzzy models
['fuzzy polynomial neural networks', 'hybrid architectures', 'fuzzy modeling', 'highlynonlinear rule-based models', 'computational intelligence', 'fuzzy sets', 'genetic algorithms', 'group method of data handling', 'gmdh', 'dynamictopology', 'fuzzy inference method', 'learning', 'standard backpropagation', 'membership functions', 'learning rates', 'momentum coefficients', 'geneticoptimization']
['fuzzy polynomial neural network', 'fuzzy modeling', 'polynomial neural network', 'fuzzy neural network', 'fpnn', 'hybrid architecture', 'genetic algorithm', 'fuzzy set', 'computational intelligence', 'synergistic usage']

MEMS applications in computer disk drive dual-stage servo systems
We present a decoupled discrete time pole placement design method, which can be
	combined with a self-tuning scheme to compensate variations in the
	microactuator's (MA's) resonance mode. Section I of the paper describes
	the design and fabrication of a prototype microactuator with an
	integrated gimbal structure. Section II presents a decoupled
	track-following controller design and a self-tuning control scheme to
	compensate for the MA's resonance mode variations
['computer disk drive dual-stage servo systems', 'mems', 'microactuator', 'servocontrol', 'hard disk drives', 'decoupled discrete time pole placementdesign method', 'self-tuning scheme', 'electrostatic design', 'fabricationprocess', 'track-following controller design']
['discrete time pole placement design method', 'stage servo system', 'computer disk', 'mems application', 'resonance mode', 'self', 'section ii', 'ma', 'prototype microactuator', 'gimbal structure']

Nuclear magnetic resonance molecular photography
A procedure is described for storing a two-dimensional (2D) pattern consisting
	of 32*32=1024 bits in a spin state of a molecular system and then
	retrieving the stored information as a stack of nuclear magnetic
	resonance spectra. The system used is a nematic liquid crystal, the
	protons of which act as spin clusters with strong intramolecular
	interactions. The technique used is a programmable multifrequency
	irradiation with low amplitude. When it is applied to the liquid
	crystal, a large number of coherent long-lived /sup 1/H response
	signals can be excited, resulting in a spectrum showing many sharp
	peaks with controllable frequencies and amplitudes. The spectral
	resolution is enhanced by using a second weak pulse with a 90 degrees
	phase shift, so that the 1024 bits of information can be retrieved as a
	set of well-resolved pseudo-2D spectra reproducing the input pattern
['nmr molecular photography', '2d pattern', 'molecular system spin state', 'informationstorage', 'nematic liquid crystal', 'spin clusters', 'strong intramolecularinteractions', 'programmable multifrequency irradiation', 'low amplitude', 'coherent long-lived /sup 1/h response signals', 'spectral resolution', 'second weak pulse', 'pseudo-2d spectra', 'spin echoes', 'hilbert spaces', 'high-content molecular information processing', 'coupled spins', 'dipole-dipole interactions', 'spin-locking', 'proton spin', 'spin dynamics', '1024 bit']
['nuclear magnetic resonance molecular photography', 'nematic liquid crystal', 'bit', 'molecular system', 'information', 'resonance spectra', 'spin state', 'low amplitude', 'strong intramolecular', 'programmable multifrequency']

Novel active noise-reducing headset using earshell vibration control
Active noise-reducing (ANR) headsets are available commercially in applications
	varying from aviation communication to consumer audio. Current ANR
	systems use passive attenuation at high frequencies and
	loudspeaker-based active noise control at low frequencies to achieve
	broadband noise reduction. This paper presents a novel ANR headset in
	which the external noise transmitted to the user's ear via earshell
	vibration is reduced by controlling the vibration of the earshell using
	force actuators acting against an inertial mass or the earshell
	headband. Model-based theoretical analysis using velocity feedback
	control showed that current piezoelectric actuators provide sufficient
	force but require lower stiffness for improved low-frequency
	performance. Control simulations based on experimental data from a
	laboratory headset showed that good performance can potentially be
	achieved in practice by a robust feedback controller, while a
	single-frequency real-time control experiment verified that noise
	reduction can be achieved using earshell vibration control
['active noise-reducing headset', 'earshell vibration control', 'aviationcommunication', 'consumer audio', 'passive attenuation', 'broadband noisereduction', 'external noise transmission', 'force actuators', 'inertial mass', 'velocity feedback control', 'piezoelectric actuators', 'stiffness', 'robustfeedback controller', 'single-frequency real-time control']
['earshell vibration control', 'novel anr headset', 'broadband noise reduction', 'active noise control', 'current piezoelectric actuator', 'novel active noise', 'active noise', 'robust feedback controller', 'time control experiment', 'high frequency']

Theoretical and experimental investigations on coherence of traffic noise
	transmission through an open window into a rectangular room in
	high-rise buildings
A method for theoretically calculating the coherence between sound pressure
	inside a rectangular room in a high-rise building and that outside the
	open window of the room is proposed. The traffic noise transmitted into
	a room is generally dominated by low-frequency components, to which
	active noise control (ANC) technology may find an application. However,
	good coherence between reference and error signals is essential for an
	effective noise reduction and should be checked first. Based on traffic
	noise prediction methods, wave theory, and mode coupling theory, the
	results of this paper enabled one to determine the potentials and
	limitations of ANC used to reduce such a transmission. Experimental
	coherence results are shown for two similar, empty rectangular rooms
	located on the 17th and 30th floors of a 34 floor high-rise building.
	The calculated results with the proposed method are generally in good
	agreement with the experimental results and demonstrate the usefulness
	of the method for predicting the coherence
['traffic noise transmission', 'open window', 'rectangular room', 'high-rise buildings', 'sound pressure', 'low-frequency components', 'active noise controltechnology', 'traffic noise prediction methods', 'mode coupling theory', 'wave theory']
['rectangular room', 'traffic noise', 'rise building', 'open window', 'mode coupling theory', 'noise prediction method', 'sound pressure', 'effective noise reduction', 'active noise control', 'coherence']

High-density remote storage: the Ohio State University Libraries depository
The article describes a high-density off-site book storage facility operated by
	the Ohio State University Libraries. Opened in 1995, it has the
	capacity to house nearly 1.5 million items in only 9000 square feet by
	shelving books by size on 30-foot tall shelving. A sophisticated
	climate control system extends the life of stored materials up to 12
	times. An online catalog record for each item informs patrons that the
	item is located in a remote location. Regular courier deliveries from
	the storage facility bring requested materials to patrons with minimal
	delay
['high-density remote storage', 'ohio state university libraries', 'high-densityoff-site book storage facility', 'shelving', 'climate control system', 'stored materials', 'patrons', 'circulation', 'online catalog record', 'remotelocation', 'courier deliveries']
['ohio state university libraries depository', 'ohio state university libraries', 'site book storage facility', 'density remote storage', 'item', 'climate control system', 'online catalog record', 'tall shelving', 'shelving book', 'high']

Hours of operation and service in academic libraries: toward a national
	standard
In an effort toward establishing a standard for academic library hours, the
	article surveys and compares hours of operation and service for ARL
	libraries and IPEDS survey respondents. The article ranks the ARL
	(Association for Research Libraries) libraries according to hours of
	operation and reference hours and then briefly discusses such issues as
	libraries offering twenty-four access and factors affecting service
	hour decisions
['academic library hours', 'operation/service hours', 'arl libraries', 'ipeds surveyrespondents', 'integrated post secondary education data system', 'association for research libraries']
['ipeds survey respondent', 'academic library hour', 'operation', 'service', 'hour', 'academic library', 'standard', 'article survey', 'library', 'national']

Using the Web to answer legal reference questions
In an effort to help non-law librarians with basic legal reference questions,
	the author highlights three basic legal Web sites and outlines useful
	subject-specific Web sites that focus on statutes and regulations, case
	law and attorney directories
['world wide web', 'legal reference questions', 'nonlaw librarians', 'case law', 'attorney directories']
['basic legal web site', 'legal reference question', 'basic legal reference question', 'law librarian', 'specific web site', 'law', 'outline useful', 'author', 'non', 'subject']

The service side of systems librarianship
Describes the role of a systems librarian at a small academic library. Although
	online catalogs and the Internet are making library accessibility more
	convenient, the need for library buildings and professionals has not
	diminished. Typical duties of a systems librarian and the effects of
	new technology on librarianship are discussed. Services provided to
	other constituencies on campus and the blurring relationship between
	the library and computer services are also presented
['systems librarianship', 'service side', 'small academic library', 'online catalogs', 'internet']
['small academic library', 'system librarian', 'service', 'online catalog', 'library accessibility', 'library building', 'system librarianship', 'typical duty', 'new technology', 'librarianship']

Defining electronic librarianship: a content analysis of job advertisements
Advances in technology create dramatic changes within libraries. The complex
	issues surrounding this new electronic, end-user environment have major
	ramifications and require expert knowledge. Electronic services
	librarians and electronic resources librarians are two specialized
	titles that have recently emerged within the field of librarianship to
	fill this niche. Job advertisements listed in American Libraries from
	January 1989 to December 1998 were examined to identify
	responsibilities, qualifications, organizational and salary information
	relating to the newly emerging role of electronic librarian
['electronic librarianship', 'content analysis', 'job advertisements', 'electronicend-user environment', 'electronic resources librarians', 'electronicservices librarians', 'american libraries', 'responsibilities', 'qualifications', 'organizational information', 'salary information']
['job advertisement', 'electronic resource librarian', 'dramatic change', 'user environment', 'new electronic', 'electronic service', 'expert knowledge', 'content analysis', 'electronic librarianship', 'american libraries']

Customer in-reach and library strategic systems: the case of ILLiad
Libraries have walls. Recognizing this fact, the Interlibrary Loan Department
	at Virginia Tech is creating systems and services that enable our
	customers to reach past our walls at anytime from anywhere. Customer
	in-reach enables Virginia Tech faculty, students, and staff anywhere in
	the world to obtain information and services heretofore available only
	to our on-campus customers. ILLiad, Virginia Tech's interlibrary
	borrowing system, is the library strategic system that attains this
	goal. The principles that guided development of ILLiad are widely
	applicable
['library strategic systems', 'interlibrary loan department', 'virginia tech', 'customer in-reach', 'illiad', 'interlibrary borrowing system']
['library strategic system', 'interlibrary loan department', 'virginia tech faculty', 'customer', 'illiad', 'virginia tech', 'reach', 'wall', 'service', 'libraries']

NuVox shows staying power with new cash, new market
Who says you can't raise cash in today's telecom market? NuVox Communications
	positions itself for the long run with $78.5 million in funding and a
	new credit facility
['telecom', 'competitive carrier market', 'nuvox communications', 'investors']
['new credit facility', 'long run', 'nuvox communications', 'new market', 'telecom market', 'new cash', 'nuvox', 'cash', 'power', 'who']

Improvements and critique on Sugeno's and Yasukawa's qualitative modeling
Investigates Sugeno's and Yasukawa's (1993) qualitative fuzzy modeling
	approach. We propose some easily implementable solutions for the
	unclear details of the original paper, such as trapezoid approximation
	of membership functions, rule creation from sample data points, and
	selection of important variables. We further suggest an improved
	parameter identification algorithm to be applied instead of the
	original one. These details are crucial concerning the method's
	performance as it is shown in a comparative analysis and helps to
	improve the accuracy of the built-up model. Finally, we propose a
	possible further rule base reduction which can be applied successfully
	in certain cases. This improvement reduces the time requirement of the
	method by up to 16% in our experiments
['qualitative modeling', 'fuzzy modeling', 'trapezoid approximation', 'membershipfunctions', 'rule creation', 'parameter identification algorithm', 'rule basereduction', 'sugeno-yasukawa method']
['possible further rule base reduction', 'parameter identification algorithm', 'sample datum point', 'qualitative fuzzy modeling', 'rule creation', 'improvement', 'important variable', 'membership function', 'investigates sugeno', 'trapezoid approximation']

The plot thins: thin-client computer systems and academic libraries
The few libraries that have tried thin client architectures have noted a number
	of compelling reasons to do so. For starters, thin client devices are
	far less expensive than most PCs. More importantly, thin client
	computing devices are believed to be far less expensive to manage and
	support than traditional PCs
['academic libraries', 'thin-client computer systems']
['thin client device', 'thin client architecture', 'client computer system', 'thin client', 'compelling reason', 'academic library', 'plot thin', 'computing device', 'expensive', 'pc']

Academic libraries and community: making the connection
I explore the theme of academic libraries serving and reaching out to the
	broader community. I highlight interesting projects reported on in the
	literature (such as the Through Our Parents' Eyes project) and report
	on others. I look at challenges to community partnerships and
	recommendations for making them succeed. Although I focus on links with
	the broader community, I also took at methods for increasing
	cooperation among various units on campus, so that the needs of campus
	community groups-such as distance education students or disabled
	students-are effectively addressed. Though academic libraries are my
	focus, we can learn a lot from the community building efforts of public
	libraries
['academic libraries', 'community partnerships', 'campus community groups', 'distanceeducation students', 'disabled students', 'public libraries']
['academic library', 'broad community', 'community building effort', 'interesting project', 'distance education student', 'through -PRON- parent', 'eye project', 'community partnership', 'library', 'community group']

Using Internet search engines to estimate word frequency
The present research investigated Internet search engines as a rapid,
	cost-effective alternative for estimating word frequencies. Frequency
	estimates for 382 words were obtained and compared across four methods:
	(1) Internet search engines, (2) the Kucera and Francis (1967) analysis
	of a traditional linguistic corpus, (3) the CELEX English linguistic
	database (Baayen et al., 1995), and (4) participant ratings of
	familiarity. The results showed that Internet search engines produced
	frequency estimates that were highly consistent with those reported by
	Kucera and Francis and those calculated from CELEX, highly consistent
	across search engines, and very reliable over a 6 month period of time.
	Additional results suggested that Internet search engines are an
	excellent option when traditional word frequency analyses do not
	contain the necessary data (e.g., estimates for forenames and slang).
	In contrast, participants' familiarity judgments did not correspond
	well with the more objective estimates of word frequency. Researchers
	are advised to use search engines with large databases (e.g.,
	AltaVista) to ensure the greatest representativeness of the frequency
	estimates
['internet search engines', 'word frequency estimation', 'linguistic corpus', 'celexenglish linguistic database', 'participant familiarity ratings', 'largedatabases']
['internet search engine', 'traditional linguistic corpus', 'celex english linguistic', 'baayen et al', 'search engine', 'traditional word frequency analysis', 'word frequency', 'present research', 'effective alternative', 'estimate']

Visual-word identification thresholds for the 260 fragmented words of the
	Snodgrass and Vanderwart pictures in Spanish
Word difficulty varies from language to language; therefore, normative data of
	verbal stimuli cannot be imported directly from another language. We
	present mean identification thresholds for the 260 screen-fragmented
	words corresponding to the total set of Snodgrass and Vanderwart (1980)
	pictures. Individual words were fragmented in eight levels using Turbo
	Pascal, and the resulting program was implemented on a PC
	microcomputer. The words were presented individually to a group of 40
	Spanish observers, using a controlled time procedure. An unspecific
	learning effect was found showing that performance improved due to
	practice with the task. Finally, of the 11 psycholinguistic variables
	that previous researchers have shown to affect word identification,
	only imagery accounted for a significant amount of variance in the
	threshold values
['visual-word identification thresholds', 'fragmented words', 'snodgrass andvanderwart pictures', 'spanish', 'word difficulty', 'verbal stimuli', 'meanidentification thresholds', 'screen-fragmented words', 'turbo pascal', 'pcmicrocomputer', 'controlled time procedure', 'unspecific learning effect', 'psycholinguistic variables', 'word identification']
['present mean identification threshold', 'word identification threshold', 'word identification', 'fragmented word', 'word difficulty', 'verbal stimulus', 'vanderwart picture', 'normative datum', 'total set', 'word']

A Web-accessible database of characteristics of the 1,945 basic Japanese kanji
In 1981, the Japanese government published a list of the 1,945 basic Japanese
	kanji (Jooyoo Kanji-hyo), including specifications of pronunciation.
	This list was established as the standard for kanji usage in print. The
	database for 1,945 basic Japanese kanji provides 30 cells that explain
	in detail the various characteristics of kanji. Means, standard
	deviations, distributions, and information related to previous research
	concerning these kanji are provided in this paper. The database is
	saved as a Microsoft Excel 2000 file for Windows. This kanji database
	is accessible on the Web site of the Oxford Text Archive, Oxford
	University (http://ota.ahds.ac.uk). Using this database, researchers
	and educators will be able to conduct planned experiments and organize
	classroom instruction on the basis of the known characteristics of
	selected kanji
['web-accessible database', 'basic japanese kanji', 'jooyoo kanji-hyo', 'pronunciation', 'kanji usage print', 'cells', 'means', 'standard deviations', 'distributions', 'microsoft excel 2000 file for windows', 'oxford text archive web site', 'classroom instruction']
['basic japanese', 'japanese government', 'jooyoo kanji', 'oxford text archive', 'database', 'characteristic', 'web site', 'accessible database', 'microsoft excel', 'previous research']

Full-screen ultrafast video modes over-clocked by simple VESA routines and
	registers reprogramming under MS-DOS
Fast full-screen presentation of stimuli is necessary in psychological
	research. Although Spitczok von Brisinski (1994) introduced a method
	that achieved ultrafast display by reprogramming the registers, he
	could not produce an acceptable full-screen display. In this report,
	the author introduces a new method combining VESA routine calling with
	register reprogramming that can yield a display at 640 * 480
	resolution, with a refresh rate of about 150 Hz
['full-screen ultrafast video modes', 'fast full-screen stimuli presentation', 'psychological research', 'vesa routine calling', 'ms-dos', 'registerreprogramming']
['screen ultrafast video mode', 'simple vesa routine', 'spitczok von brisinski', 'vesa routine', 'screen presentation', 'register', 'ultrafast display', 'screen display', 'new method', 'method']

Measuring keyboard response delays by comparing keyboard and joystick inputs
The response characteristics of PC keyboards have to be identified when they
	are used as response devices in psychological experiments. In the past,
	the proposed method has been to check the characteristics independently
	by means of external measurement equipment. However, with the
	availability of different PC models and the rapid pace of model change,
	there is an urgent need for the development of convenient and accurate
	methods of checking. The method proposed consists of raising the
	precision of the PC's clock to the microsecond level and using a
	joystick connected to the MIDI terminal of a sound board to give the PC
	an independent timing function. Statistical processing of the data
	provided by this method makes it possible to estimate accurately the
	keyboard scanning interval time and the average keyboard delay time.
	The results showed that measured keyboard delay times varied from 11 to
	73 msec, depending on the keyboard model, with most values being less
	than 30 msec
['keyboard response delay measurement', 'joystick inputs', 'keyboard inputs', 'pckeyboards', 'psychological experiments', 'model change', 'checking', 'pc clockprecision', 'midi terminal', 'sound board', 'independent timing function', 'statistical data processing', 'keyboard scanning interval time', 'averagekeyboard delay time']
['keyboard delay time varied', 'different pc model', 'average keyboard delay time', 'keyboard response delay', 'external measurement equipment', 'keyboard scanning interval time', 'independent timing function', 'proposed method', 'psychological experiment', 'pc keyboard']

Computer program to generate operant schedules
A computer program for programming schedules of reinforcement is described.
	Students can use the program to experience schedules of reinforcement
	that are typically used with nonhuman subjects. Accumulative recording
	of a student's response can be shown on the screen and/or printed with
	the computer's printer. The program can also be used to program operant
	schedules for animal subjects. The program was tested with human
	subjects experiencing fixed ratio, variable ratio, fixed interval, and
	variable interval schedules. Performance for human subjects on a given
	schedule was similar to performance for nonhuman subjects on the same
	schedule
['operant schedule generation', 'computer program', 'reinforcement scheduleprogramming', 'nonhuman subjects', 'cumulative student response recording', 'animal subjects', 'fixed ratio schedules', 'variable ratio schedules', 'fixedinterval schedules', 'variable interval schedules', 'human subjects']
['nonhuman subject', 'computer program', 'animal subject', 'operant schedule', 'variable interval schedule', 'programming schedule', 'variable ratio', 'schedule', 'computer', 'program']

On-line Homework/Quiz/Exam applet: freely available Java software for
	evaluating performance on line
The Homework/Quiz/Exam applet is a freely available Java program that can be
	used to evaluate student performance on line for any content authored
	by a teacher. It has database connectivity so that student scores are
	automatically recorded. It allows several different types of questions.
	Each question can be linked to images and detailed story problems.
	Three levels of feedback are provided to student responses. It allows
	teachers to randomize the sequence of questions and to randomize which
	of several options is the correct answer in multiple-choice questions.
	The creation and editing of questions involves menu selections, button
	presses, and the typing of content; no programming knowledge is
	required. The code is open source in order to encourage modifications
	that will meet individual pedagogical needs
['online homework/quiz/exam applet', 'freely available java software', 'onlinestudent performance evaluation', 'teacher authored content', 'databaseconnectivity', 'automatic student score recording', 'images', 'detailed storyproblems', 'feedback', 'randomized question sequence', 'multiple-choicequestions', 'question editing', 'question creation', 'menu selections', 'buttonpresses', 'typing content', 'individual pedagogical needs']
['detailed story problem', 'individual pedagogical need', 'available java program', 'available java software', 'exam applet', 'line homework', 'different type', 'student score', 'question', 'student performance']

WEXTOR: a Web-based tool for generating and visualizing experimental designs
	and procedures
WEXTOR is a Javascript-based experiment generator and teaching tool on the
	World Wide Web that can be used to design laboratory and Web
	experiments in a guided step-by-step process. It dynamically creates
	the customized Web pages and Javascripts needed for the experimental
	procedure and provides experimenters with a print-ready visual display
	of their experimental design. WEXTOR flexibly supports complete and
	incomplete factorial designs with between-subjects, within-subjects,
	and quasi-experimental factors, as well as mixed designs. The software
	implements client-side response time measurement and contains a content
	wizard for creating interactive materials, as well as dependent
	measures (graphical scales, multiple-choice items, etc.), on the
	experiment pages. However, it does not aim to replace a full-fledged
	HTML editor. Several methodological features specifically needed in Web
	experimental design have been implemented in the Web-based tool and are
	described in this paper. WEXTOR is platform independent. The created
	Web pages can be uploaded to any type of Web server in which data may
	be recorded in logfiles or via a database. The current version of
	WEXTOR is freely available for educational and noncommercial purposes.
	Its Web address is http://www.genpsylab.unizh.ch/wextor/index.html
['wextor', 'web-based tool', 'experimental design visualization', 'javascript-basedexperiment generator', 'teaching tool', 'world wide web', 'customized webpages', 'print-ready visual display', 'factorial designs', 'client-sideresponse time measurement', 'content wizard', 'html', 'web server', 'logfiles', 'database', 'free software']
['experimental design', 'incomplete factorial design', 'ready visual display', 'world wide web', 'web page', 'wextor', 'teaching tool', 'experiment generator', 'step process', 'web']

Adaptive neural/fuzzy control for interpolated nonlinear systems
Adaptive control for nonlinear time-varying systems is of both theoretical and
	practical importance. We propose an adaptive control methodology for a
	class of nonlinear systems with a time-varying structure. This class of
	systems is composed of interpolations of nonlinear subsystems which are
	input-output feedback linearizable. Both indirect and direct adaptive
	control methods are developed, where the spatially localized models (in
	the form of Takagi-Sugeno fuzzy systems or radial basis function neural
	networks) are used as online approximators to learn the unknown
	dynamics of the system. Without assumptions on rate of change of system
	dynamics, the proposed adaptive control methods guarantee that all
	internal signals of the system are bounded and the tracking error is
	asymptotically stable. The performance of the adaptive controller is
	demonstrated using a jet engine control problem
['adaptive neural/fuzzy control', 'interpolated nonlinear systems', 'time-varyingsystems', 'input-output feedback linearizable systems', 'indirect control', 'direct control', 'spatially localized models', 'takagi-sugeno fuzzysystems', 'radial basis function neural networks', 'online approximators', 'unknown dynamics', 'tracking error', 'jet engine control', 'stabilityanalysis']
['radial basis function neural', 'jet engine control problem', 'nonlinear system', 'output feedback linearizable', 'sugeno fuzzy system', 'adaptive control methodology', 'nonlinear time', 'practical importance', 'adaptive neural', 'fuzzy control']

ePsych: interactive demonstrations and experiments in psychology
ePsych (http://epsych.msstate.edu), a new Web site currently under active
	development, is intended to teach students about the discipline of
	psychology. The site presumes little prior knowledge about the field
	and so may be used in introductory classes, but it incorporates
	sufficient depth of coverage to be useful in more advanced classes as
	well. Numerous interactive and dynamic elements are incorporated into
	various modules, orientations, and guidebooks. These elements include
	Java-based experiments and demonstrations, video clips, and animated
	diagrams. Rapid access to all material is provided through a
	layer-based navigation system that allows users to visit various
	"Worlds of the Mind." Active learning is encouraged, by challenging
	students with puzzles and problems and by providing the opportunity to
	"dig deeper" to learn more about the phenomena at hand
['epsych', 'interactive demonstrations', 'psychology experiments', 'web site', 'teaching', 'java-based experiments', 'video clips', 'animated diagrams', 'layer-basednavigation system', 'worlds of the mind', 'active learning']
['little prior knowledge', 'new web site', 'interactive demonstration', 'experiment', 'dynamic element', 'psychology', 'numerous interactive', 'student', 'advanced class', 'sufficient depth']

Information architecture without internal theory: an inductive design process
This article suggests that Information Architecture (IA) design is primarily an
	inductive process. Although top-level goals, user attributes and
	available content are periodically considered, the process involves
	bottom-up design activities. IA is inductive partly because it lacks
	internal theory, and partly because it is an activity that supports
	emergent phenomena (user experiences) from basic design components. The
	nature of IA design is well described by Constructive Induction (CI), a
	design process that involves locating the best representational
	framework for the design problem, identifying a solution within that
	framework and translating it back to the design problem at hand. The
	future of IA, if it remains inductive or develops a body of theory (or
	both), is considered
['information architecture design', 'inductive design process', 'bottom-up designactivities', 'internal theory', 'emergent phenomena', 'user experiences', 'constructive induction']
['information architecture', 'basic design component', 'internal theory', 'inductive design process', 'design process', 'design problem', 'available content', 'user attribute', 'inductive process', 'level goal']

Information architecture for the Web: The IA matrix approach to designing
	children's portals
The article presents a matrix that can serve as a tool for designing the
	information architecture of a Web portal in a logical and systematic
	manner. The information architect begins by inputting the portal's
	objective, target user, and target content. The matrix then determines
	the most appropriate information architecture attributes for the portal
	by filling in the Applied Information Architecture portion of the
	matrix. The article discusses how the matrix works using the example of
	a children's Web portal to provide access to museum information
['information architecture', 'target user', 'target content', "children's web portal", 'museum information']
['applied information architecture portion', 'ia matrix approach', 'appropriate information architecture attribute', 'web portal', 'portal', 'matrix', 'child', 'article', 'target content', 'web']

Information architecture: notes toward a new curriculum
There are signs that information architecture is coalescing into a field of
	professional practice. However, if it is to become a profession, it
	must develop a means of educating new information architects. Lessons
	from other fields suggest that professional education typically evolves
	along a predictable path, from apprenticeships to trade schools to
	college- and university-level education. Information architecture
	education may develop more quickly to meet the growing demands of the
	information society. Several pedagogical approaches employed in other
	fields may be adopted for information architecture education, as long
	as the resulting curricula provide an interdisciplinary approach and
	balance instruction in technical and design skills with consideration
	of theoretical concepts. Key content areas are information
	organization, graphic. design, computer science, user and usability
	studies, and communication. Certain logistics must be worked out,
	including where information architecture studies should be housed and
	what kinds of degrees should be offered and at what levels. The
	successful information architecture curriculum will be flexible and
	adaptable in order to meet the changing needs of students and the
	marketplace
['professional practice', 'information organization', 'graphic design', 'computerscience', 'usability studies', 'information architects', 'professionaleducation', 'pedagogical approaches', 'information architecture education']
['information architecture', 'successful information architecture curriculum', 'information architecture study', 'key content area', 'information architecture education', 'new information architect', 'several pedagogical approach', 'new curriculum', 'professional practice', 'predictable path']

Information architecture in JASIST: just where did we come from?
The emergence of Information Architecture within the information systems world
	has been simultaneously drawn out yet rapid. Those with an eye on
	history are quick to point to Wurman's 1976 use of the term
	"architecture of information," but it has only been in the last 2 years
	that IA has become the source of sufficient interest for people to
	label themselves professionally as Information Architects. The impetus
	for this recent emergence of IA can be traced to a historical summit,
	supported by ASIS&T in May 2000 at Boston. It was here that several
	hundred of us gathered to thrash out the questions of just what IA was
	and what this new field might become. At the time of the summit,
	invited to present a short talk on my return journey from the annual
	ACM SIGCHI conference, I entered the summit expecting little and
	convinced that IA was nothing new. I left 2 days later refreshed, not
	just by the enthusiasm of the attendees for this term but by IA's
	potential to unify the disparate perspectives and orientations of
	professionals from a range of disciplines. It was at this summit that
	the idea for the special issue took root. I proposed the idea to Don
	Kraft, hoping he would find someone else to run with it. AS luck would
	have it, I ended up taking charge of it myself, with initial support
	from David Blair. From the suggestion to the finished product-has been
	the best part of 2 years, and in that time more than 50 volunteers
	reviewed over 20 submissions
['information architecture', 'information systems', 'metadata fields', 'controlledvocabularies', 'web sites', 'cd-rom', 'qualified information architect']
['acm sigchi conference', 'information architecture', 'information system world', 'historical summit', 'return journey', 'recent emergence', 'information architects', 'short talk', 'new field', 'year']

The impact of the Internet on public library use: an analysis of the current
	consumer market for library and Internet services
The potential impact of the Internet on the public's demand for the services
	and resources of public libraries is an issue of critical importance.
	The research reported in this article provides baseline data concerning
	the evolving relationship between the public's use of the library and
	its use of the Internet. The authors developed a consumer model of the
	American adult market for information services and resources, segmented
	by use (or nonuse) of the public library and by access (or lack of
	access) to, and use (or nonuse) of, the Internet. A national Random
	Digit Dialing telephone survey collected data to estimate the size of
	each of six market segments, and to describe their usage choices
	between the public library and the Internet. The analyses presented in
	this article provide estimates of the size and demographics of each of
	the market segments; describe why people are currently using the public
	library and the Internet; identify the decision criteria people use in
	their choices of which provider to use; identify areas in which
	libraries and the Internet appear to be competing and areas in which
	they appear to be complementary; and identify reasons why people choose
	not to use the public library and/or the Internet. The data suggest
	that some differentiation between the library and the Internet is
	taking place, which may very well have an impact on consumer choices
	between the two. Longitudinal research is necessary to fully reveal
	trends in these usage choices, which have implications for all types of
	libraries in planning and policy development
['internet', 'public libraries', 'baseline data', 'consumer model', 'american adultmarket', 'national random digit dialing telephone survey', 'decisioncriteria', 'public library', 'longitudinal research']
['public library', 'digit dialing telephone survey', 'public library use', 'internet', 'american adult market', 'article provide estimate', 'library', 'decision criterion people', 'potential impact', 'internet service']

Duality revisited: construction of fractional frequency distributions based on
	two dual Lotka laws
Fractional frequency distributions of, for example, authors with a certain
	(fractional) number of papers are very irregular, and therefore not
	easy to model or to explain. The article gives a first attempt to this
	by as suming two simple Lotka laws (with exponent 2): one for the
	number of authors with n papers (total count here) and one for the
	number of papers with n authors, n in N. Based on an earlier made
	convolution model of Egghe, interpreted and reworked now for discrete
	scores, we are able to produce theoretical fractional frequency
	distributions with only one parameter, which are in very close
	agreement with the practical ones as found in a large dataset produced
	earlier by Rao (1995). The article also shows that (irregular)
	fractional frequency distributions are a consequence of Lotka's law,
	and are not examples of breakdowns of this famous historical law
['dual lotka laws', 'convolution model', 'discrete scores', 'irregular fractionalfrequency distributions']
['fractional frequency distribution', 'dual lotka law', 'simple lotka law', 'theoretical fractional frequency', 'author', 'paper', 'number', 'example', 'irregular', 'article']

Relevance of Web documents: ghosts consensus method
The dominant method currently used to improve the quality of Internet search
	systems is often called "digital democracy." Such an approach implies
	the utilization of the majority opinion of Internet users to determine
	the most relevant documents: for example, citation index usage for
	sorting of search results (google.com) or an enrichment of a query with
	terms that are asked frequently in relation with the query's theme.
	"Digital democracy" is an effective instrument in many cases, but it
	has an unavoidable shortcoming, which is a matter of principle: the
	average intellectual and cultural level of Internet users is very low;
	everyone knows what kind of information is dominant in Internet query
	statistics. Therefore, when one searches the Internet by means of
	"digital democracy" systems, one gets answers that reflect an
	underlying assumption that the user's mind potential is very low, and
	that his cultural interests are not demanding. Thus, it is more correct
	to use the term "digital ochlocracy" to refer to Internet search
	systems with "digital democracy." Based on the well-known mathematical
	mechanism of linear programming, we propose a method to solve the
	indicated problem
['internet search systems', 'digital democracy', 'majority opinion', 'citation indexusage', 'search results', 'internet query statistics', 'digital ochlocracy', 'linear programming', 'ghosts consensus method', 'world wide web']
['digital democracy', 'citation index usage', 'ghost consensus method', 'internet user', 'internet search', 'relevant document', 'majority opinion', 'search result', 'effective instrument', 'unavoidable shortcoming']

Note on "Deterministic inventory lot-size models under inflation with shortages
	and deterioration for fluctuating demand" by Yang et al
For original paper see H.-L. Yang et al., ibid., vol.48, p.144-58 (2001). Yang
	et al. extended the lot-size models to allow for inflation and
	fluctuating demand. For this model they proved that the optimal
	replenishment schedule exists and is unique. They also proposed an
	algorithm to find the optimal policy. The present paper provides
	examples, which show that the optimal replenishment schedule and
	consequently the overall optimal policy may not exist
['deterministic inventory lot-size models', 'inflation', 'fluctuating demand', 'optimalreplenishment schedule', 'optimal policy algorithm', 'optimal schedulingparameters']
['l. yang et al', 'yang et al', 'deterministic inventory lot', 'size model', 'original paper', 'inflation', 'demand', 'replenishment schedule', 'model', 'optimal policy']

Designing a screening experiment for highly reliable products
Within a reasonable life-testing time, how to improve the reliability of highly
	reliable products is one of the great challenges. By using a resolution
	III experiment together with degradation test, Tseng et al. (1995)
	presented a case study of improving the reliability of fluorescent
	lamps. However, in conducting such an experiment, they did not address
	the problem of how to choose the optimal settings of variables, such as
	sample size, inspection frequency, and termination time for each run,
	which are influential to the correct identification of significant
	factors and the experimental cost. Assuming that the product's
	degradation paths satisfy Wiener processes, this paper proposes a
	systematic approach to the aforementioned problem. First, an
	identification rule is proposed. Next, under the constraints of a
	minimum probability of correct decision and a maximum probability of
	incorrect decision of the proposed identification rule, the optimum
	test plan can be obtained by minimizing the total experimental cost. An
	example is provided to illustrate the proposed method
['screening experiment', 'highly reliable products', 'resolution iii design', 'degradation tests', 'wiener process', 'inspection frequency', 'terminationtime', 'optimal test plan', 'fluorescent lamps', 'minimum probability ofcorrect decision', 'maximum probability of incorrect decision', 'identification rule']
['tseng et al', 'total experimental cost', 'identification rule', 'experimental cost', 'reliable product', 'aforementioned problem', 'wiener process', 'systematic approach', 'minimum probability', 'degradation path']

Analysis and efficient implementation of a linguistic fuzzy c-means
The paper is concerned with a linguistic fuzzy c-means (FCM) algorithm with
	vectors of fuzzy numbers as inputs. This algorithm is based on the
	extension principle and the decomposition theorem. It turns out that
	using the extension principle to extend the capability of the standard
	membership update equation to deal with a linguistic vector has a huge
	computational complexity. In order to cope with this problem, an
	efficient method based on fuzzy arithmetic and optimization has been
	developed and analyzed. We also carefully examine and prove that the
	algorithm behaves in a way similar to the FCM in the degenerate
	linguistic case. Synthetic data sets and the iris data set have been
	used to illustrate the behavior of this linguistic version of the FCM
['linguistic fuzzy c-means algorithm', 'fuzzy numbers', 'extension principle', 'decomposition theorem', 'computational complexity', 'fuzzy arithmetic', 'optimization', 'linguistic vectors']
['linguistic fuzzy c', 'synthetic data set', 'membership update equation', 'extension principle', 'efficient implementation', 'fcm', 'algorithm', 'fuzzy number', 'computational complexity', 'fuzzy arithmetic']

Warranty reserves for nonstationary sales processes
Estimation of warranty costs, in the event of product failure within the
	warranty period, is of importance to the manufacturer. Costs associated
	with replacement or repair of the product are usually drawn from a
	warranty reserve fund created by the manufacturer. Considering a
	stochastic sales process, first and second moments (and thereby the
	variance) are derived for the manufacturer's total discounted warranty
	cost of a single sale for single-component items under four different
	warranty policies from a manufacturer's point of view. These servicing
	strategies represent a renewable free-replacement, nonrenewable
	free-replacement, renewable pro-rata, and a nonrenewable minimal-repair
	warranty plans. The results are extended to determine the mean and
	variance of total discounted warranty costs for the total sales over
	the life cycle of the product. Furthermore, using a normal
	approximation, warranty reserves necessary for a certain protection
	level, so that reserves are not completely depleted, are found. Results
	and their managerial implications are studied through an extensive
	example
['nonstationary sales processes', 'warranty reserves', 'warranty costs estimation', 'product failure', 'product replacement', 'product repair', 'stochastic salesprocess', 'first moments', 'second moments', 'variance', 'total discountedwarranty cost', 'single-component items', 'servicing strategies', 'renewablefree-replacement', 'nonrenewable free-replacement', 'renewable pro-rata', 'nonrenewable minimal-repair warranty plans', 'total discounted warrantycosts', 'product life cycle', 'normal approximation', 'managerialimplications']
['stochastic sale process', 'warranty reserve necessary', 'nonstationary sale process', 'warranty cost', 'warranty reserve', 'warranty reserve fund', 'warranty period', 'product failure', 'cost', 'second moment']

A multimodal data collection tool using REALbasic and Mac OS X
This project uses REALbasic 3.5 in the Mac OS X environment for development of
	a configuration tool that builds a data collection procedure for
	investigating the effectiveness of sonified graphs. The advantage of
	using REALbasic with the Mac OS X system is that it provides rapid
	development of stimulus presentation, direct recording of data to
	files, and control over other procedural issues. The program can be
	made to run natively on the new Mac OS X system, older Mac OS systems,
	and Windows (98SE, ME, 2000 PRO). With modification, similar programs
	could be used to present any number of visual/auditory stimulus
	combinations, complete with questions for each stimulus
['multimodal data collection tool', 'realbasic', 'mac os x environment', 'configurationtool', 'data collection', 'sonified graphs', 'visual data comprehension', 'psychology', 'visual stimulus', 'auditory stimulus', 'stimulus presentation', 'direct data recording', 'windows']
['new mac os x system', 'mac os x system', 'mac os x environment', 'multimodal data collection tool', 'old mac os system', 'datum collection procedure', 'realbasic', 'configuration tool', 'direct recording', 'stimulus presentation']

Toward an Experimental Timing Standards Lab: benchmarking precision in the real
	world
Much discussion has taken place over the relative merits of various platforms
	and operating systems for real-time data collection. Most would agree
	that, provided great care is taken, many are capable of millisecond
	timing precision. However, to date, much of this work has focused on
	the theoretical aspects of raw performance. It is our belief that
	researchers would be better informed if they could place confidence
	limits on their own specific paradigms in situ and without
	modification. To this end, we have developed a millisecond precision
	test rig that can control and time experiments on a second presentation
	machine. We report on the specialist hardware and software used. We
	elucidate the importance of the approach in relation to real-world
	experimentation
['benchmarking precision', 'experimental timing standards lab', 'performanceevaluation', 'operating systems', 'event generation software', 'real-timedata collection', 'millisecond timing precision']
['experimental timing standards lab', 'time datum collection', 'real', 'world', 'operating system', 'relative merit', 'much discussion', 'great care', 'timing precision', 'raw performance']

A server-side program for delivering experiments with animations
A server-side program for animation experiments is presented. The program is
	capable of delivering an experiment composed of discrete animation
	sequences in various file formats, collecting a discrete or continuous
	response from the observer, evaluating the appropriateness of the
	response, and ensuring that the user is not proceeding at an
	unreasonable rate. Most parameters of the program are controllable by
	experimenter-edited text files or simple switches in the program code,
	thereby minimizing the need for programming to create new experiments.
	A simple demonstration experiment is discussed and is freely available
['server-side program', 'animation experiment delivery', 'discrete animationsequences', 'file formats', 'web based psychological experiments', 'internet', 'experimenter-edited text files']
['simple demonstration experiment', 'program code', 'new experiment', 'program', 'simple switch', 'file format', 'most parameter', 'text file', 'discrete animation', 'unreasonable rate']

Using NetCloak to develop server-side Web-based experiments without writing CGI
	programs
Server-side experiments use the Web server, rather than the participant's
	browser, to handle tasks such as random assignment, eliminating
	inconsistencies with Java and other client-side applications.
	Heretofore, experimenters wishing to create server-side experiments
	have had to write programs to create common gateway interface (CGI)
	scripts in programming languages such as Perl and C++. NetCloak uses
	simple, HTML-like commands to create CGIs. We used NetCloak to
	implement an experiment on probability estimation. Measurements of time
	on task and participants' IP addresses assisted quality control.
	Without prior training, in less than 1 month, we were able to use
	NetCloak to design and create a Web-based experiment and to help
	graduate students create three Web-based experiments of their own
['netcloak', 'server-side web-based experiments', 'cgi programs', 'web server', 'randomassignment', 'java', 'client-side applications', 'common gateway interfacescripts', 'perl', 'c++ language', 'html', 'probability estimation', 'ipaddresses', 'quality control', 'graduate students', 'internet', 'behavioraldata', 'psychology']
['common gateway interface', 'server', 'netcloak', 'experiment', 'cgi', 'web', 'random assignment', 'participant', 'web server', 'prior training']

Open courseware and shared knowledge in higher education
Most college and university campuses in the United States and much of the
	developed world today maintain one, two, or several learning management
	systems (LMSs), which are courseware products that provide students and
	faculty with Web-based tools to manage course-related applications.
	Since the mid-1990s, two predominant models of Web courseware
	management systems have emerged: commercial and noncommercial. Some of
	the commercial products available today were created in academia as
	noncommercial but have since become commercially encumbered. Other
	products remain noncommercial but are struggling to survive in a world
	of fierce commercial competition. This article argues for an ethics of
	pedagogy in higher education that would be based on the guiding
	assumptions of the non-proprietary, peer-to-peer, open-source software
	movement
['open courseware', 'shared knowledge', 'higher education', 'learning managementsystems', 'college', 'university', 'web courseware management systems', 'commercial products', 'ethics', 'internet', 'open-source software']
['high education', 'united states', 'world today', 'commercial product available today', 'university campus', 'most college', 'courseware product', 'management system', 'web courseware', 'predominant model']

Web-based experiments controlled by JavaScript: an example from probability
	learning
JavaScript programs can be used to control Web experiments. This technique is
	illustrated by an experiment that tested the effects of advice on
	performance in the classic probability-learning paradigm. Previous
	research reported that people tested via the Web or in the lab tended
	to match the probabilities of their responses to the probabilities that
	those responses would be reinforced. The optimal strategy, however, is
	to consistently choose the more frequent event; probability matching
	produces suboptimal performance. We investigated manipulations we
	reasoned should improve performance. A horse race scenario in which
	participants predicted the winner in each of a series of races between
	two horses was compared with an abstract scenario used previously. Ten
	groups of learners received different amounts of advice, including all
	combinations of (1) explicit instructions concerning the optimal
	strategy, (2) explicit instructions concerning a monetary sum to
	maximize, and (3) accurate information concerning the probabilities of
	events. The results showed minimal effects of horse race versus
	abstract scenario. Both advice concerning the optimal strategy and
	probability information contributed significantly to performance in the
	task. This paper includes a brief tutorial on JavaScript, explaining
	with simple examples how to assemble a browser-based experiment
['web-based experiments', 'javascript', 'probability learning', 'advice', 'explicitinstructions', 'probability', 'browser-based experiment', 'internet-basedresearch']
['optimal strategy', 'horse race scenario', 'abstract scenario', 'probability', 'horse race', 'web', 'experiment', 'explicit instruction', 'suboptimal performance', 'frequent event']

Using latent semantic analysis to assess reader strategies
We tested a computer-based procedure for assessing reader strategies that was
	based on verbal protocols that utilized latent semantic analysis (LSA).
	Students were given self-explanation-reading training (SERT), which
	teaches strategies that facilitate self-explanation during reading,
	such as elaboration based on world knowledge and bridging between text
	sentences. During a computerized version of SERT practice, students
	read texts and typed self-explanations into a computer after each
	sentence. The use of SERT strategies during this practice was assessed
	by determining the extent to which students used the information in the
	current sentence versus the prior text or world knowledge in their
	self-explanations. This assessment was made on the basis of human
	judgments and LSA. Both human judgments and LSA were remarkably similar
	and indicated that students who were not complying with SERT tended to
	paraphrase the text sentences, whereas students who were compliant with
	SERT tended to explain the sentences in terms of what they knew about
	the world and of information provided in the prior text context. The
	similarity between human judgments and LSA indicates that LSA will be
	useful in accounting for reading strategies in a Web-based version of
	SERT
['latent semantic analysis', 'reader strategy assessment', 'computer-based procedure', 'verbal protocols', 'self-explanation-reading training', 'elaboration', 'worldknowledge', 'text sentence bridging', 'human judgments']
['latent semantic analysis', 'world knowledge', 'reader strategy', 'student', 'lsa', 'sert', 'strategy', 'self', 'explanation', 'human judgment']

Personality research on the Internet: a comparison of Web-based and traditional
	instruments in take-home and in-class settings
Students, faculty, and researchers have become increasingly comfortable with
	the Internet, and many of them are interested in using the Web to
	collect data. Few published studies have investigated the differences
	between Web-based data and data collected with more traditional
	methods. In order to investigate these potential differences, two
	important factors were crossed in this study: whether the data were
	collected on line or not and whether the data were collected in a group
	setting at a fixed time or individually at a time of the respondent's
	choosing. The Visions of Morality scale (Shelton and McAdams, 1990) was
	used, and the participants were assigned to one of four conditions:
	in-class Web survey, in-class paper-and-pencil survey; take-home Web
	survey, and take-home paper-and-pencil survey. No significant
	differences in scores were found for any condition; however, response
	rates were affected by the type of survey administered, with the
	take-home Web-based instrument having the lowest response rate.
	Therefore, researchers need to be aware that different modes of
	administration may affect subject attrition and may, therefore,
	confound investigations of other independent variables
['personality research', 'internet', 'web-based instruments', 'data collection', 'visionsof morality scale', 'in-class web survey', 'in-class paper-and-pencilsurvey', 'take-home web survey', 'take-home paper-and-pencil survey', 'response rates', 'administration', 'subject attrition']
['pencil survey', 'low response rate', 'class web survey', 'home web', 'personality research', 'home paper', 'important factor', 'class setting', 'collect datum', 'potential difference']

Implications of document-level literacy skills for Web site design
The proliferation of World Wide Web (Web) sites and the low cost of publishing
	information on the Web have placed a tremendous amount of information
	at the fingertips of millions of people. Although most of this
	information is at least intended to be accurate, there is much that is
	rumor, innuendo, urban legend, and outright falsehood. This raises
	problems especially for students (of all ages) trying to do research or
	learn about some topic. Finding accurate, credible information requires
	document level literacy skills, such as integration, sourcing,
	corroboration, and search. This paper discusses these skills and offers
	a list of simple ways that designers of educational Web sites can help
	their visitors utilize these skills
['document-level literacy skills', 'rumor', 'innuendo', 'urban legend', 'falsehood', 'students', 'accurate credible information', 'integration', 'sourcing', 'corroboration', 'search', 'educational web site design']
['level literacy skill', 'world wide web', 'web site design', 'simple way that designer', 'document level literacy skill', 'information', 'low cost', 'outright falsehood', 'accurate', 'urban legend']

Fuzzy control of multivariable process by modified error decoupling
In this paper, a control concept for the squared (equal number of inputs and
	outputs) multivariable process systems is given. The proposed control
	system consists of two parts, single loop fuzzy controllers in each
	loop and a centralized decoupling unit. The fuzzy control system uses
	feedback control to minimize the error in the loop and the decoupler
	uses an adaptive technique to mitigate loop interactions. The decoupler
	predicts the interacting loop changes and modifies the input (error) of
	the loop controller. The controller was tested on the simulation model
	of "single component vaporizer" process
['multivariable process', 'modified error decoupling', 'squared multivariable processsystems', 'square multivariable process systems', 'single-loop fuzzycontrollers', 'centralized decoupling unit', 'feedback control', 'errorminimization', 'loop interaction mitigation', 'single component vaporizerprocess', 'set point changes', 'load changes']
['single loop fuzzy controller', 'fuzzy control system', 'multivariable process', 'multivariable process system', 'centralized decoupling unit', 'error decoupling', 'equal number', 'control concept', 'error', 'input']

Improving computer security for authentication of users: influence of proactive
	password restrictions
Entering a user name-password combination is a widely used procedure for
	identification and authentication in computer systems. However, it is a
	notoriously weak method, in that the passwords adopted by many users
	are easy to crack. In an attempt to, improve security, proactive
	password checking may be used, in which passwords must meet several
	criteria to be more resistant to cracking. In two experiments, we
	examined the influence of proactive password restrictions on the time
	that it took to generate an acceptable password and to use it
	subsequently to log in. The required length was a minimum of five
	characters in experiment I and eight characters in experiment 2. In
	both experiments, one condition had only the length restriction, and
	the other had additional restrictions. The additional restrictions
	greatly increased the time it took to generate the password but had
	only a small effect on the time it took to use it subsequently to log
	in. For the five-character passwords, 75% were cracked when no other
	restrictions were imposed, and this was reduced to 33% with the
	additional restrictions. For the eight-character passwords, 17% were
	cracked with no other restrictions, and 12.5% with restrictions. The
	results indicate that increasing the minimum character length reduces
	crackability and increases security, regardless of whether additional
	restrictions are imposed
['computer security', 'user authentication', 'proactive password checking', 'proactivepassword restrictions', 'length restriction', 'five-character passwords', 'eight-character passwords']
['additional restriction', 'proactive password restriction', 'minimum character length', 'password restriction', 'character password', 'weak method', 'password combination', 'computer system', 'password checking', 'user']

Multidimensional data visualization
Historically, data visualization has been limited primarily to two dimensions
	(e.g., histograms or scatter plots). Available software packages (e.g.,
	Data Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of
	producing three-dimensional scatter plots with (varying degrees of)
	user interactivity. We constructed our own data visualization
	application with the Visualization Toolkit (Schroeder et al., 1998) and
	Tcl/Tk to display multivariate data through the application of glyphs
	(Ware, 2000). A glyph is a visual object onto which many data
	parameters may be mapped, each with a different visual attribute (e.g.,
	size or color). We used our multi-dimensional data viewer to explore
	data from several psycholinguistic experiments. The graphical interface
	provides flexibility when users dynamically explore the
	multidimensional image rendered from raw experimental data. We
	highlight advantages of multidimensional data visualization and
	consider some potential limitations
['multidimensional data visualization', '3d scatter plots', 'user interactivity', 'visualization toolkit', 'tcl/tk', 'multivariate data display', 'glyphs', 'visual object', 'data parameters', 'visual attribute', 'multi-dimensionaldata viewer', 'psycholinguistic experiments', 'graphical interface', 'multidimensional image rendering']
['multidimensional datum visualization', 'available software package', 'dimensional scatter plot', 'schroeder et al', 'datum visualization', 'different visual attribute', 'dimensional datum viewer', 'scatter plot', 'data desk', 'raw experimental datum']

Fitting mixed-effects models for repeated ordinal outcomes with the NLMIXED
	procedure
This paper presents an analysis of repeated ordinal outcomes arising from two
	psychological studies. The first case is a repeated measures analysis
	of variance; the second is a mixed-effects regression. in a
	longitudinal design. In both, the subject-specific variation is modeled
	by including random effects in the linear predictor (inside a link
	function) of a generalized linear model. The NLMIXED procedure in SAS
	is used to fit the mixed-effects models for the categorical response
	data. The presentation emphasizes the parallel between the model.
	specifications and the SAS statements. The purpose of this paper is to
	facilitate the use of mixed-effects models in the analysis of repeated
	ordinal outcomes
['repeated ordinal outcomes', 'psychological studies', 'repeated measures analysis ofvariance', 'mixed-effects regression', 'longitudinal design', 'subject-specific variation modeling', 'random effects', 'linear predictor', 'generalized linear model', 'nlmixed procedure', 'mixed-effects modelfitting', 'categorical response data', 'model specifications']
['ordinal outcome', 'effect model', 'psychological study', 'measure analysis', 'effect regression', 'longitudinal design', 'specific variation', 'random effect', 'linear predictor', 'linear model']

Teaching psychology as a laboratory science in the age of the Internet
For over 30 years, psychologists have relied on computers to teach experimental
	psychology. With the advent of experiment generators, students can
	create well-designed experiments and can test sophisticated hypotheses
	from the start of their undergraduate training. Characteristics of new
	Net-based experiment generators are discussed and compared with
	traditional stand-alone generators. A call is made to formally evaluate
	the instructional effectiveness of the wide range of experiment
	generators now available. Specifically, software should be evaluated in
	terms of known learning outcomes, using appropriate control groups. The
	many inherent differences between any two software programs should be
	made clear. The teacher's instructional method should be fully
	described and held constant between comparisons. Finally, the often
	complex interaction between the teacher's instructional method and the
	pedagogical details of the software must be considered
['experimental psychology teaching', 'laboratory science', 'internet', 'computers', 'well-designed experiments', 'hypothesis testing', 'undergraduate training', 'net-based experiment generators', 'stand-alone generators', 'instructionaleffectiveness', 'software', 'known learning outcomes', 'control groups', 'teacher instructional method', 'pedagogical details']
['experiment generator', 'appropriate control group', 'instructional method', 'wide range', 'undergraduate training', 'sophisticated hypothesis', 'traditional stand', 'instructional effectiveness', 'software program', 'laboratory science']

Capturing niche markets with copper
For "last-mile access" in niche applications, twisted copper pair may be the
	cable of best option to gain access and deliver desired services. The
	article discusses how operators can use network edge devices to serve
	new customers. Niche market segments represent a significant
	opportunity for cable TV delivery of television and high-speed Internet
	signals. But the existing telecommunications infrastructure in those
	developments frequently presents unique challenges for the service
	provider to overcome
['last-mile access', 'twisted copper pair', 'network edge devices', 'copper cables', 'niche markets']
['cable tv delivery', 'niche market segment', 'niche market', 'twisted copper pair', 'network edge device', 'niche application', 'mile access', 'good option', 'new customer', 'service']

Fresh voices, big ideas [IBM internship program]
IBM is matching up computer-science and MBA students with its business managers
	in an 11-week summer internship program and challenging them to develop
	innovative technology ideas
['internship program', 'ibm business managers', 'mba college students', 'computer-science students', 'patents']
['innovative technology idea', 'summer internship program', 'ibm internship program', 'big idea', 'business manager', 'fresh voice', 'mba student', 'science', 'computer']

Down up [IT projects]
Despite the second quarter's gloomy GDP report, savvy CIOs are forging ahead
	with big IT projects that will position their companies to succeed when
	the economy soars again
['strategic technology projects', 'walgreen', 'ford', 'caterpillar', "victoria's secret", 'morgan stanley', 'staples']
['big it project', 'it project', 'gloomy gdp report', 'second quarter', 'savvy cio', 'economy', 'company']

An automated parallel image registration technique based on the correlation of
	wavelet features
With the increasing importance of multiple multiplatform remote sensing
	missions, fast and automatic integration of digital data from disparate
	sources has become critical to the success of these endeavors. Our work
	utilizes maxima of wavelet coefficients to form the basic features of a
	correlation-based automatic registration algorithm. Our wavelet-based
	registration algorithm is tested successfully with data from the
	National Oceanic and Atmospheric Administration (NOAA) Advanced Very
	High Resolution Radiometer (AVHRR) and the Landsat Thematic Mapper
	(TM), which differ by translation and/or rotation. By the choice of
	high-frequency wavelet features, this method is similar to an
	edge-based correlation method, but by exploiting the multiresolution
	nature of a wavelet decomposition, our method achieves higher
	computational speeds for comparable accuracies. This algorithm has been
	implemented on a single-instruction multiple-data (SIMD) massively
	parallel computer, the MasPar MP-2, as well as on the CrayT3D, the Cray
	T3E, and a Beowulf cluster of Pentium workstations
['geophysical measurement technique', 'land surface', 'terrain mapping', 'opticalimaging', 'microwave radiometry', 'image processing', 'automated parallelimage registration', 'correlation', 'wavelet feature', 'remote sensing', 'automatic registration algorithm', 'avhrr', 'landsat thematic mapper', 'wavelet decomposition', 'simd massively parallel computing']
['wavelet feature', 'registration algorithm', 'pentium workstation', 'maspar mp-2', 'multiplatform remote', 'digital datum', 'beowulf cluster', '-PRON- wavelet', 'multiple multiplatform', 'automatic integration']

Design of PID-type controllers using multiobjective genetic algorithms
The design of a PID controller is a multiobjective problem. A plant and a set
	of specifications to be satisfied are given. The designer has to adjust
	the parameters of the PID controller such that the feedback
	interconnection of the plant and the controller satisfies the
	specifications. These specifications are usually competitive and any
	acceptable solution requires a tradeoff among them. An approach for
	adjusting the parameters of a PID controller based on multiobjective
	optimization and genetic algorithms is presented in this paper. The
	MRCD (multiobjective robust control design) genetic algorithm has been
	employed. The approach can be easily generalized to design
	multivariable coupled and decentralized PID loops and has been
	successfully validated for a large number of experimental cases
['pid-type controllers', 'multiobjective genetic algorithms', 'feedbackinterconnection', 'multiobjective robust control design', 'multivariablecoupled pid loops', 'decentralized pid loops', 'tuning methods']
['pid controller', 'multiobjective robust control design', 'multiobjective genetic algorithm', 'genetic algorithm', 'multiobjective problem', 'pid', 'type controller', 'acceptable solution', 'specification', 'controller']

Temelin casts its shadow [nuclear power plant]
Reservations about Temelin nuclear plant in the Czech Republic are political
	rather than technical. This paper discusses the problems of
	turbogenerator vibrations and how they were diagnosed. The paper also
	discusses some of the other problems of commissioning the power plant.
	The simulator used for training new staff is also mentioned
['temelin nuclear plant', 'czech republic', 'turbogenerator vibrations', 'power plantcommissioning', 'training simulator']
['temelin nuclear plant', 'nuclear power plant', 'power plant', 'czech republic', 'turbogenerator vibration', 'problem', 'paper', 'discuss', 'technical', 'new staff']

How should team captains order golfers on the final day of the Ryder Cup
	matches?
I used game theory to examine how team captains should select their slates for
	the final day of the Ryder Cup matches. Under the assumption that
	golfers have different abilities and are not influenced by pressure or
	momentum, I found that drawing names from a hat will do no worse than
	any other strategy
['golf', 'golfer ordering', 'ryder cup final day', 'game theory', 'slate']
['ryder cup match', 'final day', 'captain order golfer', 'ryder cup', 'game theory', 'team captain', 'different ability', 'momentum', 'hat', 'name']

Mount Sinai Hospital uses integer programming to allocate operating room time
An integer-programming model and a post-solution heuristic allocates operating
	room time to the five surgical divisions at Toronto's Mount Sinai
	Hospital. The hospital has used this approach for several years and
	credits it with both administrative savings and the ability to produce
	quickly an equitable master surgical schedule
['mount sinai hospital', 'integer programming', 'operating room time allocation', 'toronto', 'ontario', 'canada', 'post-solution heuristic']
['solution heuristic allocate', 'equitable master surgical schedule', 'mount sinai hospital', 'mount sinai', 'room time', 'integer programming', 'programming model', 'surgical division', 'hospital', 'integer']

Using the Small Business Innovation Research Program to turn your ideas into
	products
The US Government's Small Business Innovation Research Program helps small
	businesses transform new ideas into commercial products. The program
	provides an ideal means for businesses and universities to obtaining
	funding for cooperative projects. Rules and information for the program
	are readily available, and I will give a few helpful hints to provide
	guidance
['small business innovation research program', 'commercial product development', 'businesses', 'universities', 'funding', 'cooperative projects', 'us government', 'usa']
['small business innovation research program', 'commercial product', 'ideal mean', 'us government', 'new idea', 'cooperative project', 'helpful hint', 'information', 'available', 'rule']

Student consulting projects benefit faculty and industry
Student consulting projects require students to apply OR/MS tools to obtain
	insight into the activities of firms in the community. These projects
	benefit faculty by providing clear feedback on the real capabilities of
	students, a broad connection to local industry, and material for case
	studies and research. They benefit companies by stimulating new
	thinking regarding their activities and delivering results they can
	use. Projects provide insights into the end-user modeling mode of OR/MS
	practice. Projects support continuous improvement as the lessons gained
	from a crop of projects enable better teaching during the next course
	offering, which in turn leads to better projects and further insights
	into teaching
['student consulting projects', 'or/ms tools', 'student placements', 'studentcapability feedback', 'case study material']
['student consulting project', 'project', 'insight', 'user modeling mode', 'local industry', 'activity', 'ms tool', 'broad connection', 'real capability', 'faculty']

In search of strategic operations research/management science
We define strategic OR/MS as "OR/MS work that leads to a sustainable
	competitive advantage." We found evidence of strategic OR/MS in the
	literature of strategic information systems (SIS) and OR/MS. We
	examined 30 early examples of SIS, many of which contained OR/MS work.
	Many of the most successful had high OR/MS content, while the least
	successful contained none. The inclusion of OR/MS work may be a key to
	sustaining an advantage from information technology. We also examined
	the Edelman Prize finalist articles published between 1990 and 1999. We
	found that 13 of the 42 private sector applications meet our definition
	of strategic OR/MS
['operations research', 'management science', 'strategic or/ms', 'strategic informationsystems', 'sis']
['edelman prize finalist article', 'strategic information system', 'ms work', 'strategic operation research', 'ms', 'competitive advantage', 'private sector application', 'early example', 'strategic', 'management science']

Baseball, optimization, and the World Wide Web
The competition for baseball play-off spots-the fabled pennant race-is one of
	the most closely watched American sports traditions. While play-off
	race statistics, such as games back and magic number, are informative,
	they are overly conservative and do not account for the remaining
	schedule of games. Using optimization techniques, one can model
	schedule effects explicitly and determine precisely when a team has
	secured a play-off spot or has been eliminated from contention. The
	RIOT Baseball Play-off Races Web site developed at the University of
	California, Berkeley, provides automatic updates of new,
	optimization-based play-off race statistics each day of the major
	league baseball season. In developing the site, we found that we could
	determine the first-place elimination status of all teams in a division
	using a single linear-programming formulation, since a minimum win
	threshold for teams finishing in first place applies to all teams in a
	division. We identified a similar (but weaker) result for the problem
	of play-off elimination with wildcard teams
['baseball play-off spot competition', 'optimization', 'world wide web', 'pennant race', 'play-off race statistics', 'games back', 'magic number', 'game schedule', 'riotbaseball play-off races web site', 'linear programming', 'lp', 'minimum winthreshold']
['place elimination status', 'league baseball season', 'races web site', 'american sport tradition', 'fabled pennant race', 'riot baseball play', 'world wide web', 'race statistic', 'play', 'baseball play']

From revenue management concepts to software systems
In 1999, after developing and installing over 170 revenue management (RM)
	systems for more than 70 airlines, PROS Revenue Management, Inc. had
	the opportunity to develop RM systems for three companies in nonairline
	industries. PROS research and design department designed the
	opportunity analysis study (OAS), a mix of OR/MS, consulting, and
	software development practices to determine the applicability of RM in
	new business situations. PROS executed OASs with the three companies.
	In all three cases, the OAS supported the value of RM and led to
	contracts for implementation of RM systems
['revenue management concepts', 'software systems', 'rm systems', 'pros revenuemanagement', 'inc', 'opportunity analysis study', 'oas', 'or/ms', 'consultingpractices', 'software development practices']
['new business situation', 'software development practice', 'pros revenue management', 'opportunity analysis study', 'revenue management concept', 'revenue management', 'rm', 'rm system', 'software system', 'system']

Lower bounds on the information rate of secret sharing schemes with homogeneous
	access structure
We present some new lower bounds on the optimal information rate and on the
	optimal average information rate of secret sharing schemes with
	homogeneous access structure. These bounds are found by using some
	covering constructions and a new parameter, the k-degree of a
	participant, that is introduced in this paper. Our bounds improve the
	previous ones in almost all cases
['lower bounds', 'optimal information rate', 'optimal average information rate', 'k-degree', 'cryptography', 'information rate', 'secret sharing schemes', 'homogeneous access structure']
['secret sharing scheme', 'optimal average information rate', 'optimal information rate', 'new low bound', 'homogeneous access structure', 'access structure', 'new parameter', '-PRON- bound', 'previous one', 'participant']

A self-adjusting quality of service control scheme
We propose and analyze a self-adjusting Quality of Service (QoS) control scheme
	with the goal of optimizing the system reward as a result of servicing
	different priority clients with varying workload, QoS and
	reward/penalty requirements. Our scheme is based on resource
	partitioning and designated "degrade QoS areas" such that system
	resources are partitioned into priority areas each of which is reserved
	specifically to serve only clients in a corresponding class with no QoS
	degradation, plus one "degraded QoS area" into which all clients can be
	admitted with QoS adjustment being applied only to the lowest priority
	clients. We show that the best partition is dictated by the workload
	and the reward/penalty characteristics of clients in difference
	priority classes. The analysis results can be used by a QoS manager to
	optimize the system total reward dynamically in response to changing
	workloads at run time. We demonstrate the validity of our scheme by
	means of simulation and comparing the proposed QoS self-adjusting
	scheme with those that do not use resource partitioning or designated
	degraded QoS areas
['self-adjusting quality of service control scheme', 'priority clients', 'resourcepartitioning', 'simulation', 'multimedia systems', 'performance evaluation', 'resource reservation']
['system total reward', 'degraded qos area', 'degrade qos area', 'different priority client', 'service control scheme', 'qos area', 'self', 'control scheme', 'quality', 'priority area']

Fusion of qualitative bond graph and genetic algorithms: A fault diagnosis
	application
In this paper, the problem of fault diagnosis via integration of genetic
	algorithms (GA's) and qualitative bond graphs (QBG's) is addressed. We
	suggest that GA's can be used to search for possible fault components
	among a system of qualitative equations. The QBG is adopted as the
	modeling scheme to generate a set of qualitative equations. The
	qualitative bond graph provides a unified approach for modeling
	engineering systems, in particular, mechatronic systems. In order to
	demonstrate the performance of the proposed algorithm, we have tested
	the proposed algorithm on an in-house designed and built floating disc
	experimental setup. Results from fault diagnosis in the floating disc
	system are presented and discussed. Additional measurements will be
	required to localize the fault when more than one fault candidate is
	inferred. Fault diagnosis is activated by a fault detection mechanism
	when a discrepancy between measured abnormal behavior and predicted
	system behavior is observed. The fault detection mechanism is not
	presented here
['qualitative bond graph', 'genetic algorithms', 'fault diagnosis', 'fault components', 'qualitative equations', 'engineering systems', 'mechatronic systems', 'floating disc', 'measured abnormal behavior', 'predicted system behavior']
['qualitative bond graph', 'possible fault component', 'fault detection mechanism', 'fault diagnosis', 'qualitative equation', 'algorithm', 'genetic algorithm', 'modeling scheme', 'engineering system', 'mechatronic system']

There is no optimal routing policy for the torus
A routing policy is the method used to select a specific output channel for a
	message from among a number of acceptable output channels. An optimal
	routing policy is a policy that maximizes the probability of a message
	reaching its destination without delays. Optimal routing policies have
	been proposed for several regular networks, including the mesh and the
	hypercube. An open problem in interconnection network research has been
	the identification of an optimal routing policy for the torus. In this
	paper, we show that there is no optimal routing policy for the torus.
	Our result is demonstrated by presenting a detailed example in which
	the best choice of output channel is dependent on the probability of
	each channel being available. This result settles, in the negative, a
	conjecture by J. Wu (1996) concerning an optimal routing policy for the
	torus
['optimal routing policy', 'torus', 'hypercube']
['interconnection network research', 'policy', 'optimal routing policy', 'acceptable output channel', 'optimal', 'torus', 'specific output channel', 'output channel', 'message', 'method']

Optimal online algorithm for scheduling on two identical machines with machine
	availability constraints
This paper considers the online scheduling on two identical machines with
	machine availability constraints for minimizing makespan. We assume
	that machine M/sub j/ is unavailable during period from s/sub j/ to
	t/sub j/ (0 <or= s/sub j/ < t/sub j/), j = 1, 2, and the
	unavailable periods of two machines do not overlap. We show that the
	competitive ratio of list scheduling is 3. We further give an optimal
	algorithm with a competitive ratio 5/2
['optimal online algorithm', 'makespan minimisation', 'list scheduling', 'identicalmachines scheduling', 'machine availability constraints']
['machine availability constraint', 'optimal online algorithm', 'sub j/', 'identical machine', 'availability constraint', 'online scheduling', 'or= s', 'competitive ratio', 'machine m', 'machine']

An efficient retrieval selection algorithm for video servers with random
	duplicated assignment storage technique
Random duplicated assignment (RDA) is an approach in which video data is stored
	by assigning a number of copies of each data block to different,
	randomly chosen disks. It has been shown that this approach results in
	smaller response times and lower disk and RAM costs compared to the
	well-known disk stripping techniques. Based on this storage approach,
	one has to determine, for each given batch of data blocks, from which
	disk each of the data blocks is to be retrieved. This is to be done in
	such a way that the maximum load of the disks is minimized. The problem
	is called the retrieval selection problem (RSP). In this paper, we
	propose a new efficient algorithm for RSP. This algorithm is based on
	the breadth-first search approach and is able to guarantee optimal
	solutions for RSP in O(n/sup 2/+mn), where m and n correspond to the
	number of data blocks and the number of disks, respectively. We show
	that our proposed algorithm has a lower time complexity than an
	existing algorithm, called the MFS algorithm
['efficient retrieval selection algorithm', 'video servers', 'random duplicatedassignment storage technique', 'copies', 'data block', 'randomly chosendisks', 'response times', 'ram costs', 'disk costs', 'maximum load', 'breadth-first search', 'optimal solutions', 'time complexity']
['duplicated assignment storage technique', 'efficient retrieval selection algorithm', 'data block', 'disk', 'small response time', 'datum block', 'number', 'approach', 'ram cost', 'video server']

Edit distance of run-length encoded strings
Let X and Y be two run-length encoded strings, of encoded lengths k and l,
	respectively. We present a simple O(|X|l+|Y|k) time algorithm that
	computes their edit distance
['run-length encoded strings', 'encoded lengths', 'algorithm', 'edit distance', 'computation time']
['edit distance', 'length', 'run', 'string', 'simple o(|x|l+|y|k', 'compute -PRON- edit distance', 'time algorithm']

Fault-tolerant Hamiltonian laceability of hypercubes
It is known that every hypercube Q/sub n/ is a bipartite graph. Assume that
	n>or=2 and F is a subset of edges with |F|<or=n-2. We prove that
	there exists a Hamiltonian path in Q/sub n/-F between any two vertices
	of different partite sets. Moreover, there exists a path of length
	2/sup n/-2 between any two vertices of the same partite set. Assume
	that n>or=3 and F is a subset of edges with |F|<or=n-3. We prove
	that there exists a Hamiltonian path in Q/sub n/-{v}-F between any two
	vertices in the partite set without v. Furthermore, all bounds are
	tight
['fault-tolerant hamiltonian laceability', 'hypercubes', 'bipartite graph', 'edgesubset', 'hamiltonian path', 'vertices', 'partite sets', 'tight bounds']
['different partite set', 'tolerant hamiltonian laceability', 'hamiltonian path', 'sub n/-f', 'q', 'bipartite graph', 'hypercube q', '|f|<or', 'edge', 'subset']

An identity-based society oriented signature scheme with anonymous signers
In this paper, we propose a new society oriented scheme, based on the
	Guillou-Quisquater (1989) signature scheme. The scheme is
	identity-based and the signatures are verified with respect to only one
	identity. That is, the verifier does not have to know the identity of
	the co-signers, but just that of the organization they represent
['identity-based society oriented signature scheme', 'anonymous signers', 'signatureverification']
['signature scheme', 'anonymous signer', 'new society', 'identity', 'scheme', 'society', 'signer', 'signature', 'paper', 'quisquater']

Operational phase-space probability distribution in quantum communication
	theory
Operational phase-space probability distributions are useful tools for
	describing quantum mechanical systems, including quantum communication
	and quantum information processing systems. It is shown that quantum
	communication channels with Gaussian noise and quantum teleportation of
	continuous variables are described by operational phase-space
	probability distributions. The relation of operational phase-space
	probability distribution to the extended phase-space formalism proposed
	by Chountasis and Vourdas (1998) is discussed
['operational phase-space probability distribution', 'quantum communication theory', 'quantum mechanical systems', 'quantum information processing systems', 'gaussian noise', 'quantum teleportation', 'continuous variables', 'extendedphase-space formalism']
['space probability distribution', 'operational phase', 'quantum information processing system', 'quantum mechanical system', 'probability distribution', 'quantum communication', 'useful tool', 'gaussian noise', 'communication channel', 'quantum teleportation']

Embedding of level-continuous fuzzy sets on Banach spaces
In this paper we present an extension of the Minkowski embedding theorem,
	showing the existence of an isometric embedding between the classF/sub
	c/(X) of compact-convex and level-continuous fuzzy sets on a real
	separable Banach space X and C([0, 1] * B(X*)), the Banach space of
	real continuous functions defined on the cartesian product between [0,
	1] and the unit ball B(X*) in the dual space X*. Also, by using this
	embedding, we give some applications to the characterization of
	relatively compact subsets of F/sub c/(X). In particular, an
	Ascoli-Arzela type theorem is proved and applied to solving the Cauchy
	problem x(t) = f(t, x(t)), x(t/sub 0/) = x/sub 0/ on F/sub c/(X)
['isometric embedding', 'level-continuous fuzzy sets', 'compact-convex fuzzy sets', 'real separable banach space', 'real continuous functions', 'cartesianproduct', 'unit ball', 'dual space', 'ascoli-arzela type theorem', 'cauchyproblem']
['continuous fuzzy set', 'separable banach space x', 'minkowski embedding theorem', 'level', 'real continuous function', 'dual space x*.', 'sub', 'arzela type theorem', 'isometric embedding', 'cartesian product']

Correlation of intuitionistic fuzzy sets by centroid method
In this paper, we propose a method to calculate the correlation coefficient of
	intuitionistic fuzzy sets by means of "centroid". This value obtained
	from our formula tell us not only the strength of relationship between
	the intuitionistic fuzzy sets, but also whether the intuitionistic
	fuzzy sets are positively or negatively related. This approach looks
	better than previous methods which only evaluate the strength of the
	relation. Furthermore, we extend the "centroid" method to
	interval-valued intuitionistic fuzzy sets. The value of the correlation
	coefficient between interval-valued intuitionistic fuzzy sets lies in
	the interval [-1, 1], as computed from our formula
['correlation coefficient', 'intuitionistic fuzzy sets', 'centroid method', 'interval-valued intuitionistic fuzzy sets']
['intuitionistic fuzzy set', 'correlation', 'fuzzy set', 'centroid method', 'centroid', 'method', 'correlation coefficient', 'value', 'strength', 'coefficient']

Neighborhood operator systems and approximations
This paper presents a framework for the study of generalizing the standard
	notion of equivalence relation in rough set approximation space with
	various categories of k-step neighborhood systems. Based on a binary
	relation on a finite universe, six families of binary relations are
	obtained, and the corresponding six classes of k-step neighborhood
	systems are derived. Extensions of Pawlak's (1982) rough set
	approximation operators based on such neighborhood systems are
	proposed. Properties of neighborhood operator systems and rough set
	approximation operators are investigated, and their connections are
	examined
['neighborhood operator systems', 'equivalence relation', 'rough set approximationspace', 'k-step neighborhood systems', 'binary relation', 'finite universe']
['neighborhood operator system', 'step neighborhood system', 'step neighborhood', 'neighborhood system', 'rough', 'approximation space', 'finite universe', 'equivalence relation', 'binary relation', 'approximation']

Model predictive control helps to regulate slow processes-robust barrel
	temperature control
Slow temperature control is a challenging control problem. The problem becomes
	even more challenging when multiple zones are involved, such as in
	barrel temperature control for extruders. Often, strict closed-loop
	performance requirements (such as fast startup with no overshoot and
	maintaining tight temperature control during production) are given for
	such applications. When characteristics of the system are examined, it
	becomes clear that a commonly used proportional plus integral plus
	derivative (PID) controller cannot meet such performance specifications
	for this kind of system. The system either will overshoot or not
	maintain the temperature within the specified range during the
	production run. In order to achieve the required performance, a control
	strategy that utilizes techniques such as model predictive control,
	autotuning, and multiple parameter PID is formulated. This control
	strategy proves to be very effective in achieving the desired
	specifications, and is very robust
['model predictive control', 'slow processes regulation', 'robust barrel temperaturecontrol', 'extruders', 'autotuning', 'multiple parameter pid']
['model predictive control', 'slow temperature control', 'barrel temperature control', 'tight temperature control', 'temperature control', 'multiple parameter pid', 'control problem', 'multiple zone', 'fast startup', 'robust barrel']

Numerical representation of binary relations with a multiplicative error
	function
This paper studies the case of the representation of a binary relation via a
	numerical function with threshold (error) depending on both compared
	alternatives. The error is considered to be multiplicative, its value
	being either directly or inversely proportional to the values of the
	numerical function. For the first case, it is proved that a binary
	relation is a semiorder. Moreover, any semiorder can be represented in
	this form. In the second case, the corresponding binary relation is an
	interval order
['numerical representation', 'binary relations', 'multiplicative error function', 'numerical function', 'threshold', 'error', 'semiorder', 'interval order']
['binary relation', 'multiplicative error', 'numerical function', 'numerical representation', 'error', 'representation', 'case', 'binary', 'relation', 'function']

A pretopological approach for structural analysis
The aim of this paper is to present a methodological approach for problems
	encountered in structural analysis. This approach is based upon the
	pretopological concepts of pseudoclosure and minimal closed subsets.
	The advantage of this approach is that it provides a framework which is
	general enough to model and formulate different types of connections
	that exist between the elements of a population. In addition, it has
	enabled us to develop a new structural analysis algorithm. An
	explanation of the definitions and properties of the pretopological
	concepts applied in this work is first shown and illustrated in sample
	settings. The structural analysis algorithm is then described and the
	results obtained in an economic study of the impact of geographic
	proximity on scientific collaborations are presented
['pretopological approach', 'structural analysis', 'minimal closed subsets', 'pseudoclosure', 'connections', 'economic study', 'geographic proximity', 'scientific collaborations']
['new structural analysis algorithm', 'minimal closed subset', 'structural analysis algorithm', 'pretopological concept', 'methodological approach', 'pretopological approach', 'different type', 'approach', 'pretopological', 'concept']

On batch-constructing B/sup +/-trees: algorithm and its performance evaluation
Efficient construction of indexes is very important in bulk-loading a database
	or adding a new index to an existing database since both of them should
	handle an enormous volume of data. In this paper, we propose an
	algorithm for batch-constructing the B/sup +/-tree, the most widely
	used index structure in database systems. The main characteristic of
	our algorithm is to simultaneously process all the key values to be
	placed on each B+-tree page when accessing the page. This avoids the
	overhead due to accessing the same page multiple times, which results
	from applying the B+-tree insertion algorithm repeatedly. For
	performance evaluation, we have analyzed our algorithm in terms of the
	number of disk accesses. The results show that the number of disk
	accesses excluding those in the relocation process is identical to the
	number of pages belonging to the B/sup +/-tree. Considering that the
	relocation process is an unavoidable preprocessing step for
	batch-constructing of B/sup +/-trees, our algorithm requires just one
	disk access per B+-tree page, and therefore turns out to be optimal. We
	also present the performance tendency in relation with different
	parameter values via simulation. Finally, we show the performance
	enhancement effect of our algorithm, compared with the one using
	repeated insertions through experiments
['b+-tree batch construction', 'algorithm performance evaluation', 'database bulkloading', 'index structure', 'b+-tree page', 'page access', 'b+-tree insertionalgorithm', 'disk accesses', 'relocation process', 'simulation']
['performance evaluation', 'b+-tree insertion algorithm', 'b+-tree page', 'efficient construction', 'page multiple time', 'algorithm', 'new index', 'batch', 'sup', 'enormous volume']

Synthetic simultaneity - natural and artificial
In control loops, each element introduces time delays. If those time delays are
	larger than the critical times for control of the system, a problem
	exists. I show a simple approach to mitigating this problem by basing
	the controller's decisions not on the observations themselves but on
	our projections as to what the observations will be at the time our
	controls reach the controlled system. Finally, I argue that synthetic
	simultaneity explains Libet's (1993) results better than Libet's
	explanation
['control loops', 'time delays', 'critical times', 'controller decisions', 'observations', 'synthetic simultaneity']
['time delay', 'simple approach', 'critical time', 'synthetic simultaneity', 'control loop', 'synthetic', 'simultaneity', 'control', 'time', 'system']

MACLP: multi agent constraint logic programming
Multi agent systems (MAS) have become the key technology for decomposing
	complex problems in order to solve them more efficiently, or for
	problems distributed in nature. However, many industrial applications,
	besides their distributed nature, also involve a large number of
	parameters and constraints, i.e. they are combinatorial. Solving such
	particularly hard problems efficiently requires programming tools that
	combine MAS technology with a programming schema that facilitates the
	modeling and solution of constraints. This paper presents MACLP (multi
	agent constraint logic programming), a logic programming platform for
	building, in a declarative way, multi agent systems with
	constraint-solving capabilities. MACLP extends CSPCONS, a logic
	programming system that permits distributed program execution through
	communicating sequential Prolog processes with constraints, by
	providing all the necessary facilities for communication between
	agents. These facilities abstract from the programmer all the low-level
	details of the communication and allow him to focus on the development
	of the agent itself
['multi agent constraint logic programming', 'multi agent systems', 'parameters', 'combinatorial problems', 'hard problems', 'constraint solving', 'distributedprogram execution', 'communicating sequential prolog processes']
['multi agent constraint logic programming', 'agent constraint logic programming', 'multi agent system', 'maclp', 'combine mas technology', 'logic programming platform', 'complex problem', 'large number', 'programming tool', 'industrial application']

Self-organizing feature maps predicting sea levels
In this paper, a new method for predicting sea levels employing self-organizing
	feature maps is introduced. For that purpose the maps are transformed
	from an unsupervised learning procedure to a supervised one. Two
	concepts, originally developed to solve the problems of convergence of
	other network types, are proposed to be applied to Kohonen networks: a
	functional relationship between the number of neurons and the number of
	learning examples and a criterion to break off learning. The latter one
	can be shown to be conform with the process of self-organization by
	using U-matrices for visualization of the learning procedure. The
	predictions made using these neural models are compared for accuracy
	with observations and with the prognoses prepared using six models: two
	hydrodynamic models, a statistical model, a nearest neighbor model, the
	persistence model, and the verbal forecasts that are broadcast and kept
	on record by the Sea Level Forecast Service of the Federal Maritime and
	Hydrography Agency (BSH) in Hamburg. Before training the maps, the
	meteorological and oceanographic situation has to be condensed as well
	as possible, and the weight and learning vectors have to be made as
	small as possible. The self-organizing feature maps predict sea levels
	better than all six models of comparison
['self-organizing feature maps', 'sea level prediction', 'supervised learning', 'kohonen networks', 'neurons', 'u-matrices', 'visualization', 'hydrodynamicmodels', 'statistical model', 'nearest neighbor model', 'persistence model', 'verbal forecasts', 'sea level forecast service', 'federal maritime andhydrography agency', 'oceanographic situation', 'meteorological situation', 'learning vectors']
['feature map', 'sea level', 'sea level forecast service', 'new method', 'unsupervised learning procedure', 'self', 'near neighbor model', 'learning procedure', 'functional relationship', 'network type']

Selecting rail grade crossing investments with a decision support system
The Federal Railroad Administration (FRA) has developed a series of rail and
	rail-related analysis tools that assist FRA officials, Metropolitan
	Planning Organizations (MPOs), state Department of Transportation
	(DOT), and other constituents in evaluating the cost and benefits of
	potential infrastructure projects. To meet agency objectives, the FRA
	wants to add a high-speed rail grade crossing analysis tool to its
	package of rail and rail-related intermodal software products. This
	paper presents a conceptual decision support system (DSS) that can
	assist officials in achieving this goal. The paper first introduces the
	FRA's objectives and the role of cost benefit analysis in achieving
	these objectives. Next, there is a discussion of the models needed to
	assess the feasibility of proposed high-speed rail grade crossing
	investments and the presentation of a decision support system (DSS)
	that can deliver these models transparently to users. Then, the paper
	illustrates a system session and examines the potential benefits from
	system use
['rail grade crossing investment selection', 'decision support system', 'federalrailroad administration', 'metropolitan planning organizations', 'department of transportation', 'infrastructure projects', 'high-speed railgrade crossing analysis tool', 'rail-related intermodal softwareproducts', 'rail intermodal software products', 'cost benefit analysis']
['decision support system', 'federal railroad administration', 'speed rail grade', 'potential infrastructure project', 'conceptual decision support system', 'intermodal software product', 'analysis tool', 'rail grade', 'cost benefit analysis', 'investment']

A study on meaning processing of dialogue with an example of development of
	travel consultation system
This paper describes an approach to processing meaning instead of processing
	information in computing. Human intellectual activity is supported by
	linguistic activities in the brain. Therefore, processing the meaning
	of language instead of processing information should allow us to
	realize human intelligence on a computer. As an example of the proposed
	framework for processing meaning, we build a travel consultation
	dialogue system which can understand utterance by a user and retrieve
	information through dialogue. Through a simulation example of the
	system, we show that both information processing and language
	processing are integrated
['meaning processing', 'human intellectual activity', 'linguistic activities', 'travelconsultation dialogue system', 'user utterance understanding', 'informationretrieval', 'information processing', 'language processing']
['human intellectual activity', 'travel consultation system', 'processing meaning', 'travel consultation', 'processing', 'dialogue', 'example', 'linguistic activity', 'information', 'human intelligence']

From a biological to a computational model for the autonomous behavior of an
	animat
Endowing an autonomous system like a robot with intelligent behavior is
	difficult for several reasons. First, behavior is such a wide topic
	that a general framework paradigm of inspiration must be chosen in
	order to obtain a consistent model. Such a framework can be, for
	example, biological modeling or an artificial intelligence approach.
	Second, a general framework is not sufficient to determine a fully
	specified program to be implemented in a robot. Many choices, tuning
	and tests must be carried out before obtaining a robust system. A
	biological model is presented, based on the definition of cortex-like
	automata, representing elementary functions in the perceptive, motor or
	associative domain. These automata are connected in a network whose
	architecture, functioning and learning rules are described in a
	cortical framework. Second, the computational model derived from that
	biological model is specified. The way units exchange and compute
	variables through links is explained, with reference to corresponding
	biological elements. It is then easier to report experiments allowing
	an autonomous system to learn regularities of a simple environment and
	to exploit them to satisfy some internal drives. Even if additional
	biological hints can be added, this model allow us to better understand
	how a biological model can be implemented and how biological properties
	can emerge from a distributed set of units
['autonomous system', 'robot', 'autonomous behavior', 'intelligent behavior', 'animat', 'tuning', 'tests', 'robust system', 'biological model', 'cortex-like automata', 'elementary functions', 'perceptive domain', 'associative domain', 'motordomain', 'learning rules', 'architecture', 'computational model', 'variablecomputation', 'variable exchange', 'links', 'regularity learning', 'simpleenvironment', 'internal drives']
['computational model', 'way unit exchange', 'artificial intelligence approach', 'autonomous system', 'general framework paradigm', 'intelligent behavior', 'biological model', 'general framework', 'autonomous behavior', 'wide topic']

Nissan v. Nissan [trademark dispute]
Is a trademark dispute a case of David v. Goliath or a corporation fending off
	a greedy opportunist? This paper discusses the case of Uzi Nissan, who
	is locked in a multimillion-dollar legal battle over whether or not his
	use of the nissan.com Internet domain name infringes upon Japan's
	Nissan Motor Co.'s trademark. At the heart of the matter is the impact
	of the global Internet on trademark law, which traditionally has been
	strongly influenced by geographic considerations. The paper discusses
	the background to the case from both sides and the issues involved
['trademark dispute', 'uzi nissan', 'nissan.com internet domain name', 'nissan motorcompany trademark', 'global internet', 'trademark law']
['trademark dispute', 'nissan motor co.', 'dollar legal battle', 'case', 'uzi nissan', 'greedy opportunist', 'nissan', 'internet domain', 'trademark', 'trademark law']

Design PID controllers for desired time-domain or frequency-domain response
Practical requirements on the design of control systems, especially process
	control systems, are usually specified in terms of time-domain
	response, such as overshoot and rise time, or frequency-domain
	response, such as resonance peak and stability margin. Although
	numerous methods have been developed for the design of the
	proportional-integral-derivative (PID) controller, little work has been
	done in relation to the quantitative time-domain and frequency-domain
	responses. In this paper, we study the following problem: Given a
	nominal stable process with time delay, we design a suboptimal PID
	controller to achieve the required time-domain response or
	frequency-domain response for the nominal system or the uncertain
	system. An H/sub infinity / PID controller is developed based on
	optimal control theory and the parameters are derived analytically. Its
	properties are investigated and compared with that of two developed
	suboptimal controllers: an H/sub 2/ PID controller and a Maclaurin PID
	controller
['time-domain response', 'frequency-domain response', 'process control systems', 'overshoot', 'rise time', 'resonance peak', 'stability margin', 'proportional-integral derivative controller', 'nominal stable process', 'suboptimal controller', 'h/sub infinity / pid controller', 'h/sub 2/ pidcontroller', 'optimal control', 'maclaurin pid controller']
['design pid controller', 'optimal control theory', 'nominal stable process', 'domain response', 'pid controller', 'control system', 'practical requirement', 'domain', 'time', 'frequency']

Virtual borders, real laws [Internet activity and treaties]
National governments are working to tame activity on the Internet. They have
	worked steadily to extend control over online activities that they
	believe affect their interests, even when the activities occur outside
	their borders. These usually involve what governments regard as their
	domain: protecting public order, enforcing commercial laws, and,
	occasionally, protecting consumer interests. Methods have included
	assertions or legal jurisdiction based on where material is accessible
	instead of where it originates, and the blocking of sites, service
	providers, or entire high level domains from access by citizens. Such
	instances are mentioned in this article. Whilst larger companies are
	able to defend themselves against overseas lawsuits, individuals and
	smaller organizations lack the resources to defend what are often
	normal business activities at home, but could violate the laws of local
	jurisdictions in countries around the world. The problems of libel are
	discussed as are the blocking of certain sites by certain countries.
	Efforts to draw up Internet treaties are also mentioned
['national governments', 'internet activity', 'online activities', 'public orderprotection', 'commercial laws enforcement', 'legal jurisdiction', 'consumerinterests protection', 'internet sites blocking', 'lawsuits', 'internettreaties']
['entire high level domain', 'legal jurisdiction', 'normal business activity', 'consumer interest', 'tame activity', 'online activity', 'commercial law', 'national government', 'internet activity', 'virtual border']

A better ballot box?
Election officials are examining technologies to address a wide range of voting
	issues. The problems observed in the November 2000 US election
	accelerated existing trends to get rid of lever machines, punch-cards,
	and hand-counted paper ballots and replace them with mark-sense
	balloting, Internet, and automatic teller machine (ATM) kiosk style
	computer-based systems. An estimated US $2-$4 billion will be spent in
	the United States and Canada to update voting systems during the next
	decade. Voting online might enable citizens to vote even if they are
	unable to get to the polls. Yet making these methods work right turns
	out to be considerably more difficult than originally thought. New
	electronic voting systems pose risks as well as solutions. As it turns
	out, many of the voting products currently for sale provide less
	accountability, poorer reliability, and greater opportunity for
	widespread fraud than those already in use. This paper discusses the
	technology available and how to ensure accurate ballots
['ballot box', 'mark-sense balloting', 'automatic teller machine computer-basedvoting system', 'atm kiosk style computer-based voting systems', 'electronic voting', 'online voting']
['electronic voting system', 'automatic teller machine', 'good ballot box', 'paper ballot', 'lever machine', 'wide range', 'us election', 'election official', 'kiosk style', 'voting system']

Putting pen to screen on Tablet PCs
With the release of the first Tablet PCs produced to Microsoft Corp.'s general
	specifications, handheld computers may be about to leap into the ring
	with today's laptops. They will be about the size of the smaller
	laptops, will be at least as powerful, and maybe their biggest selling
	point-will be able to handle handwritten text. The Tablet PCs will be
	amply configured, general-purpose machines with more than enough power
	to run the full-blown Windows XP operating system. In particular, they
	will allow handwritten text to be entered onto a digitizing tablet and
	recognized, a functionality that's called pen-based computing. The
	Tablet PC will far outpace the computing power of existing small
	devices such as PDAs (personal digital assistants), including those
	variants based on Microsoft's own Pocket PC operating system
['tablet pc', 'microsoft', 'handheld computers', 'handwritten text', 'windows xpoperating system', 'digitizing tablet', 'pen-based computing']
['tablet pc', 'microsoft corp.', 'handheld computer', 'handwritten text', 'pocket pc operating system', 'windows xp operating system', 'purpose machine', 'pen', 'general', 'laptop']

Horizontal waypoint guidance design using optimal control
A horizontal waypoint guidance algorithm is proposed by applying line-following
	guidance to waypoint line segments in sequence. The line-following
	guidance is designed using an LQR (linear quadratic regulator). Then,
	the optimal waypoint changing points are derived by minimizing the
	accelerations required for changing the waypoint line segments. Also
	derived is a sufficient condition for the stability bound of ground
	speed changes based on the Lyapunov stability theorem. Simulation
	results show that the proposed algorithm can effectively guide a
	vehicle along the sequence of waypoint line segments
['horizontal waypoint guidance algorithm', 'line-following guidance', 'waypoint linesegments', 'lqr', 'linear quadratic regulator', 'optimal waypoint changingpoints', 'stability bound', 'ground speed changes', 'lyapunov stabilitytheorem', 'unmanned flying vehicle', 'threat avoidance', 'terrain masking', 'attack directions', 'target location arrival time']
['horizontal waypoint guidance algorithm', 'horizontal waypoint guidance design', 'waypoint line segment', 'linear quadratic regulator', 'sequence', 'optimal control', 'optimal waypoint', 'sufficient condition', 'speed change', 'lyapunov stability']

Separation and tracking of multiple broadband sources with one electromagnetic
	vector sensor
A structure for adaptively separating, enhancing and tracking uncorrelated
	sources with an electromagnetic vector sensor (EMVS) is presented. The
	structure consists of a set of parallel spatial processors, one for
	each individual source. Two stages of processing are involved in each
	spatial processor. The first preprocessing stage rejects all other
	sources except the one of interest, while the second stage is an
	adaptive one for maximizing the signal-to-noise ratio (SNR) and
	tracking the desired source. The preprocessings are designed using the
	latest source parameter estimates obtained from the source trackers,
	and a redesign is activated periodically or whenever any source has
	been detected by the source trackers to have made significant movement.
	Compared with conventional adaptive beamforming, the algorithm has the
	advantage that no a priori information on any desired signal location
	is needed, the sources are separated at maximum SNR, and their
	locations are available. The structure is also well suited for parallel
	implementation. Numerical examples are included to illustrate the
	capability and performance of the algorithm
['multiple broadband sources separation', 'multiple broadband sources tracking', 'uncorrelated sources', 'electromagnetic vector sensor', 'single em vectorsensor', 'parallel spatial processors', 'preprocessing stage', 'adaptivesecond stage', 'signal-to-noise ratio', 'snr maximization', 'maximum snr', 'signal source location', 'parallel implementation', 'adaptive sourceenhancement']
['late source parameter estimate', 'parallel spatial processor', 'electromagnetic vector sensor', 'multiple broadband source', 'conventional adaptive beamforming', 'structure', 'tracking', 'vector sensor', 'source tracker', 'spatial processor']

Recursive state estimation for multiple switching models with unknown
	transition probabilities
This work considers hybrid systems with continuous-valued target states and
	discrete-valued regime variable. The changes (switches) of the regime
	variable are modeled by a finite state Markov chain with unknown and
	random transition probabilities following Dirichlet distributions. Our
	work analytically derives the marginal posterior distribution of the
	states and regime variables, the transition probabilities being
	integrated out. This leads to a variety of recursive hybrid state
	estimation schemes which are an appealing intuitive and straightforward
	extension of standard algorithms. Their performance is illustrated by a
	maneuvering target tracking example
['recursive state estimation', 'multiple switching models', 'unknown transitionprobabilities', 'hybrid systems', 'continuous-valued target states', 'discrete-valued regime variable', 'finite state markov chain', 'randomtransition probabilities', 'dirichlet distributions', 'marginal posteriordistribution', 'maneuvering target tracking']
['finite state markov chain', 'multiple switching model', 'recursive state estimation', 'random transition probability', 'marginal posterior distribution', 'transition probability', 'regime variable', 'recursive hybrid state', 'hybrid system', 'target state']

MATLAB code for plotting ambiguity functions
A MATLAB code capable of plotting ambiguity functions of many different radar
	signals is presented. The program makes use of MATLAB's sparse matrix
	operations, and avoids loops. The program could be useful as a
	pedagogical tool in radar courses teaching pulse compression
['matlab code', 'ambiguity functions plotting', 'radar signals', 'sparse matrixoperations', 'pedagogical tool', 'radar courses', 'pulse compression', 'matched-filter response', 'doppler-shifted signal version']
['matlab code capable', 'ambiguity function', 'different radar', 'avoid loop', 'sparse matrix', 'program', 'pedagogical tool', 'radar course', 'pulse compression', 'operation']

Incremental motion control of linear synchronous motor
In this study a particular incremental motion control problem, which is
	specified by the trapezoidal velocity profile using multisegment
	sliding mode control (MSSMC), is proposed to control a permanent magnet
	linear synchronous motor (PMLSM) servo drive system. First, the
	structure and operating principle of the PMLSM are described in detail.
	Second, a field-oriented control PMLSM servo drive is introduced. Then,
	each segment of the multisegment switching surfaces is designed to
	match the corresponding part of the trapezoidal velocity profile, thus
	the motor dynamics on the specified-segment switching surface have the
	desired velocity or acceleration corresponding part of the trapezoidal
	velocity profile. In addition, the proposed control system is
	implemented in a PC-based computer control system. Finally, the
	effectiveness of the proposed PMLSM servo drive system is demonstrated
	by some simulated and experimental results
['incremental motion control', 'linear synchronous motor', 'trapezoidal velocityprofile', 'multisegment sliding mode control', 'permanent magnet motor', 'servo drive system', 'field-oriented control', 'multisegment switchingsurfaces', 'motor dynamics']
['linear synchronous motor', 'particular incremental motion control problem', 'trapezoidal velocity profile', 'control pmlsm servo drive', 'multisegment switching surface', 'velocity profile', 'mode control', 'permanent magnet', 'motor dynamic', 'computer control system']

Feedforward maximum power point tracking of PV systems using fuzzy controller
A feedforward maximum power (MP) point tracking scheme is developed for the
	interleaved dual boost (IDB) converter fed photovoltaic (PV) system
	using fuzzy controller. The tracking algorithm changes the duty ratio
	of the converter such that the solar cell array (SCA) voltage equals
	the voltage corresponding to the MP point at that solar insolation.
	This is done by the feedforward loop, which generates an error signal
	by comparing the instantaneous array voltage and reference voltage. The
	reference voltage for the feedforward loop, corresponding to the MP
	point, is obtained by an off-line trained neural network. Experimental
	data is used for off-line training of the neural network, which employs
	back-propagation algorithm. The proposed fuzzy feedforward peak power
	tracking effectiveness is demonstrated through the simulation and
	experimental results, and compared with the conventional proportional
	plus integral (PI) controller based system. Finally, a comparative
	study of interleaved boost and conventional boost converter for the PV
	applications is given and their suitability is discussed
['feedforward maximum power point tracking', 'pv systems', 'fuzzy controller', 'interleaved dual boost converter feed', 'photovoltaic system', 'trackingalgorithm', 'duty ratio', 'solar cell array voltage', 'solar insolation', 'feedforward loop', 'error signal', 'instantaneous array voltage', 'referencevoltage', 'off-line trained neural network', 'back-propagation algorithm', 'fuzzy feedforward peak power tracking effectiveness']
['feedforward maximum power point tracking', 'point tracking scheme', 'fuzzy controller', 'solar cell array', 'instantaneous array voltage', 'feedforward loop', 'reference voltage', 'pv', 'system', 'neural network']

A PID standard: What, why, how?
The paper is written for all who develop and use P&IDs. It will aid in solving
	the long existing and continuing problem of confusing information on
	P&IDs. The acronym P&ID is widely understood to mean the principal
	document used to define the details of how a process works and how it
	is controlled. The ISA Dictionary definition for P&ID tells what they
	do, "show the interconnection of process equipment and the
	instrumentation used to control the process. In the process industry a
	standard set of symbols is used to prepare drawings of processes. The
	instrument symbols used in these drawings are generally based on
	ISA-S5.1." In the paper the ISA standard is referred to as ISA-5.1. The
	article develops the concept of the "standard" and poses some of the
	questions that the "standard" can answer
['p&id standard', 'principal document', 'process controlled', 'isa-5.1', 'isa standard']
['isa dictionary definition', 'standard', 'pid standard', 'acronym p&id', 'paper', 'standard set', 'instrument symbol', 'process equipment', 'process industry', 'process']

Quantitative speed control for SRM drive using fuzzy adapted inverse model
Quantitative and robust speed control for a switched reluctance motor (SRM)
	drive is considered to be rather difficult and challenging owing to its
	highly nonlinear dynamic behavior. A speed control scheme having
	two-degree-of-freedom (2DOF) structure is developed here to improve the
	speed dynamic response of an SRM drive. In the proposed control scheme,
	the feedback controller is quantitatively designed to meet the desired
	regulation control requirements first. Then a reference model and a
	command feedforward controller based on an inverse plant model are
	employed to yield the desired tracking response at nominal case. As the
	variations of system parameters and operating conditions occur, the
	prescribed control specifications may not be satisfied any more. To
	improve this, the inverse model is adaptively tuned by a fuzzy control
	scheme so that the model-following tracking error is significantly
	reduced. In addition, a simple disturbance cancellation robust
	controller is added to improve the tracking and regulation control
	performances further
['quantitative speed control', 'srm drive', 'fuzzy adapted inverse model', 'switchedreluctance motor', 'nonlinear dynamic behavior', 'two-degree-of-freedomstructure', 'speed dynamic response', 'regulation control requirements', 'reference model', 'command feedforward controller', 'inverse plant model', 'tracking response', 'system parameters', 'operating conditions', 'controlspecifications', 'fuzzy control scheme', 'model-following tracking error', 'disturbance cancellation controller']
['simple disturbance cancellation robust', 'prescribed control specification', 'inverse plant model', 'command feedforward controller', 'speed control scheme', 'speed dynamic response', 'regulation control requirement', 'quantitative speed control', 'switched reluctance motor', 'nonlinear dynamic behavior']

Robust fuzzy controlled photovoltaic power inverter with Taguchi method
This paper presents design and implementation of a robust fuzzy controlled
	photovoltaic (PV) power inverter with Taguchi tuned scaling factors. To
	achieve fast transient response, small steady-state error and system
	robustness, a robust fuzzy controller is adopted, in which its input
	and output scaling factors are determined efficiently by using the
	Taguchi-tuning algorithm. The proposed system can operate in different
	modes, grid-connection mode and stand-alone mode, and can accommodate
	wide load variations. Simulation results and hardware measurements
	obtained from a prototype with a microcontroller (Intel 80196KC) are
	presented to verify the theoretical discussions, and its adaptivity,
	robustness and feasibility
['robust fuzzy controlled photovoltaic power inverter', 'taguchi method', 'tunedscaling factors', 'transient response', 'steady-state error', 'systemrobustness', 'output scaling factors', 'grid-connection mode', 'stand-alonemode', 'load variations', 'microcontroller', 'adaptivity', 'feasibility']
['wide load variation', 'output scaling factor', 'robust fuzzy', 'robust fuzzy controller', 'photovoltaic power inverter', 'fast transient response', 'power inverter', 'taguchi method', 'scaling factor', 'state error']

Robust wavelet neuro control for linear brushless motors
Design, simulation and experimental implementation of a wavelet basis function
	network learning controller for linear brushless dc motors (LBDCM) are
	considered. Stability robustness with position tracking is the primary
	concern. The proposed controller deals mainly with external
	disturbances, e.g. nonlinear friction force and payload variation in
	motion control of linear motors. It consists of two parts, one is a
	state feedback component, and the other one is a learning feedback
	component. The state feedback controller is designed on the basis of a
	simple linear model, and the learning feedback component is a wavelet
	neural controller. The attenuation effect of wavelet neural networks on
	friction force is first verified by the numerical method. The learning
	effect of wavelet neural networks on friction force is also shown in
	the numerical results. Then, a wavelet neural network is applied on a
	real LBDCM to on-line suppress the friction force, which may be
	variable due to the different lubrication. The effectiveness of the
	proposed control schemes is demonstrated by simulated and experimental
	results
['robust wavelet neuro control', 'linear brushless motors', 'wavelet basis functionnetwork', 'lbdcm', 'stability robustness', 'position tracking', 'externaldisturbances', 'nonlinear friction force', 'payload variation', 'motioncontrol', 'state feedback component', 'learning feedback component', 'attenuation effect', 'friction force', 'lubrication']
['linear brushless dc motor', 'robust wavelet neuro control', 'wavelet neural network', 'friction force', 'linear brushless motor', 'wavelet basis function', 'learning feedback component', 'state feedback component', 'state feedback controller', 'simple linear model']

Outlier resistant adaptive matched filtering
Robust adaptive matched filtering (AMF) whereby outlier data vectors are
	censored from the covariance matrix estimate is considered in a maximum
	likelihood estimation (MLE) setting. It is known that outlier data
	vectors whose steering vector is highly correlated with the desired
	steering vector, can significantly degrade the performance of AMF
	algorithms such as sample matrix inversion (SMI) or fast maximum
	likelihood (FML). Four new algorithms that censor outliers are
	presented which are derived via approximation to the MLE solution. Two
	algorithms each are related to using the SMI or the FML to estimate the
	unknown underlying covariance matrix. Results are presented using
	computer simulations which demonstrate the relative effectiveness of
	the four algorithms versus each other and also versus the SMI and FML
	algorithms in the presence of outliers and no outliers. It is shown
	that one of the censoring algorithms, called the reiterative censored
	fast maximum likelihood (CFML) technique is significantly superior to
	the other three censoring methods in stressful outlier scenarios
['outlier resistant adaptive matched filtering', 'covariance matrix estimate', 'maximum likelihood estimation setting', 'steering vector', 'sample matrixinversion', 'fast maximum likelihood', 'censoring algorithms', 'reiterativecensored fast maximum likelihood']
['new algorithm that censor outlier', 'vector whose steering vector', 'covariance matrix estimate', 'outlier resistant adaptive', 'outlier datum vector', 'sample matrix inversion', 'covariance matrix', 'outlier datum', 'likelihood estimation', 'steering vector']

Brightness-independent start-up routine for star trackers
Initial attitude acquisition by a modern star tracker is investigated here.
	Criteria for efficient organization of the on-board database are
	discussed with reference to a brightness-independent initial
	acquisition algorithm. Star catalog generation preprocessing is
	described, with emphasis on the identification of minimum star
	brightness for detection by a sensor based on a charge coupled device
	(CCD) photodetector. This is a crucial step for proper evaluation of
	the attainable sky coverage when selecting the stars to be included in
	the on-board catalog. Test results are also reported, both for
	reliability and accuracy, even if the former is considered to be the
	primary target. Probability of erroneous solution is 0.2% in the case
	of single runs of the procedure, while attitude determination accuracy
	is in the order of 0.02 degrees in the average for the computation of
	the inertial pointing of the boresight axis
['brightness-independent start-up routine', 'star trackers', 'initial attitudeacquisition', 'on-board database', 'star catalog generation preprocessing', 'gyroless spacecraft', 'minimum star brightness', 'charge coupled devicephotodetector', 'reliability', 'boresight axis']
['attitude determination accuracy', 'attainable sky coverage', 'charge coupled device', 'star catalog generation', 'modern star tracker', 'initial attitude acquisition', 'star tracker', 'brightness', 'acquisition algorithm', 'independent start']

Multiple model adaptive estimation with filter spawning
Multiple model adaptive estimation (MMAE) with filter spawning is used to
	detect and estimate partial actuator failures on the VISTA F-16. The
	truth model is a full six-degree-of-freedom simulation provided by
	Calspan and General Dynamics. The design models are chosen as 13-state
	linearized models, including first order actuator models. Actuator
	failures are incorporated into the truth model and design model
	assuming a "failure to free stream." Filter spawning is used to include
	additional filters with partial actuator failure hypotheses into the
	MMAE bank. The spawned filters are based on varying degrees of partial
	failures (in terms of effectiveness) associated with the
	complete-actuaton-failure hypothesis with the highest conditional
	probability of correctness at the current time. Thus, a blended
	estimate of the failure effectiveness is found using the filters'
	estimates based upon a no-failure hypothesis, a complete actuator
	failure hypothesis, and the spawned filters' partial-failure
	hypotheses. This yields substantial precision in effectiveness
	estimation, compared with what is possible without spawning additional
	filters, making partial failure adaptation a viable methodology
['multiple model adaptive estimation', 'filter spawning', 'partial actuator failures', 'vista f-16', 'truth model', 'six-degree-of-freedom simulation', 'calspan', 'in-flight simulator', 'test aircraft', 'flight control systems', 'generaldynamics', 'linearized models', 'mmae', 'partial failures', 'conditionalprobability', 'no-failure hypothesis']
['multiple model adaptive estimation', 'filter spawning', 'partial actuator failure hypothesis', 'partial actuator failure', 'filter', 'order actuator model', 'truth model', 'failure hypothesis', 'design model', 'mmae bank']

Matched-filter template generation via spatial filtering: application to fetal
	biomagnetic recordings
We have developed a two-step procedure for signal processing of fetal
	biomagnetic recordings that removes cardiac interference and noise.
	First, a modified matched filter (MF) is applied to remove maternal
	cardiac interference; then, a simple signal space projection (SSP) is
	applied to remove noise. The key difference between our MF and a
	conventional one is that the interference template and the template
	scaling are derived from a signal that has been spatially filtered to
	isolate the interference, rather than from the raw signal. Unlike
	conventional MFs, ours is able to separate maternal and fetal cardiac
	complexes, even when they have similar morphology and overlap strongly.
	When followed by a SSP that preserves only the signal subspace, the
	noise is reduced to a low level
['maternal cardiac interference removal', 'simple signal space projection', 'noiseremoval', 'signal subspace preservation', 'fetal magnetocardiography', 'spatial filtering', 'interference template', 'raw signal', 'template scaling', 'modified matched filter', 'maternal cardiac interference']
['simple signal space projection', 'filter template generation', 'biomagnetic recording', 'cardiac interference', 'signal processing', 'step procedure', 'fetal', 'noise', 'spatial filtering', 'key difference']

Design and implementation of a brain-computer interface with high transfer
	rates
This paper presents a brain-computer interface (BCI) that can help users to
	input phone numbers. The system is based on the steady-state visual
	evoked potential (SSVEP). Twelve buttons illuminated at different rates
	were displayed on a computer monitor. The buttons constituted a virtual
	telephone keypad, representing the ten digits 0-9, BACKSPACE, and
	ENTER. Users could input phone number by gazing at these buttons. The
	frequency-coded SSVEP was used to judge which button the user desired.
	Eight of the thirteen subjects succeeded in ringing the mobile phone
	using the system. The average transfer rate over all subjects was 27.15
	bits/min. The attractive features of the system are noninvasive signal
	recording, little training required for use, and high information
	transfer rate. Approaches to improve the performance of the system are
	discussed
['brain-computer interface with high transfer rates', 'phone numbers input', 'steady-state visual evoked potential', 'illuminated buttons', 'systemperformance improvement', 'virtual telephone keypad', 'frequency-codedssvep', 'mobile phone ringing', 'computer monitor']
['average transfer rate', 'input phone number', 'computer interface', 'phone number', 'high transfer', 'system', 'telephone keypad', 'state visual', 'computer monitor', 'different rate']

Noninvasive myocardial activation time imaging: a novel inverse algorithm
	applied to clinical ECG mapping data
Linear approaches like the minimum-norm least-square algorithm show
	insufficient performance when it comes to estimating the activation
	time map on the surface of the heart from electrocardiographic (ECG)
	mapping data. Additional regularization has to be considered leading to
	a nonlinear problem formulation. The Gauss-Newton approach is one of
	the standard mathematical tools capable of solving this kind of
	problem. To our experience, this algorithm has specific drawbacks which
	are caused by the applied regularization procedure. In particular,
	under clinical conditions the amount of regularization cannot be
	determined clearly. For this reason, we have developed an iterative
	algorithm solving this nonlinear problem by a sequence of regularized
	linear problems. At each step of iteration, an individual L-curve is
	computed. Subsequent iteration steps are performed with the individual
	optimal regularization parameter. This novel approach is compared with
	the standard Gauss-Newton approach. Both methods are applied to
	simulated ECG mapping data as well as to single beat sinus rhythm data
	from two patients recorded in the catheter laboratory. The proposed
	approach shows excellent numerical and computational performance, even
	under clinical conditions at which the Gauss-Newton approach begins to
	break down
['noninvasive myocardial activation time imaging', 'electrodiagnostics', 'activationtime imaging', 'l-curve method', 'noninvasive electrocardiography', 'tikhonovregularization', 'gauss-newton approach', 'individual optimalregularization parameter', 'catheter laboratory', 'clinical conditions', 'iteration steps', 'heart surface', 'regularization procedure', 'inversealgorithm', 'clinical ecg mapping data']
['ecg mapping datum', 'newton approach', 'mapping datum', 'nonlinear problem', 'clinical condition', 'tool capable', 'mathematical tool', 'standard mathematical', 'regularization procedure', 'problem formulation']

Bayesian nonstationary autoregressive models for biomedical signal analysis
We describe a variational Bayesian algorithm for the estimation of a
	multivariate autoregressive model with time-varying coefficients that
	adapt according to a linear dynamical system. The algorithm allows for
	time and frequency domain characterization of nonstationary
	multivariate signals and is especially suited to the analysis of
	event-related data. Results are presented on synthetic data and real
	electroencephalogram data recorded in event-related desynchronization
	and photic synchronization scenarios
['photic synchronization scenarios', 'event-related desynchronization', 'frequencydomain characterization', 'time domain characterization', 'biomedicalsignal analysis', 'kalman smoother', 'eeg analysis', 'bayesian nonstationaryautoregressive models', 'linear dynamical system', 'variational bayesianalgorithm', 'time-varying coefficients']
['bayesian nonstationary autoregressive model', 'multivariate autoregressive model', 'variational bayesian algorithm', 'biomedical signal analysis', 'linear dynamical system', 'frequency domain characterization', 'multivariate signal', 'time', 'event', 'synthetic datum']

Supervisory control design based on hybrid systems and fuzzy events detection.
	Application to an oxichlorination reactor
This paper presents a supervisory control scheme based on hybrid systems theory
	and fuzzy events detection. The fuzzy event detector is a linguistic
	model, which synthesizes complex relations between process variables
	and process events incorporating experts' knowledge about the process
	operation. This kind of detection allows the anticipation of
	appropriate control actions, which depend upon the selected membership
	functions used to characterize the process under scrutiny. The proposed
	supervisory control scheme was successfully implemented for an
	oxichlorination reactor in a vinyl monomer plant
['supervisory control design', 'hybrid systems', 'events detection. fuzzy', 'oxichlorination reactor', 'linguistic model', 'complex relations', 'processvariables', 'process events', 'expert knowledge', 'process operation', 'controlactions', 'membership functions', 'vinyl monomer plant', 'reactor stability', 'raw material consumption', 'discrete events systems', 'reactive systems', 'finite state machines']
['fuzzy event detection', 'supervisory control scheme', 'hybrid system theory', 'fuzzy event detector', 'supervisory control design', 'appropriate control action', 'oxichlorination reactor', 'hybrid system', 'process event', 'process variable']

Automated breath detection on long-duration signals using feedforward
	backpropagation artificial neural networks
A new breath-detection algorithm is presented, intended to automate the
	analysis of respiratory data acquired during sleep. The algorithm is
	based on two independent artificial neural networks (ANN/sub insp/ and
	ANN/sub expi/) that recognize, in the original signal, windows of
	interest where the onset of inspiration and expiration occurs.
	Postprocessing consists in finding inside each of these windows of
	interest minimum and maximum corresponding to each inspiration and
	expiration. The ANN/sub insp/ and ANN/sub expi/ correctly determine
	respectively 98.0% and 98.7% of the desired windows, when compared with
	29 820 inspirations and 29 819 expirations detected by a human expert,
	obtained from three entire-night recordings. Postprocessing allowed
	determination of inspiration and expiration onsets with a mean
	difference with respect to the same human expert of (mean +or- SD) 34
	+or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method
	proved to be effective in detecting the onset of inspiration and
	expiration in full night continuous recordings. A comparison of five
	human experts performing the same classification task yielded that the
	automated algorithm was undifferentiable from these human experts,
	failing within the distribution of human expert results. Besides being
	applicable to adult respiratory volume data, the presented algorithm
	was also successfully applied to infant sleep data, consisting of
	uncalibrated rib cage and abdominal movement recordings. A comparison
	with two previously published algorithms for breath detection in
	respiratory volume signal shows that the presented algorithm has a
	higher specificity, while presenting similar or higher positive
	predictive values
['respiratory movements', 'automated breath detection', 'postprocessing', 'inspiration', 'expiration', 'automated algorithm', 'human experts', 'entire-nightrecordings', 'uncalibrated rib cage', 'abdominal movement recordings', 'infant sleep data', 'adult respiratory volume data', 'long-durationsignals', 'feedforward backpropagation artificial neural networks', '34 ms', '5 ms']
['artificial neural network', 'breath detection', 'human expert', 'sub insp/', 'duration signal', 'backpropagation artificial', 'detection algorithm', 'new breath', 'sub expi/', 'respiratory datum']

Model selection in electromagnetic source analysis with an application to VEFs
In electromagnetic source analysis, it is necessary to determine how many
	sources are required to describe the electroencephalogram or
	magnetoencephalogram adequately. Model selection procedures (MSPs) or
	goodness of fit procedures give an estimate of the required number of
	sources. Existing and new MSPs are evaluated in different source and
	noise settings: two sources which are close or distant and noise which
	is uncorrelated or correlated. The commonly used MSP residual variance
	is seen to be ineffective, that is it often selects too many sources.
	Alternatives like the adjusted Hotelling's test, Bayes information
	criterion and the Wald test on source amplitudes are seen to be
	effective. The adjusted Hotelling's test is recommended if a
	conservative approach is taken and MSPs such as Bayes information
	criterion or the Wald test on source amplitudes are recommended if a
	more liberal approach is desirable. The MSPs are applied to empirical
	data (visual evoked fields)
['model selection', 'electromagnetic source analysis', 'noise settings', 'residualvariance', 'wald test', "adjusted hotelling's test", 'empirical data', 'vefs', 'meg source analysis', 'eeg source analysis', 'goodness-of-fit', 'sourcelocalization', 'visual evoked fields']
['electromagnetic source analysis', 'msp residual variance', 'model selection procedure', 'model selection', 'adjusted hotelling', 'bayes information', 'wald test', 'source amplitude', 'noise setting', 'different source']

Time-varying properties of renal autoregulatory mechanisms
In order to assess the possible time-varying properties of renal
	autoregulation, time-frequency and time-scaling methods were applied to
	renal blood flow under broad-band forced arterial blood pressure
	fluctuations and single-nephron renal blood flow with spontaneous
	oscillations obtained from normotensive (Sprague-Dawley, Wistar, and
	Long-Evans) rats, and spontaneously hypertensive rats. Time-frequency
	analyses of normotensive and hypertensive blood flow data obtained from
	either the whole kidney or the single-nephron show that indeed both the
	myogenic and tubuloglomerular feedback (TGF) mechanisms have
	time-varying characteristics. Furthermore, we utilized the Renyi
	entropy to measure the complexity of blood-flow dynamics in the
	time-frequency plane in an effort to discern differences between
	normotensive and hypertensive recordings. We found a clear difference
	in Renyi entropy between normotensive and hypertensive blood flow
	recordings at the whole kidney level for both forced (p < 0.037) and
	spontaneous arterial pressure fluctuations (p < 0.033), and at the
	single-nephron level (p < 0.008). Especially at the single-nephron
	level, the mean Renyi entropy is significantly larger for hypertensive
	than normotensive rats, suggesting more complex dynamics in the
	hypertensive condition. To further evaluate whether or not the
	separation of dynamics between normotensive and hypertensive rats is
	found in the prescribed frequency ranges of the myogenic and TGF
	mechanisms, we employed multiresolution wavelet transform. Our analysis
	revealed that exclusively over scale ranges corresponding to the
	frequency intervals of the myogenic and TGF mechanisms, the widths of
	the blood flow wavelet coefficients fall into disjoint sets for
	normotensive and hypertensive rats. The separation of the scales at the
	myogenic and TGF frequency ranges is distinct and obtained with 100%
	accuracy. However, this observation remains valid only for the whole
	kidney blood pressure/flow data. The results suggest that understanding
	of the time-varying properties of the two mechanisms is required for a
	complete description of renal autoregulation
['time-varying properties', 'sprague-dawley rats', 'wistar rats', 'long-evans rats', 'whole kidney', 'single-nephron', 'renyi entropy', 'spontaneous arterialpressure fluctuations', 'hypertensive rats', 'normotensive rats', 'renalautoregulatory mechanisms', 'broad-band forced arterial blood pressurefluctuations', 'single-nephron renal blood flow', 'spontaneous oscillations']
['hypertensive blood flow', 'renal blood flow', 'time', 'hypertensive rat', 'blood pressure', 'flow datum', 'blood flow', 'renyi entropy', 'mechanism', 'nephron renal']

The use of the SPSA method in ECG analysis
The classification, monitoring, and compression of electrocardiogram (ECG)
	signals recorded of a single patient over a relatively long period of
	time is considered. The particular application we have in mind is
	high-resolution ECG analysis, such as late potential analysis,
	morphology changes in QRS during arrythmias, T-wave alternants, or the
	study of drug effects on ventricular activation. We propose to apply a
	modification of a classical method of cluster analysis or vector
	quantization. The novelty of our approach is that we use a new
	distortion measure to quantify the distance of two ECG cycles, and the
	class-distortion measure is defined using a min-max criterion. The new
	class-distortion-measure is much more sensitive to outliers than the
	usual distortion measures using average-distance. The price of this
	practical advantage is that computational complexity is significantly
	increased. The resulting nonsmooth optimization problem is solved by an
	adapted version of the simultaneous perturbation stochastic
	approximation (SPSA) method of J. Spall (IEEE Trans. Automat. Contr.,
	vol. 37, p. 332-41, Mar. 1992). The main idea is to generate a smooth
	approximation by a randomization procedure. The viability of the method
	is demonstrated on both simulated and real data. An experimental
	comparison with the widely used correlation method is given on real
	data
['class-distortion-measure', 'nonsmooth optimization problem', 'simultaneousperturbation stochastic approximation method', 'cluster analysis', 'randomization procedure', 'correlation method', 'electrodiagnostics', 'ecgsignals compression', 'distortion measure', 'ecg cycles']
['distortion measure', 'ecg analysis', 'method', 'drug effect', 'wave alternant', 'morphology change', 'potential analysis', 'long period', 'late potential', 'single patient']

Model intestinal microflora in computer simulation: a simulation and modeling
	package for host-microflora interactions
The ecology of the human intestinal microflora and its interaction with the
	host are poorly understood. Though more and more data are being
	acquired, in part using modern molecular methods, development of a
	quantitative theory has not kept pace with this increase in observing
	power. This is in part due to the complexity of the system and to the
	lack of simulation environments in which to test what the ecological
	effect of a hypothetical mechanism of interaction would be, before
	resorting to laboratory experiments. The MIMICS project attempts to
	address this through the development of a cellular automaton for
	simulation of the intestinal microflora. In this paper, the design and
	evaluation of this simulator is discussed
['human intestines', 'intestinal microflora', 'molecular methods', 'observing power', 'system complexity', 'microbial ecology', 'parallel computing', 'host-microflora interactions', 'quantitative theory', 'mimics project', 'complex microbial ecosystem']
['modern molecular method', 'model intestinal microflora', 'human intestinal microflora', 'intestinal microflora', 'computer simulation', 'microflora interaction', 'simulation', 'interaction', 'quantitative theory', 'host']

Conformal-mapping design tools for coaxial couplers with complex cross section
Numerical conformal mapping is exploited as a simple, accurate, and efficient
	tool for the analysis and design of coaxial waveguides and couplers of
	complex cross section. An implementation based on the
	Schwarz-Christoffel Toolbox, a public-domain MATLAB package, is applied
	to slotted coaxial cables and to symmetrical coaxial couplers, with
	circular or polygonal inner conductors and external shields. The effect
	of metallic diaphragms of arbitrary thickness, partially separating the
	inner conductors, is also easily taken into account. The proposed
	technique is validated against the results of the finite-element
	method, showing excellent agreement at a fraction of the computational
	cost, and is also extended to the case of nonsymmetrical couplers,
	providing the designer with important additional degrees of freedom
['conformal mapping design tools', 'coaxial couplers', 'complex cross section', 'coaxial waveguides', 'schwarz-christoffel toolbox', 'public-domain matlabpackage', 'slotted coaxial cables', 'symmetrical couplers', 'circular innerconductors', 'polygonal inner conductors', 'external shields', 'metallicdiaphragms', 'nonsymmetrical couplers', 'numerical conformaltransformations']
['complex cross section', 'polygonal inner conductor', 'important additional degree', 'numerical conformal mapping', 'symmetrical coaxial coupler', 'slotted coaxial cable', 'domain matlab package', 'mapping design tool', 'coaxial coupler', 'coaxial waveguide']

Convolution-based global simulation technique for millimeter-wave photodetector
	and photomixer circuits
A fast convolution-based time-domain approach to global photonic-circuit
	simulation is presented that incorporates a physical device model in
	the complete detector or mixer circuit. The device used in the
	demonstration of this technique is a GaAs metal-semiconductor-metal
	(MSM) photodetector that offers a high response speed for the detection
	and generation of millimeter waves. Global simulation greatly increases
	the accuracy in evaluating the complete circuit performance because it
	accounts for the effects of the millimeter-wave embedding circuit.
	Device and circuit performance are assessed by calculating optical
	responsivity and bandwidth. Device-only simulations using GaAs MSMs are
	compared with global simulations that illustrate the strong
	interdependence between device and external circuit
['convolution-based time-domain approach', 'global photonic-circuit simulation', 'physical device model', 'gaas msm photodetector', 'millimeter-wavephotodetector', 'photomixer', 'mm-wave embedding circuit', 'opticalresponsivity', 'bandwidth', 'convolution-based global simulation', 'gaas']
['complete circuit performance', 'high response speed', 'physical device model', 'global simulation', 'global simulation technique', 'global photonic', 'fast convolution', 'wave photodetector', 'photomixer circuit', 'domain approach']

Accurate modeling of lossy nonuniform transmission lines by using differential
	quadrature methods
This paper discusses an efficient numerical approximation technique, called the
	differential quadrature method (DQM), which has been adapted to model
	lossy uniform and nonuniform transmission lines. The DQM can quickly
	compute the derivative of a function at any point within its bounded
	domain by estimating a weighted linear sum of values of the function at
	a small set of points belonging to the domain. Using the DQM, the
	frequency-domain Telegrapher's partial differential equations for
	transmission lines can be discretized into a set of easily solvable
	algebraic equations. DQM reduces interconnects into multiport models
	whose port voltages and currents are related by rational formulas in
	the frequency domain. Although the rationalization process in DQM is
	comparable with the Pade approximation of asymptotic waveform
	evaluation (AWE) applied to transmission lines, the derivation
	mechanisms in these two disparate methods are significantly different.
	Unlike AWE, which employs a complex moment-matching process to obtain
	rational approximation, the DQM requires no approximation of
	transcendental functions, thereby avoiding the process of moment
	generation and moment matching. Due to global sampling of points in the
	DQM approximation, it requires far fewer grid points in order to build
	accurate discrete models than other numerical methods do. The DQM-based
	time-domain model can be readily integrated in a circuit simulator like
	SPICE
['lossy nonuniform transmission lines', 'differential quadrature method', 'numericalapproximation technique', 'frequency-domain telegrapher pde', 'partialdifferential equations', 'algebraic equations', 'interconnects', 'multiportmodels', 'multiconductor transmission lines', 'rationalization process', 'time-domain model']
['nonuniform transmission line', 'transmission line', 'quadrature method', 'approximation technique', 'dqm', 'numerical approximation', 'differential quadrature', 'efficient numerical', 'lossy uniform', 'accurate modeling']

An unconditionally stable extended (USE) finite-element time-domain solution of
	active nonlinear microwave circuits using perfectly matched layers
This paper proposes an extension of the unconditionally stable finite-element
	time-domain (FETD) method for the global electromagnetic analysis of
	active microwave circuits. This formulation has two advantages. First,
	the time-step size is no longer governed by the spatial discretization
	of the mesh, but rather by the Nyquist sampling criterion. Second, the
	implementation of the truncation by the perfectly matched layers (PML)
	is straightforward. An anisotropic PML absorbing material is presented
	for the truncation of FETD lattices. Reflection less than -50 dB is
	obtained numerically over the entire propagation bandwidth in waveguide
	and microstrip line. A benchmark test on a microwave amplifier
	indicates that this extended FETD algorithm is not only superior to
	finite-difference time-domain-based algorithm in mesh flexibility and
	simulation accuracy, but also reduces computation time dramatically
['unconditionally stable fetd method', 'finite-element time-domain method', 'globalelectromagnetic analysis', 'global em analysis', 'active nonlinearmicrowave circuits', 'nyquist sampling criterion', 'time-step size', 'pmltruncation', 'perfectly matched layers', 'anisotropic pml absorbingmaterial', 'fetd lattices truncation', 'waveguide', 'microstrip line', 'microwave amplifier', 'mesh flexibility', 'simulation accuracy', 'computationtime reduction']
['active nonlinear microwave circuit', 'active microwave circuit', 'global electromagnetic analysis', 'unconditionally stable finite', 'nyquist sampling criterion', 'domain solution', 'element time', 'entire propagation bandwidth', 'domain', 'stable extended']

Low-voltage DRAM sensing scheme with offset-cancellation sense amplifier
A novel bitline sensing scheme is proposed for low-voltage DRAM to achieve low
	power dissipation and compatibility with low-voltage CMOS. One of the
	major obstacles in low-voltage DRAM is the degradation of
	data-retention time due to low signal level at the memory cell, which
	requires power-consuming refresh operations more frequently. This paper
	proposes an offset-cancellation sense-amplifier scheme (OCSA) that
	improves data-retention time significantly even at low supply voltage.
	It also improves die efficiency, because the proposed scheme reduces
	the number of sense amplifiers by supporting more cells in each sense
	amplifier. Measurements show that the data-retention time of the
	proposed scheme at 1.5-V supply voltage is 2.4 times of the
	conventional scheme at 2.0 V
['lv dram sensing scheme', 'low-voltage sensing scheme', 'offset-cancellation senseamplifier scheme', 'bitline sensing scheme', 'low power dissipation', 'low-voltage cmos compatibility', 'data-retention time', 'memory cell', 'differential amplifier configuration', 'power-consuming refreshoperations', 'sensing margin', '1.5 v']
['voltage dram sensing scheme', 'cancellation sense amplifier', 'voltage dram', 'retention time', 'sense amplifier', 'low', 'cancellation sense', 'low signal level', 'power dissipation', 'low supply voltage']

Industry insiders loading up on cheap company stock
A surge of telecom executives and directors purchasing their own companies,
	stock in the last two months points toward a renewed optimism in the
	beleaguered sector, say some observers, who view the rash of insider
	buying as a vote of confidence from management. Airgate PCS, Charter
	Communications, Cox Communications, Crown Castle International, Nextel
	Communications and Nortel Networks all have seen infusions of insider
	investment this summer, echoing trends in both the telecom industry and
	the national economy
['telecom industry', 'insider investment']
['crown castle international', 'cheap company stock', 'industry insider', 'insider', 'telecom executive', 'beleaguered sector', 'month point', 'airgate pcs', 'cox communications', 'nortel networks']

New tuning method for PID controller
In this paper, a tuning method for proportional-integral-derivative (PID)
	controller and the performance assessment formulas for this method are
	proposed. This tuning method is based on a genetic algorithm based PID
	controller design method. For deriving the tuning formula, the genetic
	algorithm based design method is applied to design PID controllers for
	a variety of processes. The relationship between the controller
	parameters and the parameters that characterize the process dynamics
	are determined and the tuning formula is then derived. Using simulation
	studies, the rules for assessing the performance of a PID controller
	tuned by the proposed method are also given. This makes it possible to
	incorporate the capability to determine if the PID controller is well
	tuned or not into an autotuner. An autotuner based on this new tuning
	method and the corresponding performance assessment rules is also
	established. Simulations and real-time experimental results are given
	to demonstrate the effectiveness and usefulness of these formulas
['tuning method', 'pid controller', 'proportional-integral-derivative controller', 'genetic algorithm', 'controller design method', 'process dynamics', 'autotuner']
['design pid controller', 'time experimental result', 'pid controller', 'tuning method', 'new tuning method', 'performance assessment rule', 'controller design method', 'performance assessment formula', 'new tuning', 'tuning formula']

A 120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB/s runtime
	reconfigurable bus for PDA chip
A low-power three-dimensional (3-D) rendering engine is implemented as part of
	a mobile personal digital assistant (PDA) chip. Six-megabit embedded
	DRAM macros attached to 8-pixel-parallel rendering logic are logically
	localized with a 3.2-GB/s runtime reconfigurable bus, reducing the area
	by 25% compared with conventional local frame-buffer architectures. The
	low power consumption is achieved by polygon-dependent access to the
	embedded DRAM macros with line-block mapping providing
	read-modify-write data transaction. The 3-D rendering engine with
	2.22-Mpolygons/s drawing speed was fabricated using 0.18- mu m CMOS
	embedded memory logic technology. Its area is 24 mm/sup 2/ and its
	power consumption is 120 mW
['low-power 3d rendering engine', 'three-dimensional rendering engine', 'mobile pdachip', 'mobile personal digital assistant chip', 'embedded dram macros', '8-pixel-parallel rendering logic', 'reconfigurable bus', 'low powerconsumption', 'polygon-dependent access', 'line-block mapping', 'read-modify-write data transaction', 'cmos embedded memory logictechnology', '3d graphics rendering', '120 mw', '6 mbit', '3.2 gb/s', '0.18micron']
['mobile personal digital assistant', 'low power consumption', 'engine', 'conventional local frame', 'reconfigurable bus', 'runtime reconfigurable bus', 'pda chip', 'write datum transaction', 'dram', 'dram macro']

A digital-to-analog converter based on differential-quad switching
A high-conversion-rate high-resolution oversampling digital-to-analog converter
	(DAC) for direct digital modulation is addressed in this paper. A new
	type of switching scheme, called differential-quad switching, is
	presented. To verify the feasibility of this scheme, essential parts
	with some auxiliary circuitry for interfacing were fabricated in a 0.8-
	mu m CMOS technology. Measured results show that the switching scheme
	provides 11-b resolution at 100 MSamples/s and 6-b at 1 GSamples/s. The
	degradation in signal-to-noise ratio is not observed for the variation
	of the supply voltage down to 1.5 V, which means the proposed scheme is
	suitable for low-voltage applications
['high-conversion-rate dac', 'high-resolution dac', 'oversampling dac', 'digital-to-analog converter', 'cmos technology', 'direct digitalmodulation', 'differential-quad switching', 'signal-to-noise ratio', 'snr', '1.5 v', '0.8 micron']
['analog converter', 'mu m cmos technology', 'quad switching', 'direct digital modulation', 'rate high', 'switching scheme', 'auxiliary circuitry', 'differential', 'essential part', 'scheme']

Fast frequency acquisition phase-frequency detectors for Gsamples/s
	phase-locked loops
This paper describes two techniques for designing phase-frequency detectors
	(PFDs) with higher operating frequencies [periods of less than 8* the
	delay of a fan-out-4 inverter (FO-4)] and faster frequency acquisition.
	Prototypes designed in 0.25- mu m CMOS process exhibit operating
	frequencies of 1.25 GHz [=1/(8.FO-4)] and 1.5 GHz [=1/(6.7.FO-4)] for
	two techniques, respectively, whereas a conventional PFD operates at
	<1 GHz [=1/(10.FO-4)]. The two proposed PFDs achieve a capture range
	of 1.7* and 1.4* the conventional design, respectively
['phase-frequency detectors', 'fast frequency acquisition', 'cmos process', 'clockgenerator', 'latch-based pfd architecture', 'phase-locked loop', 'gsamples/spll', 'pass-transistor dff pfd architecture', '1.25 ghz', '1.5 ghz', '0.25micron']
['fast frequency acquisition phase', 'fast frequency acquisition', 'frequency detector', 'high operating frequency', 'cmos process exhibit', 'technique', 'out-4 inverter', 'pfd', '0.25- mu', 'ghz']

High-voltage transistor scaling circuit techniques for high-density
	negative-gate channel-erasing NOR flash memories
In order to scale high-voltage transistors for high-density negative-gate
	channel-erasing NOR flash memories, two circuit techniques were
	developed. A proposed level shifter with low operating voltage is
	composed of three parts, a latch holding the negative erasing voltage,
	two coupling capacitors connected with the latched nodes in the latch,
	and high-voltage drivers inverting the latch, resulting in reduction of
	the maximum internal voltage by 0.5 V. A proposed high-voltage
	generator adds a path-gate logic to a conventional high-voltage
	generator to realize both low noise and low ripple voltage, resulting
	in a reduction of the maximum internal voltage by 0.5 V. As a result,
	these circuit techniques along with high coupling-ratio cell technology
	can scale down the high-voltage transistors by 15% and can realize
	higher density negative-gate channel-erase NOR flash memories in
	comparison with the source-erase NOR flash memories
['hv transistor scaling circuit techniques', 'high-density nor flash memories', 'negative-gate channel-erasing flash memories', 'level shifter', 'lowoperating voltage shifter', 'high-voltage drivers', 'high-voltagegenerator', 'path-gate logic', 'hv generator', 'low noise', 'low ripplevoltage', 'high coupling-ratio cell technology']
['maximum internal voltage', 'voltage transistor', 'circuit technique', 'gate channel', 'low ripple voltage', 'high density negative', 'high', 'density negative', 'ratio cell technology', 'low operating voltage']

A 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using
	two-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) scheme
This paper reports a 0.8-V 128-kb four-way set-associative two-level CMOS cache
	memory using a novel two-stage wordline/bitline-oriented tag-compare
	(WLOTC/BLOTC) and sense wordline/bitline (SWL/SBL) tag-sense amplifiers
	with an eight-transistor (8-T) tag cell in Level 2 (L2) and a 10-T
	shrunk logic swing (SLS) memory cell. with the ground/floating (G/F)
	data sense amplifier in Level 1 (L1) for high-speed operation for
	low-voltage low-power VLSI system applications. Owing to the reduced
	loading at the SWL in the new 11-T tag cell using the WLOTC scheme, the
	10-T SLS memory cell with G/F sense amplifier in L1, and the split
	comparison of the index signal in the 8-T tag cells with SWL/SBL tag
	sense amplifiers in L2, this 0.8-V cache memory implemented in a 1.8-V
	0.18- mu m CMOS technology has a measured L1/L2 hit time of 11.6/20.5
	ns at the average dissipation of 0.77 mW at 50 MHz
['four-way set-associative memory', 'two-level cmos cache memory', 'cache memoryarchitecture', 'wordline/bitline-oriented tag-compare', 'sensewordline/bitline amplifiers', 'tag-sense amplifiers', 'eight-transistor tagcell', 'ten-transistor memory cell', 'shrunk logic swing memory cell', 'ground/floating data sense amplifier', 'high-speed operation', 'low-voltagevlsi system applications', 'low-power vlsi system applications', '0.8 v', '128 kbit', '50 mhz', '0.77 mw', '1.8 v', '0.18 micron', '11.6 ns', '20.5 ns']
['level cmos cache', 'cmos cache memory', 'f sense amplifier', 'sls memory cell', '11-t tag cell', 'way set', 'vlsi system application', 'new 11-t tag', 'stage wordline', 'datum sense amplifier']

Learning spatial relations using an inductive logic programming system
The ability to learn spatial relations is a prerequisite for performing many
	relevant tasks such as those associated with motion, orientation,
	navigation, etc. This paper reports on using an Inductive Logic
	Programming (ILP) system for learning function-free Horn-clause
	descriptions of spatial knowledge. Its main contribution, however, is
	to show that an existing relation between two reference systems-the
	speaker-relative and the absolute-can be automatically learned by an
	ILP system, given the proper background knowledge and positive examples
['spatial relations learning', 'inductive logic programming system', 'spatialrelations', 'function-free horn-clause descriptions']
['inductive logic programming system', 'spatial relation', '-PRON- main contribution', 'relevant task', 'relation', 'spatial knowledge', 'free horn', 'reference system', 'ilp', 'proper background knowledge']

Windows XP fast user switching
The Windows NT family of operating systems has always supported the concept of
	multiple user accounts, but they've taken the concept a step further
	with Windows XP's Fast User Switching feature. Fast User Switching is a
	new feature of Windows XP that allows multiple users to log on to the
	same machine and quickly switch between the logged on accounts. Fast
	User Switching is implemented using some of the built-in capabilities
	of Terminal Services. Terminal Server has been around for a while but
	is much more feature rich and integrated in Windows XP. A machine with
	the terminal services (Remote Desktop) client can log on to and run
	applications on a remote machine running the terminal server
['windows xp fast user switching', 'multiple user logon access', 'operating systems', 'multiple user accounts', 'terminal services', 'terminal server', 'remotedesktop']
['windows xp fast user switching', 'windows xp', 'fast user switching feature', 'windows nt family', 'multiple user account', 'multiple user', 'operating system', 'terminal server', 'new feature', 'machine']

Generating code at run time with Reflection.Emit
The .NET framework SDK includes several tools that convert source code into
	executable code-the C# and VB.NET compilers get most of the attention,
	but there are others. The Regex class (in the
	System.Text.RegularExpressions namespace) has the ability to compile
	favorite regular expressions into a .NET assembly. In fact, the NET
	Common Language Runtime (CLR) contains a whole namespace full of
	classes to help us build assemblies, define types, and emit their
	implementations, all at run time. These classes, which comprise the
	System.Reflection.Emit namespace, are known collectively as Reflection.
	Emit
['.net framework sdk', 'runtime code generation', 'regex class', '.net common languageruntime', 'assemblies', 'types', 'system.reflection.emit namespace', 'reflection.emit']
['common language runtime', 'run time', '.net framework sdk', 'favorite regular expression', 'executable code', 'vb.net compiler', 'reflection', 'regex class', 'regularexpression namespace', 'emit']

.NET obfuscation and intellectual property
The author considers obfuscation options for protecting .NET code. Many
	programs won't need obfuscation because the loss caused by reverse
	engineering will be nonexistent. Numerous obfuscators are already
	available for the .NET platform, ranging from a basic renaming
	obfuscator to a fully functional obfuscator that handles mixed
	IL/native code assemblies created in any managed language, including
	Microsoft's C++ with Managed Extensions. An obfuscator simply makes
	your application harder to reverse engineer. It does not prevent
	reverse engineering. However, the cost of obfuscation is insignificant
	when compared to the cost of a typical software development project. If
	you feel like an obfuscator provides you any benefit at all, it's
	probably worth the price
['.net obfuscation', 'intellectual property', 'reverse engineering']
['typical software development project', 'native code assembly', 'obfuscation', 'functional obfuscator', '.net code', '.net platform', 'numerous obfuscator', 'managed extensions', 'obfuscation option', 'intellectual property']

Controller performance analysis with LQG benchmark obtained under closed loop
	conditions
This paper proposes a new method for obtaining a linear quadratic Gaussian
	(LQG) benchmark in terms of the variances of process input and output
	from closed-loop data, for assessing the controller performance. LQG
	benchmark has been proposed in the literature to assess controller
	performance since the LQG tradeoff curve represents the limit of
	performance in terms of input and output variances. However, an
	explicit parametric model is required to calculate the LQG benchmark.
	In this work, we propose a data driven subspace approach to calculate
	the LQG benchmark under closed-loop conditions with certain external
	excitations. The optimal LQG-benchmark variances are obtained directly
	from the subspace matrices corresponding to the deterministic inputs
	and the stochastic inputs, which are identified using closed-loop data
	with setpoint excitation. These variances are used for assessing the
	controller performance. The method proposed in this paper is applicable
	to both univariate and multivariate systems. Profit analysis for the
	implementation of feedforward control to the existing feedback-only
	control system is also analyzed under the optimal LQG performance
	framework
['controller performance analysis', 'lqg benchmark', 'linear quadratic gaussianbenchmark', 'closed-loop data', 'subspace matrices', 'deterministic inputs', 'stochastic inputs', 'univariate systems', 'multivariate systems', 'profitanalysis', 'feedforward control', 'state space model']
['explicit parametric model', 'optimal lqg performance', 'controller performance', 'controller performance analysis', 'linear quadratic gaussian', 'lqg tradeoff curve', 'lqg benchmark', 'loop datum', 'closed loop', 'new method']

Lossy SPICE models produce realistic averaged simulations
In previous averaged models, the state-space averaging technique or switch
	waveforms analysis were usually applied over perfect elements,
	non-inclusive of the ohmic losses. However, if these elements play an
	active role in the DC transfer function, they affect the small-signal
	AC analysis by introducing various damping effects. A model is
	introduced in a boost voltage-mode application
['lossy spice models', 'realistic averaged simulations', 'state-space averagingtechnique', 'switch waveforms analysis', 'damping effects', 'boostvoltage-mode application', 'ohmic losses', 'dc transfer function']
['dc transfer function', 'space averaging technique', 'previous averaged model', 'lossy spice model', 'realistic averaged simulation', 'perfect element', 'ohmic loss', 'waveform analysis', 'active role', 'ac analysis']

CAD/CAE software aids converter design [DC/DC power conversion]
Typically, power supply design involves electronic and magnetic components. In
	this paper, the authors describe, using a flyback converter example,
	how CAD/CAE tools can aid the power supply engineer in both areas,
	reducing prototyping costs and providing insights into system
	performance
['dc/dc power convertor design', 'power supply design', 'electronic components', 'magnetic components', 'cad/cae software', 'flyback power convertortopology', 'prototyping costs']
['power supply engineer', 'flyback converter example', 'power supply design', 'dc power conversion', 'converter design', 'cae software', 'cad', 'magnetic component', 'cae tool', 'author']

Using virtual reality to teach disability awareness
A desktop virtual reality (VR) program was designed and evaluated to teach
	children about the accessibility and attitudinal barriers encountered
	by their peers with mobility impairments. Within this software,
	children sitting in a virtual wheelchair experience obstacles such as
	stairs, narrow doors, objects too high to reach, and attitudinal
	barriers such as inappropriate comments. Using a collaborative research
	methodology, 15 youth with mobility impairments assisted in developing
	and beta-testing the software. The effectiveness of the program was
	then evaluated with 60 children in Grades 4-6 using a controlled
	pretest/posttest design. The results indicated that the program was
	effective for increasing children's knowledge of accessibility
	barriers. Attitudes, grade level, familiarity with individuals with a
	disability, and gender were also investigated
['virtual reality', 'disability awareness teaching', 'children', 'accessibility', 'virtual wheelchair', 'collaborative research methodology', 'mobilityimpairments', 'software beta-testing', 'collaborative software development', 'computer aided instruction', 'software effectiveness', 'gender']
['virtual wheelchair experience obstacle', 'desktop virtual reality', 'mobility impairment', 'virtual reality', 'disability awareness', 'child', 'program', 'narrow door', 'attitudinal barrier', 'collaborative research']

Effects of white space in learning via the Web
This study measured the effect of specific white space features on learning
	from instructional Web materials. The study also measured learners'
	beliefs regarding Web-based instruction. Prior research indicated that
	small changes in the handling of presentation elements can affect
	learning. Achievement results from this study indicated that in on-line
	materials, when content and overall structure are sound, minor
	differences regarding table borders and vertical spacing in text do not
	hinder learning. Beliefs regarding Web-based instruction and
	instructors who use it did not differ significantly between treatment
	groups. Implications of the study and cautions regarding generalizing
	from the results are discussed
['white space features', 'web-based instruction', 'presentation', 'online educationalmaterials', 'table borders', 'text vertical spacing', 'internet']
['specific white space feature', 'study', 'instructional web material', 'effect', 'web', 'belief', 'achievement result', 'presentation element', 'vertical spacing', 'small change']

The efficacy of electronic telecommunications in fostering interpersonal
	relationships
The effectiveness of electronic telecommunications as a supplementary aid to
	instruction and as a communication link between students, and between
	students and instructors in fostering interpersonal relationships was
	explored in this study. More specifically, the impacts of e-mail, one
	of the most accessible, convenient, and easy to use computer-mediated
	communications, on student attitudes toward the instructor,
	group-mates, and other classmates were investigated. A posttest-only
	experimental design was adopted. In total, 68 prospective teachers
	enrolling in a "Computers in Education" course participated in the
	study for a whole semester. Results from the study provided substantial
	evidence supporting e-mail's beneficial effects on student attitudes
	toward the instructor and other classmates
['interpersonal relationships', 'telecommunications', 'student communication link', 'e-mail', 'computer-mediated communications', 'student attitudes', 'computersin education course', 'educational technology']
['student attitude', 'experimental design', 'prospective teacher', 'electronic telecommunication', 'interpersonal relationship', 'communication link', 'supplementary aid', 'instructor', 'beneficial effect', 'student']

